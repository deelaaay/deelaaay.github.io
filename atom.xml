<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>deelaaay learning</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://idcat.cn/"/>
  <updated>2018-05-11T02:07:10.100Z</updated>
  <id>http://idcat.cn/</id>
  
  <author>
    <name>deelaaay</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>systemtap-centos7上安装调试</title>
    <link href="http://idcat.cn/2018/05/11/systemtap-centos7%E4%B8%8A%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95/"/>
    <id>http://idcat.cn/2018/05/11/systemtap-centos7上安装调试/</id>
    <published>2018-05-11T02:04:36.000Z</published>
    <updated>2018-05-11T02:07:10.100Z</updated>
    
    <content type="html"><![CDATA[<p><strong>systemtap简介</strong></p><p>SystemTap 是一款诊断Linux系统性能的工具，可以跟踪内核以及用户态程序中的任意函数、syscall、语句甚至指令，可以用来动态地收集调试和性能信息的工具，不需要我们重新编译、重启内核。</p><p>详细介绍查看官网地址。其官网地址 <a href="https://sourceware.org/systemtap/" title="https://sourceware.org/systemtap/" target="_blank" rel="noopener">https://sourceware.org/systemtap/</a>。<br><a id="more"></a><br><strong>systemtap安装</strong></p><p>本例环境 centos7 7.2.1511 内核版本3.10.0-693.21.1.el7.x86_64</p><p>systemtap包安装</p><pre><code>yum install systemtap systemtap-runtime</code></pre><p>systemtap内核信息安装包安装</p><p>• kernel-debuginfo</p><p>• kernel-debuginfo-common</p><p>• kernel-devel</p><p>通过root用户执行如下命令进行以上3个包的自动安装</p><pre><code>stap-prep</code></pre><p><strong>确认安装成功</strong></p><p>执行如下脚本，返回complete即可</p><pre><code>[root@node1 ~]# stap -v -e &apos;probe vfs.read {printf(&quot;read performed\n&quot;); exit()}&apos;Pass 1: parsed user script and 466 library scripts using 226636virt/39688res/3308shr/36524data kb, in 190usr/130sys/323real ms.Pass 2: analyzed script: 1 probe, 1 function, 7 embeds, 0 globals using 371188virt/179300res/4640shr/181076data kb, in 1270usr/690sys/1964real ms.Pass 3: using cached /root/.systemtap/cache/df/stap_df47b68c3f1c63f931107c651e9afa76_2692.cPass 4: using cached /root/.systemtap/cache/df/stap_df47b68c3f1c63f931107c651e9afa76_2692.koPass 5: starting run.read performedPass 5: run completed in 0usr/50sys/448real ms.[root@node1 ~]# </code></pre><p><strong>其他机器上执行</strong></p><p>当用户允许一个systemtap 脚本的时候，它会将脚本build为一个内核模块，然后加载该模块，当有事件触发该模块的时候，则执行模块。但是systemtap 脚本只能在部署了debuginfo等内核信息包的主机运行，那么在其他没有安装这些debuginfo的主机如何运行systemtap脚本呢？</p><p>这里systemtap，命令stap支持指定参数-m 将脚本编译为一个内核模块，然后将该模块复制到其他没有安装debuginfo的节点。在其他节点上通过staprun module.ko执行脚本。</p><p>举例：</p><ul><li><p>node1 作为host system是安装了systemtap systemtap-runtime kernel-debuginfo等所有包的节点。</p></li><li><p>node3 作为target system 仅仅安装了systemtap-runtime包。</p></li><li><p>node1 和node3 内核尽量保持一致。不一致也没有关系，通过-r参数指定target system内核版本即可,但是前提是node1即host system节点必须也安装了target system节点的内核版本。</p></li></ul><p>查看内核信息以及将脚本编译输出到模块teststap</p><pre><code>[root@node1 ~]# uname -r3.10.0-693.21.1.el7.x86_64[root@node3 ~]# uname -r3.10.0-693.21.1.el7.x86_64[root@node1 ~]# stap -v -r 3.10.0-693.21.1.el7.x86_64 -e &apos;probe vfs.read {printf(&quot;read performed\n&quot;); exit()}&apos; -m teststap 1: parsed user script and 466 library scripts using 226648virt/39732res/3336shr/36536data kb, in 290usr/30sys/329real ms.Pass 2: analyzed script: 1 probe, 1 function, 7 embeds, 0 globals using 371200virt/179340res/4660shr/181088data kb, in 1160usr/650sys/1810real ms.Pass 3: translated to C into &quot;/tmp/stap2XZAPY/teststap_src.c&quot; using 371200virt/179588res/4908shr/181088data kb, in 20usr/70sys/81real ms.Pass 4: compiled C into &quot;teststap.ko&quot; in 790usr/1380sys/2079real ms.Pass 5: starting run.read performedPass 5: run completed in 0usr/70sys/393real ms.[root@node1 ~]# ll teststap.ko  -rw-r--r-- 1 root root 96912 May 10 16:29 teststap.ko</code></pre><p>上面命令-r 指定内核版本，若是2者之间内核一致，则可以不需要-r参数。<br>将该模块文件复制到node3节点，然后执行如下确认：</p><pre><code>[root@node1 ~]# scp teststap.ko root@node3:/rootteststap.ko                                                                                  100%   95KB  33.9MB/s   00:00    [root@node1 ~]# ssh node3Last login: Thu May 10 14:36:32 2018 from 192.168.0.99             [root@node3 ~]# staprun teststap.ko read performed[root@node3 ~]# </code></pre><p>成功执行模块指令</p><p><strong>运行systemtap 脚本</strong></p><p>常用命令stap staprun 相关使用使用man查看</p><p>运行stap命令用户最好拥有root权限，若是普通用户则需要将其加入到组stapdev 或者stapusr ，详情查看官网介绍。</p><p>脚本运行方式：</p><p>1、通过脚本文件</p><p>stap script_file_name</p><p>2、通过- 从标准输出中获取脚本信息</p><p>echo “probe timer.s(1) {exit()}” | stap -v -</p><p><strong>stap基本参数</strong></p><p>-v 详细列出输出信息-vvv列出更多信息</p><p>-o filename 将标准输出到指定文件名</p><p>-S size,count 限制output文件的数量和单个文件大小。</p><p>-x process_id 设置systemtap句柄函数target()获取指定进程ID的信息</p><p>-c command  设置systemtap句柄函数target()通过指定的command来运行</p><p>-e ‘scrip’ 使用脚本作为systemtap的输入</p><p>-F 让systemtap在后台已进程方式运行，默认后台模式有2种，一种是in-memory ，一种是file 模式</p><ul><li>in-memory 模式</li></ul><p>执行</p><pre><code>stap -F iotime.stp </code></pre><p>执行改命令后，stap打印简单的改命令执行信息，方便你重新连接改脚本：</p><pre><code>Disconnecting from systemtap module.To reconnect, type &quot;staprun -A stap_5dd0073edcb1f13f7565d8c343063e68_19556&quot;</code></pre><p>获取结果</p><pre><code>staprun -A stap_5dd0073edcb1f13f7565d8c343063e68_19556</code></pre><ul><li>file 模式</li></ul><p>执行</p><pre><code>stap -F -o /tmp/pfaults.log -S 1,2 pfaults.stp</code></pre><p>生成2个文件，每个文件1M大小，文件格式如/tmp/iotime.log.[0-9]+<br>始终保持最新的数据到这2个文件中，旧数据会被remove。该命令执行后会输出进程id号，则通过kill 命令停止脚本运行，举例id号为7590</p><pre><code>kill -s SIGTERM 7590</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;systemtap简介&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;SystemTap 是一款诊断Linux系统性能的工具，可以跟踪内核以及用户态程序中的任意函数、syscall、语句甚至指令，可以用来动态地收集调试和性能信息的工具，不需要我们重新编译、重启内核。&lt;/p&gt;
&lt;p&gt;详细介绍查看官网地址。其官网地址 &lt;a href=&quot;https://sourceware.org/systemtap/&quot; title=&quot;https://sourceware.org/systemtap/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://sourceware.org/systemtap/&lt;/a&gt;。&lt;br&gt;
    
    </summary>
    
    
      <category term="systemtap" scheme="http://idcat.cn/tags/systemtap/"/>
    
  </entry>
  
  <entry>
    <title>记录一次因对象权限不对导致osd无法启动的排查过程</title>
    <link href="http://idcat.cn/2018/05/11/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E5%9B%A0%E5%AF%B9%E8%B1%A1%E6%9D%83%E9%99%90%E4%B8%8D%E5%AF%B9%E5%AF%BC%E8%87%B4osd%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8%E7%9A%84%E6%8E%92%E6%9F%A5%E8%BF%87%E7%A8%8B/"/>
    <id>http://idcat.cn/2018/05/11/记录一次因对象权限不对导致osd无法启动的排查过程/</id>
    <published>2018-05-11T02:03:48.000Z</published>
    <updated>2018-05-11T02:06:39.057Z</updated>
    
    <content type="html"><![CDATA[<p>记录一次osd启动失败的例子，仅个人记录使用。<br><a id="more"></a><br>集群状态：</p><pre><code>[root@node1 omap]# ceph -scluster 911c57dc-a930-4da8-ab0e-69f6b6586e3d health HEALTH_WARN        328 pgs degraded        328 pgs stuck degraded        328 pgs stuck unclean        328 pgs stuck undersized        328 pgs undersized        recovery 319/957 objects degraded (33.333%)        too many PGs per OSD (328 &gt; max 300) monmap e1: 3 mons at {node1=192.168.1.141:6789/0,node2=192.168.1.142:6789/0,node3=192.168.1.143:6789/0}        election epoch 108, quorum 0,1,2 node1,node2,node3  fsmap e705662: 1/1/1 up {0=node1=up:active}, 2 up:standby osdmap e705733: 3 osds: 2 up, 2 in; 328 remapped pgs        flags sortbitwise,require_jewel_osds  pgmap v1296660: 328 pgs, 12 pools, 457 MB data, 319 objects        1071 MB used, 29626 MB / 30697 MB avail        319/957 objects degraded (33.333%)             328 active+undersized+degraded</code></pre><p>查看是node1上的osd0 没有启动，于是启动osd0</p><pre><code>2018-05-10 12:08:08.756988 7fa896131800  0 filestore(/var/lib/ceph/osd/ceph-0) mount: enabling WRITEAHEAD journal mode: checkpoint is not enabled2018-05-10 12:08:08.760327 7fa896131800  1 journal _open /var/lib/ceph/osd/ceph-0/journal fd 18: 5368709120 bytes, block size 4096 bytes, directio = 1, aio = 12018-05-10 12:08:08.762157 7fa896131800  1 journal _open /var/lib/ceph/osd/ceph-0/journal fd 18: 5368709120 bytes, block size 4096 bytes, directio = 1, aio = 12018-05-10 12:08:08.763165 7fa896131800  1 filestore(/var/lib/ceph/osd/ceph-0) upgrade2018-05-10 12:08:08.763591 7fa896131800  0 &lt;cls&gt; cls/cephfs/cls_cephfs.cc:202: loading cephfs_size_scan2018-05-10 12:08:08.763774 7fa896131800  0 &lt;cls&gt; cls/hello/cls_hello.cc:305: loading cls_hello2018-05-10 12:08:08.770974 7fa896131800 -1 osd/OSD.h: In function &apos;OSDMapRef OSDService::get_map(epoch_t)&apos; thread 7fa896131800 time 2018-05-10 12:08:08.769611osd/OSD.h: 894: FAILED assert(ret) ceph version 10.2.10 (5dc1e4c05cb68dbf62ae6fce3f0700e4654fdbbe) 1: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x85) [0x560b85e5c9e5] 2: (OSDService::get_map(unsigned int)+0x3d) [0x560b8582f14d] 3: (OSD::init()+0x1fe2) [0x560b857e2b42] 4: (main()+0x2c01) [0x560b85746461] 5: (__libc_start_main()+0xf5) [0x7fa892e2fc05] 6: (()+0x35d917) [0x560b85790917] NOTE: a copy of the executable, or `objdump -rdS &lt;executable&gt;` is needed to interpret this.</code></pre><p>只能看到get_map(epoch_t) FAILED assert(ret)。不知道什么原因导致的。于是开启调试日志启动该osd查看更多详细信息。</p><pre><code>[root@node1 ceph-0]# ceph-osd -f --cluster ceph -i 0 --setuser ceph --setgroup ceph --debug-osd=10 --debug-filestore=10 --log-to-stderr=1</code></pre><p>查看输出中显示如下：</p><pre><code>-2&gt; 2018-05-10 12:57:33.725952 7f66199a7800 10 filestore(/var/lib/ceph/osd/ceph-0) error opening file /var/lib/ceph/osd/ceph-0/current/meta/DIR_4/DIR_A/DIR_0/osdmap.705730__0_C8EB90A4__none with flags=2: (13) Permission denied-1&gt; 2018-05-10 12:57:33.725960 7f66199a7800 10 filestore(/var/lib/ceph/osd/ceph-0) FileStore::read(meta/#-1:2509d713:::osdmap.705730:0#) open error: (13) Permission denied 0&gt; 2018-05-10 12:57:33.727508 7f66199a7800 -1 osd/OSD.h: In function &apos;OSDMapRef OSDService::get_map(epoch_t)&apos; thread 7f66199a7800 time 2018-05-10 12:57:33.725969osd/OSD.h: 894: FAILED assert(ret)</code></pre><p>有对象文件权限拒绝。</p><p>进入目录查看</p><pre><code>[root@node1 DIR_0]# lltotal 24-rw-r--r-- 1 ceph ceph 5171 Apr 25 09:32 osdmap.705305__0_C8F530A4__none-rw-r--r-- 1 ceph ceph 5171 Apr 25 09:39 osdmap.705518__0_C8F460A4__none-rw-r--r-- 1 root root 5659 May 10 11:48 osdmap.705730__0_C8EB90A4__none</code></pre><p>均是root用户。于是修改权限</p><pre><code>[root@node1 DIR_0]# chown ceph:ceph osdmap.705730__0_C8EB90A4__none </code></pre><p>启动该osd</p><pre><code>[root@node1 DIR_0]# systemctl start ceph-osd@0</code></pre><p>查看集群状态</p><pre><code>[root@node1 DIR_0]# ceph -scluster 911c57dc-a930-4da8-ab0e-69f6b6586e3d health HEALTH_WARN        too many PGs per OSD (328 &gt; max 300) monmap e1: 3 mons at {node1=192.168.1.141:6789/0,node2=192.168.1.142:6789/0,node3=192.168.1.143:6789/0}        election epoch 108, quorum 0,1,2 node1,node2,node3  fsmap e705662: 1/1/1 up {0=node1=up:active}, 2 up:standby osdmap e705736: 3 osds: 3 up, 3 in        flags sortbitwise,require_jewel_osds  pgmap v1296809: 328 pgs, 12 pools, 457 MB data, 319 objects        1593 MB used, 44453 MB / 46046 MB avail             328 active+clean</code></pre><p>至此，集群恢复OK。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录一次osd启动失败的例子，仅个人记录使用。&lt;br&gt;
    
    </summary>
    
    
      <category term="ceph" scheme="http://idcat.cn/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>blktrace+fio对比测试分析</title>
    <link href="http://idcat.cn/2018/05/08/blktrace-fio%E5%AF%B9%E6%AF%94%E6%B5%8B%E8%AF%95%E5%88%86%E6%9E%90/"/>
    <id>http://idcat.cn/2018/05/08/blktrace-fio对比测试分析/</id>
    <published>2018-05-08T08:13:20.000Z</published>
    <updated>2018-05-08T08:25:44.517Z</updated>
    
    <content type="html"><![CDATA[<p>利用BLKTRACE分析磁盘IO </p><p>在Linux系统上，查看磁盘的负载情况，咱们一般使用iostat监控工具，iostat的详细介绍查看另外的培训资料。其中很重要的参数就是await，await表示单个I/O所需的平均时间，但它同时包含了I/O Scheduler所消耗的时间和硬件所消耗的时间，所以不能作为硬件性能的指标。那如何才能分辨一个io从下发到返回整个时间上，是硬件层耗时多还是在io调度上耗时多呢？如何查看io在各个时间段所消耗的时间呢？那么，blktrace在这种场合就能派上用场，因为它能记录I/O所经历的各个步骤，从中可以分析是IO Scheduler慢还是硬件响应慢，以及各个时间段所用时间。<br><a id="more"></a><br>blktrace的原理<br>一个I/O请求进入block layer之后，可能会经历下面的过程：</p><ul><li>Remap: 可能被DM(Device Mapper)或MD(Multiple Device, Software RAID) remap到其它设备</li><li>Split: 可能会因为I/O请求与扇区边界未对齐、或者size太大而被分拆(split)成多个物理I/O</li><li>Merge: 可能会因为与其它I/O请求的物理位置相邻而合并(merge)成一个I/O</li></ul><ul><li>被IO Scheduler依照调度策略发送给driver</li></ul><ul><li>被driver提交给硬件，经过HBA、电缆（光纤、网线等）、交换机（SAN或网络）、最后到达存储设备，设备完成IO请求之后再把结果发回。</li></ul><p>blktrace能记录I/O所经历的各个步骤，来看一下它记录的数据，包含9个字段，下图标示了其中8个字段的含义，大致的意思是“哪个进程在访问哪个硬盘的哪个扇区，进行什么操作，进行到哪个步骤，时间戳是多少”：<br><img src="https://i.imgur.com/tyws5Pz.png" alt=""></p><ul><li>第一个字段：8,0 这个字段是设备号 major device ID和minor device ID。-第二个字段：3 表示CPU</li><li>第三个字段：11 序列号</li><li>第四个字段：0.009507758 Time Stamp是时间偏移</li><li>第五个字段：PID 本次IO对应的进程ID</li><li>第六个字段：Event，这个字段非常重要，反映了IO进行到了那一步</li><li>第七个字段：R表示 Read， W是Write，D表示block，B表示BarrierOperation</li><li>第八个字段：223490+56，表示的是起始block number 和 number of blocks，即我们常说的Offset 和 Size(扇区？？)</li><li>第九个字段： 进程名</li></ul><p>其中第六个字段非常有用：每一个字母都代表了IO请求所经历的某个阶段。</p><ul><li>A  映射值对应设备 IO was remapped to a different device</li><li>B  IO反弹，由于32位地址长度限制，所以需要copy数据到低位内存，这会有性能损耗。IO bounced</li><li>C  IO完成 IO completion</li><li>D  将IO发送给驱动 IO issued to driver</li><li>F  IO请求，前合并 IO front merged with request on queue</li><li>G  获取 请求 Get request</li><li>I  IO插入请求队列 IO inserted onto request queue</li><li>M  IO请求，后合并 IO back merged with request on queue</li><li>P  插上块设备队列（队列插入机制） Plug request</li><li>Q  io被请求队列处理代码接管。 IO handled by request queue code</li><li>S  等待发送请求。 Sleep request</li><li>T  由于超时而拔出设备队列 Unplug due to timeout</li><li>U  拔出设备队列 Unplug request</li><li>X  开始新的扇区 Split</li></ul><p>其中最重要的几个阶段如下：</p><pre><code>Q – 即将生成IO请求|G – IO请求生成|I – IO请求进入IO Scheduler队列|D – IO请求进入driver|C – IO请求执行完毕</code></pre><p>根据以上步骤对应的时间戳就可以计算出I/O请求在每个阶段所消耗的时间：</p><pre><code>Q2G – 生成IO请求所消耗的时间，包括remap和split的时间；G2I – IO请求进入IO Scheduler所消耗的时间，包括merge的时间；I2D – IO请求在IO Scheduler中等待的时间；D2C – IO请求在driver和硬件上所消耗的时间；Q2C – 整个IO请求所消耗的时间(Q2I + I2D + D2C = Q2C)，相当于iostat的await。</code></pre><p>btt工具，blkparse等参考<a href="http://www.idcat.cn/2018/04/21/blktrace-%E5%B7%A5%E5%85%B7%E7%AE%80%E4%BB%8B/" title="http://www.idcat.cn/2018/04/21/blktrace-%E5%B7%A5%E5%85%B7%E7%AE%80%E4%BB%8B/" target="_blank" rel="noopener">http://www.idcat.cn/2018/04/21/blktrace-%E5%B7%A5%E5%85%B7%E7%AE%80%E4%BB%8B/</a></p><h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><p><strong>1、4k direct=1</strong> </p><p>通过fio对本地磁盘sdc挂载点进行4K小文件顺序写测试，期间监控磁盘信息。完成fio测试后，手动终止监控程序，然后进行数据分析。并与iostat的数据进行对比分析。</p><p>fio 开启测试</p><pre><code>[root@node1 samba-test]# fio -filename=./4k_file -direct=1 -iodepth=1 -thread -rw=write -ioengine=libaio -bs=4k -size=3G -numjobs=4  -times=300 -group_reporting -name=mytest2mytest2: (g=0): rw=write, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=1...fio-2.2.5Starting 4 threads</code></pre><p>注意：</p><p>1、direct=1即o_direct直写io。不会经过系统缓存，即没有合并</p><p>2、numjobs=4 bs=4k  对应iostat即队列深度为4.</p><p>3、bs=4k 对应iostat扇区数为8 </p><p>开启监控磁盘sdc</p><pre><code>[root@node1 ~]# blktrace -d /dev/sdc   </code></pre><p>iostat监控</p><p><img src="https://i.imgur.com/pbfZKtL.jpg" alt=""></p><p>FIO运行结束后，终止blktrace进程</p><p>收集数据</p><pre><code>[root@node1 fioblk]# blktrace -d /dev/sdc^C=== sdc ===  CPU  0:              5504242 events,   258012 KiB data  CPU  1:              5693967 events,   266905 KiB data  Total:              11198209 events (dropped 0),   524917 KiB data[root@node1 fioblk]# lltotal 524920-rw-r--r-- 1 root root 264203656 Apr 27 14:08 sdc.blktrace.0-rw-r--r-- 1 root root 273310432 Apr 27 14:08 sdc.blktrace.1[root@node1 fioblk]#</code></pre><p>会看到生成文件数以当前cpu个数命名。</p><p>生成数据</p><p>blkparse -i sdc -d sdc.blktrace.bin</p><p>会将所有数据汇总到文件sdc.blktrace.bin里面。期间我截图了如下，代表3个io的过程。对比上面介绍。<br> <img src="https://i.imgur.com/nGTPClj.jpg" alt=""><br>WS表示同步写</p><p>分析数据</p><p>生成所有数据，后面介绍针对性的数据。</p><pre><code>[root@node1 fioblk]# btt -i sdc.blktrace.bin -A |less </code></pre><p><img src="https://i.imgur.com/QbV8sv4.jpg" alt=""></p><ul><li>Q2G – 生成IO请求所消耗的时间，包括remap和split的时间；本例0.009ms</li><li>G2I – IO请求进入IO Scheduler所消耗的时间，包括merge的时间;0.001ms</li><li>I2D – IO请求在IO Scheduler中等待的时间；0.0009ms</li><li>D2C – IO请求在driver和硬件上所消耗的时间；0.7ms</li></ul><p>因此：</p><p>Q2C – 整个IO请求所消耗的时间(Q2I + I2D + D2C = Q2C)，相当于iostat的await。 几乎0.78ms与iostat wait值一致。在io scheduler上几乎没有消耗任何时间。</p><p>生成iops 和bw每秒值</p><pre><code>btt -i sdc.blktrace.bin -q sdc.q2c_latencytotal 1079400-rw-r--r-- 1 root root      2759 Apr 27 14:45 8,32_iops_fp.dat-rw-r--r-- 1 root root      4340 Apr 27 14:45 8,32_mbps_fp.dat-rw-r--r-- 1 root root 264203656 Apr 27 14:08 sdc.blktrace.0-rw-r--r-- 1 root root 273310432 Apr 27 14:08 sdc.blktrace.1-rw-r--r-- 1 root root 537514088 Apr 27 14:16 sdc.blktrace.bin-rw-r--r-- 1 root root  30244692 Apr 27 14:45 sdc.q2c_latency_8,32_q2c.dat-rw-r--r-- 1 root root      2759 Apr 27 14:45 sys_iops_fp.dat-rw-r--r-- 1 root root      4340 Apr 27 14:45 sys_mbps_fp.dat</code></pre><p>生成io大小分布</p><pre><code>btt -i sdc.blktrace.bin -B sdc.offset[root@node1 fioblk]# lltotal 1183168-rw-r--r-- 1 root root      2759 Apr 27 14:57 8,32_iops_fp.dat-rw-r--r-- 1 root root      4340 Apr 27 14:57 8,32_mbps_fp.dat-rw-r--r-- 1 root root 264203656 Apr 27 14:08 sdc.blktrace.0-rw-r--r-- 1 root root 273310432 Apr 27 14:08 sdc.blktrace.1-rw-r--r-- 1 root root 537514088 Apr 27 14:16 sdc.blktrace.bin-rw-r--r-- 1 root root  53125272 Apr 27 14:57 sdc.offset_8,32_c.dat-rw-r--r-- 1 root root  53125272 Apr 27 14:57 sdc.offset_8,32_w.dat</code></pre><p>本例只有write,所有生成了_w _c   写是w  读是r  c是w+c 本例c和w一样大。查看文件内容</p><pre><code>0.000002716 52608272 526082800.000129844 52608000 526080080.000324497 52607688 526076960.000927928 52606144 526061520.001015187 52608280 526082880.001449302 52608008 52608016</code></pre><p>第一行为时间，第二个为其实扇区大小，第3行为每个io结束扇区大小。可以算出每个io为8个扇区大小，既4k 与测试实际相符合。</p><p><strong>2、1M direct=1</strong></p><p>fio运行</p><pre><code>[root@node1 samba-test]# fio -filename=./1m_file -direct=1 -iodepth=1 -thread -rw=write -ioengine=libaio -bs=1M -size=3G -numjobs=4  -times=300 -group_reporting -name=mytest2mytest2: (g=0): rw=write, bs=1M-1M/1M-1M/1M-1M, ioengine=libaio, iodepth=1...fio-2.2.5Starting 4 threadsmytest2: Laying out IO file(s) (1 file(s) / 3072MB)</code></pre><p>注意：</p><ul><li>1、bs=1M ，sdc块设备最大单次io大小为512，因此会分为2个io写入，numjobs=4,因此队列深度为2*4 = 8 </li><li>[root@node1 1m]# cat /sys/block/sdc/queue/max_sectors_kb<br>512</li><li>[root@node1 ~]# cat /sys/block/sdc/queue/nr_requests<br>128   磁盘最大队列深度。</li><li>2、bs=1M  最大io数为512kb，除以512byte扇区大小为操作扇区数1024<br>与iostat一致。</li></ul><p>blktrace取值</p><pre><code>[root@node1 1m]# blktrace -d /dev/sdc</code></pre><p>iostat取值<br><img src="https://i.imgur.com/O8hjNoU.jpg" alt=""></p><p>fio运行完成后，生成数据，并分析数据</p><p>生成数据</p><pre><code>[root@node1 1m]# blkparse -i sdc -d sdc.blktrace.bin</code></pre><p><img src="https://i.imgur.com/5YcmCt4.jpg" alt=""></p><p>可以看到Q-C一个完整的io路径。右边起始扇区+1024个扇区数。与实际相符合。</p><p>分析数据</p><pre><code>root@node1 1m]# btt -i sdc.blktrace.bin -A | less</code></pre><p><img src="https://i.imgur.com/FiYWzt6.jpg" alt=""></p><ul><li>Q2G – 生成IO请求所消耗的时间，包括remap和split的时间；<br>本例0.7ms 因为有块分为2部分。1m分为2个512kb io，单个512kb io 到扇区后得与扇区对齐等等</li><li>G2I – IO请求进入IO Scheduler所消耗的时间，包括merge的时间;<br>0.009ms  没有进行合并 绕过缓存了。</li><li>I2D – IO请求在IO Scheduler中等待的时间；<br>2.8ms</li><li>D2C – IO请求在driver和硬件上所消耗的时间；<br>34.5 ms</li></ul><p>因此：</p><p>Q2C – 整个IO请求所消耗的时间(Q2I + I2D + D2C = Q2C)，相当于iostat的await。 为38.1ms与iostat wait值几乎一致。在io scheduler上几乎没有消耗任何时间。时间花费在driver硬件层。</p><p>生成iops 和bw每秒值</p><pre><code>btt -i sdc.blktrace.bin -q sdc.q2c_latency[root@node1 1m]# lltotal 14508-rw-r--r-- 1 root root     913 Apr 27 15:40 8,32_iops_fp.dat-rw-r--r-- 1 root root    1731 Apr 27 15:40 8,32_mbps_fp.dat-rw-r--r-- 1 root root 3295528 Apr 27 15:20 sdc.blktrace.0-rw-r--r-- 1 root root 3894704 Apr 27 15:20 sdc.blktrace.1-rw-r--r-- 1 root root 7190232 Apr 27 15:28 sdc.blktrace.bin-rw-r--r-- 1 root root  452027 Apr 27 15:40 sdc.q2c_latency_8,32_q2c.dat-rw-r--r-- 1 root root     913 Apr 27 15:40 sys_iops_fp.dat-rw-r--r-- 1 root root    1731 Apr 27 15:40 sys_mbps_fp.dat</code></pre><p>生成io大小分布</p><pre><code>btt -i sdc.blktrace.bin -B sdc.offset[root@node1 1m]# lltotal 16148-rw-r--r-- 1 root root     913 Apr 27 15:44 8,32_iops_fp.dat-rw-r--r-- 1 root root    1731 Apr 27 15:44 8,32_mbps_fp.dat-rw-r--r-- 1 root root 3295528 Apr 27 15:20 sdc.blktrace.0-rw-r--r-- 1 root root 3894704 Apr 27 15:20 sdc.blktrace.1-rw-r--r-- 1 root root 7190232 Apr 27 15:28 sdc.blktrace.bin-rw-r--r-- 1 root root  835954 Apr 27 15:44 sdc.offset_8,32_c.dat-rw-r--r-- 1 root root  835954 Apr 27 15:44 sdc.offset_8,32_w.dat-rw-r--r-- 1 root root  452027 Apr 27 15:40 sdc.q2c_latency_8,32_q2c.dat-rw-r--r-- 1 root root     913 Apr 27 15:44 sys_iops_fp.dat-rw-r--r-- 1 root root    1731 Apr 27 15:44 sys_mbps_fp.dat</code></pre><p>只有写，即_w 与_c一致。查看_w文件sdc.offset_8,32_w.dat</p><pre><code>127.845942908 65054784 65055808127.846185883 65055808 65056832127.952831800 65056832 65057856127.953065986 65057856 65058880127.955207647 65058880 65059904</code></pre><p>第一行为时间，第二个为其实扇区大小，第3行为每个io结束扇区大小。可以算出每个io为1024个扇区大小，既1M 与测试实际相符合。</p><p><strong>3、4k direct=0</strong></p><pre><code>[root@node1 samba-test]# fio -filename=./4k_file_direct0  -direct=0 -iodepth=1 -thread -rw=write -ioengine=libaio -bs=4k -size=3G -numjobs=4  -times=300 -group_reporting -name=mytest2mytest2: (g=0): rw=write, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=1...fio-2.2.5Starting 4 threadsmytest2: Laying out IO file(s) (1 file(s) / 3072MB)</code></pre><ul><li>direct=0后 启动io buffer </li></ul><p>iostat监控<br><img src="https://i.imgur.com/ap1qitG.jpg" alt=""></p><p>blktrace监控数据</p><p>生成数据</p><pre><code>[root@node1 1m]# blkparse -i sdc -d sdc.blktrace.bin</code></pre><p><img src="https://i.imgur.com/wm94VIu.jpg" alt=""></p><p>4k对象大小，在io路径变成1024 既最大512kb字节，很明显进行IO合并，大部分io动作都是Q G I来回循环，即生成io请求到io请求队列，就返回继续申请新的io到队列，整体由内核kworker进行io任务的调度，在内存swapper中完成io写入。W表示异步写。</p><p>分析数据</p><pre><code>[root@node1 1m]# btt -i sdc.blktrace.bin -A | less</code></pre><p><img src="https://i.imgur.com/sVzF5Lq.jpg" alt=""></p><ul><li>Q2G – 生成IO请求所消耗的时间，包括remap和split的时间；<br>本例5.6ms</li><li>G2I – IO请求进入IO Scheduler所消耗的时间，包括merge的时间;<br>0.009ms  没有进行合并。</li><li>I2D – IO请求在IO Scheduler中等待的时间；<br>635 ms</li><li>D2C – IO请求在driver和硬件上所消耗的时间；<br>188 ms</li><li>Q2C – 整个IO请求所消耗的时间(Q2I + I2D + D2C = Q2C)，相当于iostat的await。 为828.6ms与iostat wait值几乎一致。在io scheduler上消耗太多时间。在driver硬件层耗时相对少一些，但是也很大了达到188ms。<br>其中M2D io合并时间平均4.5s.</li></ul><p>生成io大小分布</p><pre><code>btt -i sdc.blktrace.bin -B sdc.offset93.463618386 74670904 74671928   93.463870465 74671928 74672952   93.464140190 74672952 74673976   93.464445965 74673976 74675000   93.464740898 74675000 74676024   93.465027976 74676024 74677048   93.465313376 74677048 74678072</code></pre><p>同样的是以1024个扇区为最小io。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;利用BLKTRACE分析磁盘IO &lt;/p&gt;
&lt;p&gt;在Linux系统上，查看磁盘的负载情况，咱们一般使用iostat监控工具，iostat的详细介绍查看另外的培训资料。其中很重要的参数就是await，await表示单个I/O所需的平均时间，但它同时包含了I/O Scheduler所消耗的时间和硬件所消耗的时间，所以不能作为硬件性能的指标。那如何才能分辨一个io从下发到返回整个时间上，是硬件层耗时多还是在io调度上耗时多呢？如何查看io在各个时间段所消耗的时间呢？那么，blktrace在这种场合就能派上用场，因为它能记录I/O所经历的各个步骤，从中可以分析是IO Scheduler慢还是硬件响应慢，以及各个时间段所用时间。&lt;br&gt;
    
    </summary>
    
    
      <category term="blktrace fio" scheme="http://idcat.cn/tags/blktrace-fio/"/>
    
  </entry>
  
  <entry>
    <title>hadoop2.7.3通过s3对接ceph10.2 radosgw测试</title>
    <link href="http://idcat.cn/2018/04/25/hadoop2-7-3%E9%80%9A%E8%BF%87s3%E5%AF%B9%E6%8E%A5ceph10-2-radosgw%E6%B5%8B%E8%AF%95/"/>
    <id>http://idcat.cn/2018/04/25/hadoop2-7-3通过s3对接ceph10-2-radosgw测试/</id>
    <published>2018-04-25T10:02:18.000Z</published>
    <updated>2018-04-25T10:15:36.977Z</updated>
    
    <content type="html"><![CDATA[<p>公司提出测试需求，将Hadoop2.7与ceph10.2 S3对象存储进行集成测试，hadoop官网介绍：<a href="http://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html" target="_blank" rel="noopener">官网介绍</a><br>后查阅相关资料完成对接测试，现将环境部署，对接测试完整过程，整理如下：<br><a id="more"></a><br><strong>hadoop环境</strong></p><p>2台主机 主机名分别为master slave .  master作为hadoop namenode，slave作为datanode.<br>hadoop集群部署过程参考： <a href="http://www.178pt.com/156.html" target="_blank" rel="noopener">hadoop集群部署</a></p><p>ceph10.2 radosgw配置过程参考：<a href="http://www.178pt.com/250.html" target="_blank" rel="noopener">radosgw配置</a></p><p><strong>hadoop集成s3</strong></p><p>在master（namenode）节点上修改core-site.xml，增加如下配置（endpoint key根据实际填写）：</p><pre><code>&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;&lt;property&gt;    &lt;name&gt;fs.defaultFS&lt;/name&gt;    &lt;value&gt;hdfs://master:9000&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;io.file.buffer.size&lt;/name&gt;    &lt;value&gt;131072&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;    &lt;value&gt;file:/usr/hadoop/tmp&lt;/value&gt;    &lt;description&gt;Abase for other temporary   directories.&lt;/description&gt;&lt;/property&gt;##增加如下内容&lt;property&gt;      &lt;name&gt;fs.s3a.access.key&lt;/name&gt;      &lt;value&gt;YZ8H5J5B4BS4HGJ6U8YC&lt;/value&gt;      &lt;description&gt;AWS access key ID. Omit for Role-based authentication.&lt;/description&gt;&lt;/property&gt;&lt;property&gt;      &lt;name&gt;fs.s3a.secret.key&lt;/name&gt;      &lt;value&gt;KzPrV6ytwoZoQCMHzbnXXMQKrjH5MLnD3Wsb0AjJ&lt;/value&gt;      &lt;description&gt;AWS secret key&lt;/description&gt;&lt;/property&gt;&lt;property&gt;      &lt;name&gt;fs.s3a.endpoint&lt;/name&gt;      &lt;value&gt;192.168.1.31:7480&lt;/value&gt;      &lt;description&gt;AWS S3 endpoint to connect to. An up-to-date list is    provided in the AWS Documentation: regions and endpoints. Without this    property, the standard region (s3.amazonaws.com) is assumed.      &lt;/description&gt;&lt;/property&gt;&lt;property&gt;      &lt;name&gt;fs.s3a.connection.ssl.enabled&lt;/name&gt;      &lt;value&gt;false&lt;/value&gt;      &lt;description&gt;Enables or disables SSL connections to S3.&lt;/description&gt;&lt;/property&gt;##增加结束&lt;/configuration&gt;</code></pre><p>在master  slave 2个hadoop节点上拷贝s3相关的jar包，否则会报错。</p><pre><code>[root@master etc]# pwd/usr/hadoop/hadoop-2.7.3/etc[root@master etc]# cp hadoop/share/hadoop/tools/lib/hadoop-aws-2.7.3.jar hadoop/share/hadoop/common/lib/[root@master etc]# cp hadoop/share/hadoop/tools/lib/aws-java-sdk-1.7.4.jar hadoop/share/hadoop/common/lib/[root@master etc]# cp hadoop/share/hadoop/tools/lib/joda-time-2.9.4.jar hadoop/share/hadoop/common/lib/[root@master etc]# cp hadoop/share/hadoop/tools/lib/jackson-*.jar hadoop/share/hadoop/common/lib/</code></pre><p>重启hadoop</p><pre><code>[root@master etc]# stop-all.sh[root@master etc]# start-all.sh</code></pre><p><strong>hadoop集成s3测试</strong></p><p>ceph 节点上创建桶hadoop,并上传文件</p><pre><code>[root@radosgw1 ~]# s3cmd mb s3://hadoopBucket &apos;s3://hadoop/&apos; created[root@radosgw1 ~]# s3cmd put abc s3://hadoopupload: &apos;abc&apos; -&gt; &apos;s3://hadoop/abc&apos;  [1 of 1]     1109 of 1109   100% in    1s  1096.74 B/s  done[root@radosgw1 ~]# s3cmd ls s3://hadoop2018-04-25 08:47      1109   s3://hadoop/abc</code></pre><p>hadoop master节点上查看</p><pre><code>[root@master ~]# hadoop fs -ls s3a://hadoop/Found 1 items-rw-rw-rw-   1       1109 2018-04-25 16:47 s3a://hadoop/abc</code></pre><p>1、 从hadoop client本机上传文件到对象存储</p><pre><code>[root@master ~]# ls ceshi.txt ceshi.txt[root@master ~]# hadoop fs -put ceshi.txt s3a://hadoop/[root@master ~]# hadoop fs -ls s3a://hadoop/Found 2 items-rw-rw-rw-   1       1109 2018-04-25 16:47 s3a://hadoop/abc-rw-rw-rw-   1       1083 2018-04-25 16:52 s3a://hadoop/ceshi.txt[root@master ~]#</code></pre><p>集群端查看</p><pre><code>[root@radosgw1 ~]# s3cmd ls s3://hadoop2018-04-25 08:47      1109   s3://hadoop/abc2018-04-25 08:52      1083   s3://hadoop/ceshi.txt</code></pre><p>2、 将文件从对象存储下载到本地</p><pre><code> [root@master ~]# rm -f ceshi.txt [root@master ~]# ls ceshi.txtls: cannot access ceshi.txt: No such file or directory[root@master ~]# hadoop fs -get s3a://hadoop/ceshi.txt [root@master ~]# ls ceshi.txtceshi.txt[root@master ~]#</code></pre><p>3、 将文件从对象拷贝到hdfs文件系统</p><pre><code>[root@master ~]# hdfs dfs -ls /Found 4 itemsdrwxr-xr-x   - root supergroup          0 2018-04-25 15:21 /hahadrwxr-xr-x   - root supergroup          0 2018-04-25 12:10 /inputdrwxr-xr-x   - root supergroup          0 2018-04-25 12:11 /outputdrwx------   - root supergroup          0 2018-04-25 12:11 /tmp[root@master ~]# hdfs dfs -ls /ceshi.txtls: `/ceshi.txt&apos;: No such file or directory[root@master ~]# hadoop distcp s3a://hadoop/ceshi.txt /ceshi.txt18/04/25 17:00:10 INFO tools.DistCp: Input Options: DistCpOptions{atomicCommit=false, syncFolder=false, deleteMissing=false, ignoreFailures=false, maxMaps=20, sslConfigurationFile=&apos;null&apos;, copyStrategy=&apos;uniformsize&apos;, sourceFileListing=null, sourcePaths=[s3a://hadoop/ceshi.txt], targetPath=/ceshi.txt, targetPathExists=false, preserveRawXattrs=false}18/04/25 17:00:10 INFO client.RMProxy: Connecting to ResourceManager at master/192.168.1.20:803218/04/25 17:00:30 INFO Configuration.deprecation: io.sort.mb is deprecated. Instead, use mapreduce.task.io.sort.mb18/04/25 17:00:30 INFO Configuration.deprecation: io.sort.factor is deprecated. Instead, use mapreduce.task.io.sort.factor18/04/25 17:00:31 INFO client.RMProxy: Connecting to ResourceManager at master/192.168.1.20:803218/04/25 17:00:31 INFO mapreduce.JobSubmitter: number of splits:118/04/25 17:00:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1524633996089_000918/04/25 17:00:32 INFO impl.YarnClientImpl: Submitted application application_1524633996089_000918/04/25 17:00:32 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1524633996089_0009/18/04/25 17:00:32 INFO tools.DistCp: DistCp job-id: job_1524633996089_000918/04/25 17:00:32 INFO mapreduce.Job: Running job: job_1524633996089_000918/04/25 17:00:40 INFO mapreduce.Job: Job job_1524633996089_0009 running in uber mode : false18/04/25 17:00:40 INFO mapreduce.Job:  map 0% reduce 0%18/04/25 17:00:52 INFO mapreduce.Job:  map 100% reduce 0%18/04/25 17:01:05 INFO mapreduce.Job: Job job_1524633996089_0009 completed successfully18/04/25 17:01:05 INFO mapreduce.Job: Counters: 38File System Counters    FILE: Number of bytes read=0    FILE: Number of bytes written=121596    FILE: Number of read operations=0    FILE: Number of large read operations=0    FILE: Number of write operations=0    HDFS: Number of bytes read=330    HDFS: Number of bytes written=1083    HDFS: Number of read operations=14    HDFS: Number of large read operations=0    HDFS: Number of write operations=4    S3A: Number of bytes read=1083    S3A: Number of bytes written=0    S3A: Number of read operations=3    S3A: Number of large read operations=0    S3A: Number of write operations=0Job Counters     Launched map tasks=1    Other local map tasks=1    Total time spent by all maps in occupied slots (ms)=20780    Total time spent by all reduces in occupied slots (ms)=0    Total time spent by all map tasks (ms)=20780    Total vcore-milliseconds taken by all map tasks=20780    Total megabyte-milliseconds taken by all map tasks=21278720Map-Reduce Framework    Map input records=1    Map output records=0    Input split bytes=135    Spilled Records=0    Failed Shuffles=0    Merged Map outputs=0    GC time elapsed (ms)=154    CPU time spent (ms)=1350    Physical memory (bytes) snapshot=113676288    Virtual memory (bytes) snapshot=862224384    Total committed heap usage (bytes)=29032448File Input Format Counters     Bytes Read=195File Output Format Counters     Bytes Written=0org.apache.hadoop.tools.mapred.CopyMapper$Counter    BYTESCOPIED=1083    BYTESEXPECTED=1083    COPY=1[root@master ~]# hdfs dfs -ls /ceshi.txt-rw-r--r--   1 root supergroup       1083 2018-04-25 17:00 /ceshi.txt[root@master ~]# </code></pre><p>4、  将文件从HDFS文件系统拷贝到s3对象存储中</p><pre><code>s3对象列出所有文件[root@radosgw1 ~]# s3cmd ls s3://hadoop2018-04-25 08:47      1109   s3://hadoop/abc2018-04-25 08:52      1083   s3://hadoop/ceshi.txt[root@radosgw1 ~]# 将hdfs文件系统下的/haha目录中anaconda-ks.cfg文件传到s3对象存储里面[root@master ~]# hdfs dfs -ls /hahaFound 1 items-rw-r--r--   1 root supergroup       1083 2018-04-25 15:21 /haha/anaconda-ks.cfg[root@master ~]# hadoop distcp /haha/anaconda-ks.cfg s3a://hadoop/18/04/25 17:06:18 INFO tools.DistCp: Input Options: DistCpOptions{atomicCommit=false, syncFolder=false, deleteMissing=false, ignoreFailures=false, maxMaps=20, sslConfigurationFile=&apos;null&apos;, copyStrategy=&apos;uniformsize&apos;, sourceFileListing=null, sourcePaths=[/haha/anaconda-ks.cfg], targetPath=s3a://hadoop/, targetPathExists=true, preserveRawXattrs=false}18/04/25 17:06:18 INFO client.RMProxy: Connecting to ResourceManager at master/192.168.1.20:803218/04/25 17:06:24 INFO Configuration.deprecation: io.sort.mb is deprecated. Instead, use mapreduce.task.io.sort.mb18/04/25 17:06:24 INFO Configuration.deprecation: io.sort.factor is deprecated. Instead, use mapreduce.task.io.sort.factor18/04/25 17:06:25 INFO client.RMProxy: Connecting to ResourceManager at master/192.168.1.20:803218/04/25 17:06:26 INFO mapreduce.JobSubmitter: number of splits:118/04/25 17:06:26 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1524633996089_001018/04/25 17:06:26 INFO impl.YarnClientImpl: Submitted application application_1524633996089_001018/04/25 17:06:26 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1524633996089_0010/18/04/25 17:06:26 INFO tools.DistCp: DistCp job-id: job_1524633996089_001018/04/25 17:06:26 INFO mapreduce.Job: Running job: job_1524633996089_001018/04/25 17:06:35 INFO mapreduce.Job: Job job_1524633996089_0010 running in uber mode : false18/04/25 17:06:35 INFO mapreduce.Job:  map 0% reduce 0%18/04/25 17:06:57 INFO mapreduce.Job:  map 100% reduce 0%18/04/25 17:08:14 INFO mapreduce.Job: Job job_1524633996089_0010 completed successfully18/04/25 17:08:14 INFO mapreduce.Job: Counters: 38File System Counters    FILE: Number of bytes read=0    FILE: Number of bytes written=121562    FILE: Number of read operations=0    FILE: Number of large read operations=0    FILE: Number of write operations=0    HDFS: Number of bytes read=1459    HDFS: Number of bytes written=0    HDFS: Number of read operations=10    HDFS: Number of large read operations=0    HDFS: Number of write operations=2    S3A: Number of bytes read=0    S3A: Number of bytes written=1083    S3A: Number of read operations=11    S3A: Number of large read operations=0    S3A: Number of write operations=3Job Counters     Launched map tasks=1    Other local map tasks=1    Total time spent by all maps in occupied slots (ms)=86489    Total time spent by all reduces in occupied slots (ms)=0    Total time spent by all map tasks (ms)=86489    Total vcore-milliseconds taken by all map tasks=86489    Total megabyte-milliseconds taken by all map tasks=88564736Map-Reduce Framework    Map input records=1    Map output records=0    Input split bytes=134    Spilled Records=0    Failed Shuffles=0    Merged Map outputs=0    GC time elapsed (ms)=151    CPU time spent (ms)=1760    Physical memory (bytes) snapshot=116514816    Virtual memory (bytes) snapshot=863125504    Total committed heap usage (bytes)=29032448File Input Format Counters     Bytes Read=242File Output Format Counters     Bytes Written=0org.apache.hadoop.tools.mapred.CopyMapper$Counter    BYTESCOPIED=1083    BYTESEXPECTED=1083    COPY=1[root@master ~]# s3集群端验证[root@radosgw1 ~]# s3cmd ls s3://hadoop2018-04-25 08:47      1109   s3://hadoop/abc2018-04-25 09:08      1083   s3://hadoop/anaconda-ks.cfg2018-04-25 08:52      1083   s3://hadoop/ceshi.txt[root@radosgw1 ~]# </code></pre><p>5、 将对象存储中的文件作为mapreduce的输入，进行计算之后将结果输出到hdfs文件系统中。</p><pre><code>将对象存储中的/hadoop/abc文件作为mapreduce的文件输入，计算结果输出到hdfs的/result目录    [root@master ~]# hadoop fs -ls s3a://hadoop/Found 3 items-rw-rw-rw-   1       1109 2018-04-25 16:47 s3a://hadoop/abc-rw-rw-rw-   1       1083 2018-04-25 17:08 s3a://hadoop/anaconda-ks.cfg-rw-rw-rw-   1       1083 2018-04-25 16:52 s3a://hadoop/ceshi.txt[root@master ~]# hdfs dfs -ls /resultls: `/result&apos;: No such file or directory[root@master ~]#</code></pre><p>当前hdfs是没有/result目录的，下面进行计算操作</p><pre><code>[root@master ~]# hadoop jar /usr/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount s3a://hadoop/abc /result18/04/25 17:19:53 INFO client.RMProxy: Connecting to ResourceManager at master/192.168.1.20:803218/04/25 17:19:55 INFO input.FileInputFormat: Total input paths to process : 118/04/25 17:19:56 INFO mapreduce.JobSubmitter: number of splits:118/04/25 17:19:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1524633996089_001118/04/25 17:19:57 INFO impl.YarnClientImpl: Submitted application application_1524633996089_001118/04/25 17:19:57 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1524633996089_0011/18/04/25 17:19:57 INFO mapreduce.Job: Running job: job_1524633996089_001118/04/25 17:20:06 INFO mapreduce.Job: Job job_1524633996089_0011 running in uber mode : false18/04/25 17:20:06 INFO mapreduce.Job:  map 0% reduce 0%18/04/25 17:20:23 INFO mapreduce.Job:  map 100% reduce 0%18/04/25 17:20:31 INFO mapreduce.Job:  map 100% reduce 100%18/04/25 17:20:32 INFO mapreduce.Job: Job job_1524633996089_0011 completed successfully18/04/25 17:20:32 INFO mapreduce.Job: Counters: 54File System Counters    FILE: Number of bytes read=1442    FILE: Number of bytes written=240937    FILE: Number of read operations=0    FILE: Number of large read operations=0    FILE: Number of write operations=0    HDFS: Number of bytes read=81    HDFS: Number of bytes written=1121    HDFS: Number of read operations=5    HDFS: Number of large read operations=0    HDFS: Number of write operations=2    S3A: Number of bytes read=1109    S3A: Number of bytes written=0    S3A: Number of read operations=1    S3A: Number of large read operations=0    S3A: Number of write operations=0Job Counters     Launched map tasks=1    Launched reduce tasks=1    Rack-local map tasks=1    Total time spent by all maps in occupied slots (ms)=14541    Total time spent by all reduces in occupied slots (ms)=5450    Total time spent by all map tasks (ms)=14541    Total time spent by all reduce tasks (ms)=5450    Total vcore-milliseconds taken by all map tasks=14541    Total vcore-milliseconds taken by all reduce tasks=5450    Total megabyte-milliseconds taken by all map tasks=14889984    Total megabyte-milliseconds taken by all reduce tasks=5580800Map-Reduce Framework    Map input records=43    Map output records=104    Map output bytes=1517    Map output materialized bytes=1442    Input split bytes=81    Combine input records=104    Combine output records=79    Reduce input groups=79    Reduce shuffle bytes=1442    Reduce input records=79    Reduce output records=79    Spilled Records=158    Shuffled Maps =1    Failed Shuffles=0    Merged Map outputs=1    GC time elapsed (ms)=230    CPU time spent (ms)=2230    Physical memory (bytes) snapshot=324866048    Virtual memory (bytes) snapshot=1723260928    Total committed heap usage (bytes)=162926592Shuffle Errors    BAD_ID=0    CONNECTION=0    IO_ERROR=0    WRONG_LENGTH=0    WRONG_MAP=0    WRONG_REDUCE=0File Input Format Counters     Bytes Read=1109File Output Format Counters     Bytes Written=1121[root@master ~]#</code></pre><p>计算成功后，查看hdfs目录，下面可以看到目录存在，且计算结果文件也存在，且返回SUCCESS</p><pre><code>[root@master ~]# hdfs dfs -ls /resultFound 2 items-rw-r--r--   1 root supergroup          0 2018-04-25 17:20 /result/_SUCCESS-rw-r--r--   1 root supergroup       1121 2018-04-25 17:20 /result/part-r-00000[root@master ~]# </code></pre><p>6、 将对象存储中的文件作为mapreduce的输入，进行计算之后将结果输出到对象存储桶中。</p><pre><code>首先查看对象存储桶中hadoop下result目录是否存在。待会输出结果会传到这里。[root@master ~]# hadoop fs -ls s3a://hadoop/resultls: `s3a://hadoop/result&apos;: No such file or directory[root@master ~]# hadoop jar /usr/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount s3a://hadoop/abc s3a://hadoop/result18/04/25 17:25:27 INFO client.RMProxy: Connecting to ResourceManager at master/192.168.1.20:803218/04/25 17:25:39 INFO input.FileInputFormat: Total input paths to process : 118/04/25 17:25:40 INFO mapreduce.JobSubmitter: number of splits:118/04/25 17:25:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1524633996089_001218/04/25 17:25:41 INFO impl.YarnClientImpl: Submitted application application_1524633996089_001218/04/25 17:25:41 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1524633996089_0012/18/04/25 17:25:41 INFO mapreduce.Job: Running job: job_1524633996089_001218/04/25 17:25:53 INFO mapreduce.Job: Job job_1524633996089_0012 running in uber mode : false18/04/25 17:25:53 INFO mapreduce.Job:  map 0% reduce 0%18/04/25 17:26:57 INFO mapreduce.Job:  map 100% reduce 0%18/04/25 17:27:18 INFO mapreduce.Job:  map 100% reduce 67%18/04/25 17:27:27 INFO mapreduce.Job:  map 100% reduce 100%18/04/25 17:32:44 INFO mapreduce.Job: Job job_1524633996089_0012 completed successfully18/04/25 17:32:44 INFO mapreduce.Job: Counters: 54File System Counters    FILE: Number of bytes read=1442    FILE: Number of bytes written=240925    FILE: Number of read operations=0    FILE: Number of large read operations=0    FILE: Number of write operations=0    HDFS: Number of bytes read=81    HDFS: Number of bytes written=0    HDFS: Number of read operations=1    HDFS: Number of large read operations=0    HDFS: Number of write operations=0    S3A: Number of bytes read=1109    S3A: Number of bytes written=1121    S3A: Number of read operations=19    S3A: Number of large read operations=0    S3A: Number of write operations=5Job Counters     Launched map tasks=1    Launched reduce tasks=1    Rack-local map tasks=1    Total time spent by all maps in occupied slots (ms)=22928    Total time spent by all reduces in occupied slots (ms)=198775    Total time spent by all map tasks (ms)=22928    Total time spent by all reduce tasks (ms)=198775    Total vcore-milliseconds taken by all map tasks=22928    Total vcore-milliseconds taken by all reduce tasks=198775    Total megabyte-milliseconds taken by all map tasks=23478272    Total megabyte-milliseconds taken by all reduce tasks=203545600Map-Reduce Framework    Map input records=43    Map output records=104    Map output bytes=1517    Map output materialized bytes=1442    Input split bytes=81    Combine input records=104    Combine output records=79    Reduce input groups=79    Reduce shuffle bytes=1442    Reduce input records=79    Reduce output records=79    Spilled Records=158    Shuffled Maps =1    Failed Shuffles=0    Merged Map outputs=1    GC time elapsed (ms)=256    CPU time spent (ms)=1550    Physical memory (bytes) snapshot=336670720    Virtual memory (bytes) snapshot=1724592128    Total committed heap usage (bytes)=162926592Shuffle Errors    BAD_ID=0    CONNECTION=0    IO_ERROR=0    WRONG_LENGTH=0    WRONG_MAP=0    WRONG_REDUCE=0File Input Format Counters     Bytes Read=1109File Output Format Counters     Bytes Written=1121[root@master ~]# </code></pre><p>成功后，验证查看</p><pre><code>hadoop节点验证[root@master ~]# hadoop fs -ls s3a://hadoop/resultFound 2 items-rw-rw-rw-   1          0 2018-04-25 17:33 s3a://hadoop/result/_SUCCESS-rw-rw-rw-   1       1121 2018-04-25 17:32 s3a://hadoop/result/part-r-00000[root@master ~]# ceph集群节点验证[root@radosgw1 ~]# s3cmd ls s3://hadoop/result/2018-04-25 09:33         0   s3://hadoop/result/_SUCCESS2018-04-25 09:32      1121   s3://hadoop/result/part-r-00000</code></pre><p>7、 将 HDFS 中的文件作为 MapReduce 的输入，计算结果输出到对象存储的存储空间中</p><pre><code>下面将hdfs中ceshi.txt作为计算输入，将结果输出对象存储中hadoop/output目录中。 前期查看[root@master ~]# hdfs dfs -ls /Found 6 items-rw-r--r--   1 root supergroup       1083 2018-04-25 17:00 /ceshi.txt[root@master ~]# hadoop fs -ls s3a://hadoop/outputls: `s3a://hadoop/output&apos;: No such file or directory开始计算并输出[root@master ~]# hadoop jar /usr/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount /ceshi.txt  s3a://hadoop/output18/04/25 17:39:55 INFO client.RMProxy: Connecting to ResourceManager at master/192.168.1.20:803218/04/25 17:40:04 INFO input.FileInputFormat: Total input paths to process : 118/04/25 17:40:05 INFO mapreduce.JobSubmitter: number of splits:118/04/25 17:40:05 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1524633996089_001318/04/25 17:40:06 INFO impl.YarnClientImpl: Submitted application application_1524633996089_001318/04/25 17:40:06 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1524633996089_0013/18/04/25 17:40:06 INFO mapreduce.Job: Running job: job_1524633996089_001318/04/25 17:40:19 INFO mapreduce.Job: Job job_1524633996089_0013 running in uber mode : false18/04/25 17:40:19 INFO mapreduce.Job:  map 0% reduce 0%18/04/25 17:41:16 INFO mapreduce.Job:  map 100% reduce 0%18/04/25 17:41:36 INFO mapreduce.Job:  map 100% reduce 67%18/04/25 17:41:45 INFO mapreduce.Job:  map 100% reduce 100%18/04/25 17:46:38 INFO mapreduce.Job: Job job_1524633996089_0013 completed successfully18/04/25 17:46:38 INFO mapreduce.Job: Counters: 54File System Counters    FILE: Number of bytes read=1404    FILE: Number of bytes written=240873    FILE: Number of read operations=0    FILE: Number of large read operations=0    FILE: Number of write operations=0    HDFS: Number of bytes read=1176    HDFS: Number of bytes written=0    HDFS: Number of read operations=2    HDFS: Number of large read operations=0    HDFS: Number of write operations=0    S3A: Number of bytes read=0    S3A: Number of bytes written=1091    S3A: Number of read operations=18    S3A: Number of large read operations=0    S3A: Number of write operations=5Job Counters     Launched map tasks=1    Launched reduce tasks=1    Data-local map tasks=1    Total time spent by all maps in occupied slots (ms)=17302    Total time spent by all reduces in occupied slots (ms)=173487    Total time spent by all map tasks (ms)=17302    Total time spent by all reduce tasks (ms)=173487    Total vcore-milliseconds taken by all map tasks=17302    Total vcore-milliseconds taken by all reduce tasks=173487    Total megabyte-milliseconds taken by all map tasks=17717248    Total megabyte-milliseconds taken by all reduce tasks=177650688Map-Reduce Framework    Map input records=41    Map output records=102    Map output bytes=1483    Map output materialized bytes=1404    Input split bytes=93    Combine input records=102    Combine output records=77    Reduce input groups=77    Reduce shuffle bytes=1404    Reduce input records=77    Reduce output records=77    Spilled Records=154    Shuffled Maps =1    Failed Shuffles=0    Merged Map outputs=1    GC time elapsed (ms)=261    CPU time spent (ms)=1570    Physical memory (bytes) snapshot=325062656    Virtual memory (bytes) snapshot=1724448768    Total committed heap usage (bytes)=162926592Shuffle Errors    BAD_ID=0    CONNECTION=0    IO_ERROR=0    WRONG_LENGTH=0    WRONG_MAP=0    WRONG_REDUCE=0File Input Format Counters     Bytes Read=1083File Output Format Counters     Bytes Written=1091</code></pre><p>在hadoop节点和集群节点验证</p><pre><code>[root@master ~]# hadoop fs -ls s3a://hadoop/outputFound 2 items-rw-rw-rw-   1          0 2018-04-25 17:47 s3a://hadoop/output/_SUCCESS-rw-rw-rw-   1       1091 2018-04-25 17:46 s3a://hadoop/output/part-r-00000[root@master ~]# [root@radosgw1 ~]# s3cmd ls s3://hadoop/output/2018-04-25 09:47         0   s3://hadoop/output/_SUCCESS2018-04-25 09:46      1091   s3://hadoop/output/part-r-00000[root@radosgw1 ~]# </code></pre><p>可以看到集群端和hadoop节点端都能看到。</p><p>至此，配置测试结束。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;公司提出测试需求，将Hadoop2.7与ceph10.2 S3对象存储进行集成测试，hadoop官网介绍：&lt;a href=&quot;http://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官网介绍&lt;/a&gt;&lt;br&gt;后查阅相关资料完成对接测试，现将环境部署，对接测试完整过程，整理如下：&lt;br&gt;
    
    </summary>
    
    
      <category term="ceph10.2 radosgw  hadoop" scheme="http://idcat.cn/tags/ceph10-2-radosgw-hadoop/"/>
    
  </entry>
  
  <entry>
    <title>nginx反向代理和负载均衡最基础配置实现</title>
    <link href="http://idcat.cn/2018/04/24/nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E5%92%8C%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E6%9C%80%E5%9F%BA%E7%A1%80%E9%85%8D%E7%BD%AE%E5%AE%9E%E7%8E%B0/"/>
    <id>http://idcat.cn/2018/04/24/nginx反向代理和负载均衡最基础配置实现/</id>
    <published>2018-04-24T09:39:44.000Z</published>
    <updated>2018-04-24T09:40:56.474Z</updated>
    
    <content type="html"><![CDATA[<p>本文仅在最基础的nginx环境下做反向代理和负载均衡的大概配置介绍，具体调优以及其他功能暂不考虑。仅仅展现这2个功能的基本实现配置，本文记录仅供参考。</p><p><strong>环境：</strong></p><p>单台centos7的虚拟机，nginx安装使用yum进行安装。默认nginx配置文件在/etc/nginx/ 下 nginx.conf文件。<br><a id="more"></a></p><p><strong>默认配置</strong></p><p>备份默认配置文件</p><pre><code>[root@node1 ~]# cp /etc/nginx/nginx.conf{,_bak}</code></pre><p>清空注释内容，使配置文件更加清晰</p><pre><code>[root@node1 ~]# sed -i &apos;/^#/d&apos; /etc/nginx/nginx.conf</code></pre><p>查看默认配置</p><pre><code>[root@node1 ~]# cat /etc/nginx/nginx.conf_bak  |grep -v ^# | grep -v ^$user nginx;worker_processes auto;error_log /var/log/nginx/error.log;pid /run/nginx.pid;include /usr/share/nginx/modules/*.conf;events {    worker_connections 1024;}http {    log_format  main  &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos;                  &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos;                  &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;;    access_log  /var/log/nginx/access.log  main;    sendfile            on;    tcp_nopush          on;    tcp_nodelay         on;    keepalive_timeout   65;    types_hash_max_size 2048;    include             /etc/nginx/mime.types;    default_type        application/octet-stream;    include /etc/nginx/conf.d/*.conf;    server {        listen       80 default_server;        listen       [::]:80 default_server;        server_name  _;        root         /usr/share/nginx/html;        include /etc/nginx/default.d/*.conf;        location / {        }           error_page 404 /404.html;            location = /40x.html {        }        error_page 500 502 503 504 /50x.html;            location = /50x.html {        }    }}</code></pre><p>简单介绍：</p><p>http模块下面包括server 等</p><ul><li><p>listen server监听端口</p></li><li><p>server_name 通过什么域名进行访问。本例为空即本机IP</p></li><li><p>root 定义服务器网站根目录默认位置</p></li><li><p>location 通用匹配，任何未匹配到其它location的请求都会匹配到这里</p></li></ul><p>修改默认配置下主页内容</p><pre><code>[root@node1 nginx]# cat /usr/share/nginx/html/index.html this is example</code></pre><p>访问测试</p><pre><code>[root@node1 ~]# curl node1this is example</code></pre><p><strong>反向代理</strong></p><p>例如客户端A访问服务端B ，然而B实际上访问了服务端C，然后返回结果给B，在返回结果给A。至始至终，客户端A是不知道C的存在的，只会认为访问B并得到返回。这样可以很好的保护了实际WEB服务端C。</p><p>修改默认配置文件nginx.conf </p><pre><code>server {    listen       80 default_server;    listen       [::]:80 default_server;    server_name  -;    root         /usr/share/nginx/html;    include /etc/nginx/default.d/*.conf;    location /{            proxy_pass http://node1:8081;    }}    server {            listen       8081 default_server;            root         /usr/share/nginx/html_1;            server_name  -;            index   index.html;            location / {                    }    }}</code></pre><p>在原有server下面增加了一个server，即新的虚拟机，后台默认端口为8081，默认root为html_ 1，该目录是从默认html 拷贝过来的，修改默认index.html内容为8081。然后在默认80端口的虚拟机上增加proxy_pass <a href="http://node1:8081" target="_blank" rel="noopener">http://node1:8081</a>; 表示客户端访问80端口的时候，其实指向了另外的虚拟机的8081端口上。修改后重载nginx。</p><pre><code>[root@node1 ~]# systemctl reload nginx[root@node1 ~]# cat /usr/share/nginx/html_1/index.html 8081</code></pre><p>访问测试</p><pre><code>[root@node1 html_1]# curl node18081</code></pre><p>已经配置成功了。本来访问80端口的下index.html应该是this is example ，现在返回的是8081虚拟机的index.html内容。</p><p><strong>负载均衡</strong></p><p>单台web服务器受限于其资源限制，提供对外访问连接毕竟有限。增加多台nginx后端web服务器同时提供访问，可增加并发性能，以及提高可拓展性。nginx负载均衡支持客户端访问请求根据策略分摊到后端实际的某台web服务器上。</p><p>修改默认配置文件nginx.conf </p><pre><code>##在http下面增加upstream配置    upstream my_http { #默认轮训策略，还有ip_hash ，weight，最小连接数等算法。    server 192.168.1.141:8081;    server 192.168.1.141:8082;    server 192.168.1.141:8083;  }server {    listen       80 default_server;    listen       [::]:80 default_server;    server_name  -;    root         /usr/share/nginx/html;    include /etc/nginx/default.d/*.conf;    location /{            proxy_pass http://my_http;    }}    ##本机起3个虚拟机，分别使用不同的端口号，模拟多台server服务端。root定义网站路径，index指定默认网页    server {            listen       8081 ;            root         /usr/share/nginx/html_1;            server_name  -;            index   index.html;            location / {                    }    }    server {            listen       8082;            root         /usr/share/nginx/html_2;            server_name  -;            index   index.html;            location / {                    }    }    server {            listen       8083;            root         /usr/share/nginx/html_3;            server_name  -;            index   index.html;            location / {                    }    }</code></pre><p>在原有server下面增加了一个3个server，即3台新的虚拟机，后台默认端口分别为8081、8082、8083，默认root为html_ 1，html_ 2，html_ 3。这些目录是从默认html 拷贝过来的，分别修改默认index.html内容为8081，8082，8083 然后在默认80端口的虚拟机上增加proxy_ pass <a href="http://my_http" target="_blank" rel="noopener">http://my_http</a>;   即http:// + upstream 名称 ；  表示客户端每次访问80端口的时候，其实指向了另外3台虚拟机。修改后重载nginx。</p><pre><code>[root@node1 ~]# cat /usr/share/nginx/html_1/index.html 8081[root@node1 ~]# cat /usr/share/nginx/html_2/index.html 8082[root@node1 ~]# cat /usr/share/nginx/html_3/index.html 8083[root@node1 ~]# systemctl reload nginx</code></pre><p>访问测试</p><pre><code>[root@node1 ~]# curl node18081[root@node1 ~]# curl node18082[root@node1 ~]# curl node18083</code></pre><p>已经配置成功了。本来访问80端口的下index.html应该是this is example ，现在客户端每次访问80端口，依次返回的是后端3台虚拟机的index.html内容。</p><p>记录到此。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文仅在最基础的nginx环境下做反向代理和负载均衡的大概配置介绍，具体调优以及其他功能暂不考虑。仅仅展现这2个功能的基本实现配置，本文记录仅供参考。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;环境：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;单台centos7的虚拟机，nginx安装使用yum进行安装。默认nginx配置文件在/etc/nginx/ 下 nginx.conf文件。&lt;br&gt;
    
    </summary>
    
    
      <category term="nginx" scheme="http://idcat.cn/tags/nginx/"/>
    
  </entry>
  
  <entry>
    <title>tdb文件简介</title>
    <link href="http://idcat.cn/2018/04/23/tdb%E6%96%87%E4%BB%B6%E7%AE%80%E4%BB%8B/"/>
    <id>http://idcat.cn/2018/04/23/tdb文件简介/</id>
    <published>2018-04-23T09:50:57.000Z</published>
    <updated>2018-04-23T09:53:02.997Z</updated>
    
    <content type="html"><![CDATA[<p><strong>TDB文件介绍</strong></p><p>samba在运行时，Samba 存储许多信息，从本地密码到希望从中收到信息的一系列客户端。这类数据其中一些是暂时的，在 Samba 重启时可能会被丢弃，但是另一些却是永久的，不会被丢弃。这类数据可能是很大的，也可能是不经常访问只是在内存中保留，或者在重启时保持存在。要满足这些要求，Samba 团队创建了 Trivial Database。它实际上是一个键值存储，这意味着数据通过惟一键的方式存储和检索，且没有像在关系数据库中那样的表联接。键值存储 — 尤其是 TDB — 被设计成将数据存储到磁盘并将其取回的一种快速方式。<br><a id="more"></a></p><p>查看samba下tdb文件，只列出/var/lib/samba下面的，还有很多其他目录存在samba tdb文件。</p><pre><code>[root@node1 samba]# cd /var/lib/samba/[root@node1 samba]# lltotal 2236-rw------- 1 root root   421888 Apr 23 15:10 account_policy.tdbdrwxr-xr-x 1 root root        0 Nov 28 00:21 drivers-rw-r--r-- 1 root root   425984 Apr 23 15:10 gencache.tdb-rw------- 1 root root      696 Apr 23 15:10 group_mapping.tdbdrwxr-xr-x 1 root root      456 Apr 23 15:10 lockdrwxr-xr-x 1 root root        0 Apr 23 15:10 printingdrwx------ 1 root root       86 Apr 23 17:21 private-rw------- 1 root root   528384 Apr 23 15:10 registry.tdb-rw------- 1 root root   421888 Apr 23 15:10 share_info.tdb-rw-r--r-- 1 root root   483328 Apr 23 17:43 smbprofile.tdbdrwxr-x--- 1 root wbpriv      0 Nov 28 00:21 winbindd_privileged</code></pre><p>至于这些tdb文件如何查看数据，以及修改备份，下面介绍samba自带的几个tdb工具。</p><p><strong>tdbtool工具介绍</strong></p><p>tdbtool工具可以在命令行上接受命令，也可以打开交互式控制台类似shell一样。要在命令行上完成任务，请运行 tdbtool example.tdb command options，其中 example.tdb 是文件名，command 是命令，针对命令的选项位于最后。要使用 tdb shell，只需单独运行 tdbtool 或在命令行上传递文件的名称。个人建议使用交互式控制台方式。以下是tdbtool参数介绍</p><pre><code>tdbtool:   create    dbname     : create a database  open      dbname     : open an existing database  transaction_start    : start a transaction  transaction_commit   : commit a transaction  transaction_cancel   : cancel a transaction  erase                : erase the database  dump                 : dump the database as strings  keys                 : dump the database keys as strings  hexkeys              : dump the database keys as hex values info                 : print summary info about the database  insert    key  data  : insert a record  move      key  file  : move a record to a destination tdb  storehex  key  data  : store a record (replace), key/value in hex format  store     key  data  : store a record (replace)  show      key        : show a record by key  delete    key        : delete a record by key  list                 : print the database hash table and freelist  free                 : print the database freelist  freelist_size        : print the number of records in the freelist  check                : check the integrity of an opened database  repack               : repack the database  speed                : perform speed tests on the database  ! command            : execute system command  1 | first            : print the first record  n | next             : print the next record  q | quit             : terminate  \n                   : repeat &apos;next&apos; command</code></pre><p>下面分别介绍：</p><p>1、创建数据库</p><pre><code>[root@node1 tdbtest]# tdbtool tdb&gt; create hello[root@node1 tdbtest]# lltotal 4-rw------- 1 root root 696 Apr 23 15:53 hello</code></pre><p>2、打开数据库</p><pre><code>tdb&gt; open hello</code></pre><p>3、插入数据</p><pre><code>tdb&gt; insert name zhangsan</code></pre><p>4、查询数据</p><pre><code>tdb&gt; show namekey 4 bytesnamedata 8 bytes[000] 7A 68 61 6E 67 73 61 6E                           zhangsan </code></pre><p>5、查看所有数据</p><pre><code>tdb&gt; dumpkey 5 bytesname1data 4 bytes[000] 6C 69 73 69                                       lisi key 4 bytesnamedata 8 bytes[000] 7A 68 61 6E 67 73 61 6E                           zhangsan </code></pre><p>总共2条KEY/VALUES键值对，既2条数据信息。</p><p>6、列出key值</p><pre><code>tdb&gt; keyskey 5 bytes: name1key 4 bytes: name</code></pre><p>7、修改values值</p><pre><code>tdb&gt; store name zhangStoring key:key 4 bytesnamedata 5 bytes[000] 7A 68 61 6E 67                                    zhang </code></pre><p>将name值由zhangsan 修改为zhang,查看修改结果</p><pre><code>tdb&gt; dump    key 5 bytesname1data 4 bytes[000] 77 61 6E 67 77 75                                 lisi key 4 bytesnamedata 5 bytes[000] 7A 68 61 6E 67                                    zhang </code></pre><p>8、删除某个key值</p><pre><code>tdb&gt; delete nametdb&gt; dumpkey 5 bytesname1data 4 bytes[000] 6C 69 73 69                                       lisi </code></pre><p>将key值为name的删掉后，查看只剩下name1记录。</p><p>9、检查数据完整性</p><pre><code>tdb&gt; checkDatabase integrity is OK and has 2 records.</code></pre><p>10、复制数据到另外的数据库（后者数据库必须存在）</p><pre><code>tdb&gt; move name2 hello1key 5 bytesname2    data 6 bytes[000] 77 61 6E 67 77 75                                 wangwu record moved</code></pre><p>查看hello1记录</p><pre><code>tdb&gt; open hello1tdb&gt; dumpkey 5 bytesname2data 6 bytes[000] 77 61 6E 67 77 75                                 wangwu </code></pre><p>11、执行系统命令</p><pre><code>tdb&gt; ! pwd/root/tdbtesttdb&gt; ! dateMon Apr 23 16:36:18 CST 2018</code></pre><p>12、支持事务处理</p><p>开启事务</p><pre><code>tdb&gt; transaction_starttdb&gt; insert name3 testtdb&gt; show name3key 5 bytesname3data 4 bytes[000] 74 65 73 74                                       test </code></pre><p>取消事务</p><pre><code>tdb&gt; transaction_canceltdb&gt; show name3fetch failed</code></pre><p>提交事务</p><pre><code>tdb&gt; transaction_starttdb&gt; insert name3 testtdb&gt; transaction_committdb&gt; show name3key 5 bytesname3data 4 bytes[000] 74 65 73 74                                       test</code></pre><p><strong>tdbdump 工具介绍</strong></p><p>tdbdump是用来查看tdb文件中的所有键值对数据的工具</p><p>已hello为例， 查看所有数据</p><pre><code>[root@node1 tdbtest]# tdbdump hello{key(5) = &quot;name1&quot;data(4) = &quot;lisi&quot;}{key(5) = &quot;name2&quot;data(6) = &quot;wangwu&quot;}{key(5) = &quot;name3&quot;data(4) = &quot;test&quot;}</code></pre><p>每个键值对数据key data 数字为字节数</p><p><strong>tdbbackup 工具介绍</strong></p><p>tdbbackup工具为tdb数据库文件的备份工具。</p><ul><li><p>备份hello数据库</p><p>  [root@node1 tdbtest]# tdbbackup hello<br>  [root@node1 tdbtest]# ll<br>  total 828<br>  -rw——- 1 root root 831488 Apr 23 16:42 hello<br>  -rw——- 1 root root   8192 Apr 23 16:38 hello1<br>  -rw——- 1 root root   8192 Apr 23 17:25 hello.bak<br>hello.bak就是备份文件。这里发现两者文件大小不一样，通过md5对比。因为是不同的文件，文件MD5值肯定是不一样的，但是文件内容是完全一样的。</p></li></ul><p>查看文件md5</p><pre><code>[root@node1 tdbtest]# md5sum hello8c55e7dabbeab30e3cd96e96b59fb052  hello[root@node1 tdbtest]# md5sum hello.bak c20b4f9b01f5715bbec8f950cf394f51  hello.bak</code></pre><p>查看文件内容md5</p><pre><code>[root@node1 tdbtest]# tdbdump hello | md5sum 88be32a888d3cd63132e09a0de8d69de  -[root@node1 tdbtest]# tdbdump hello.bak | md5sum 88be32a888d3cd63132e09a0de8d69de  -</code></pre><ul><li>恢复hello数据</li></ul><p>模拟删除数据</p><pre><code>[root@node1 tdbtest]# lltotal 828-rw------- 1 root root 831488 Apr 23 16:42 hello-rw------- 1 root root   8192 Apr 23 16:38 hello1-rw------- 1 root root   8192 Apr 23 17:25 hello.bak[root@node1 tdbtest]# &gt;hello[root@node1 tdbtest]# lltotal 16-rw------- 1 root root    0 Apr 23 17:33 hello-rw------- 1 root root 8192 Apr 23 16:38 hello1-rw------- 1 root root 8192 Apr 23 17:25 hello.bak[root@node1 tdbtest]# tdbbackup -v hellorestoring hello[root@node1 tdbtest]# lltotal 24-rw------- 1 root root 8192 Apr 23 17:33 hello-rw------- 1 root root 8192 Apr 23 16:38 hello1-rw------- 1 root root 8192 Apr 23 17:25 hello.bak</code></pre><p>看到文件大小一致了，现在对比md5值</p><pre><code>[root@node1 tdbtest]# md5sum helloc20b4f9b01f5715bbec8f950cf394f51  hello[root@node1 tdbtest]# md5sum hello.bak c20b4f9b01f5715bbec8f950cf394f51  hello.bak[root@node1 tdbtest]# tdbdump hello |md5sum 88be32a888d3cd63132e09a0de8d69de  -[root@node1 tdbtest]# tdbdump hello.bak |md5sum 88be32a888d3cd63132e09a0de8d69de  -</code></pre><p>看到MD5值与之前备份之前一致了。查看数据</p><pre><code>[root@node1 tdbtest]# tdbdump hello{key(5) = &quot;name1&quot;data(4) = &quot;lisi&quot;}{key(5) = &quot;name2&quot;data(6) = &quot;wangwu&quot;}{key(5) = &quot;name3&quot;data(4) = &quot;test&quot;}</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;TDB文件介绍&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;samba在运行时，Samba 存储许多信息，从本地密码到希望从中收到信息的一系列客户端。这类数据其中一些是暂时的，在 Samba 重启时可能会被丢弃，但是另一些却是永久的，不会被丢弃。这类数据可能是很大的，也可能是不经常访问只是在内存中保留，或者在重启时保持存在。要满足这些要求，Samba 团队创建了 Trivial Database。它实际上是一个键值存储，这意味着数据通过惟一键的方式存储和检索，且没有像在关系数据库中那样的表联接。键值存储 — 尤其是 TDB — 被设计成将数据存储到磁盘并将其取回的一种快速方式。&lt;br&gt;
    
    </summary>
    
    
      <category term="tdbdump  tdbbakcup tdbtool" scheme="http://idcat.cn/tags/tdbdump-tdbbakcup-tdbtool/"/>
    
  </entry>
  
  <entry>
    <title>blktrace 工具简介</title>
    <link href="http://idcat.cn/2018/04/21/blktrace-%E5%B7%A5%E5%85%B7%E7%AE%80%E4%BB%8B/"/>
    <id>http://idcat.cn/2018/04/21/blktrace-工具简介/</id>
    <published>2018-04-21T14:00:18.000Z</published>
    <updated>2018-04-21T14:07:43.764Z</updated>
    
    <content type="html"><![CDATA[<p>利用BLKTRACE分析IO性能</p><p>在Linux系统上，如果I/O发生性能问题，有没有办法进一步定位故障位置呢？iostat等最常用的工具肯定是指望不上的，blktrace在这种场合就能派上用场，因为它能记录I/O所经历的各个步骤，从中可以分析是IO Scheduler慢还是硬件响应慢。简化版io路径图：<br><a id="more"></a><br><img src="https://i.imgur.com/tnNhcA4.png" alt=""></p><p>一个I/O请求进入block layer之后，可能会经历下面的过程：</p><ul><li>Remap: 可能被DM(Device Mapper)或MD(Multiple Device, Software RAID) remap到其它设备</li><li>Split: 可能会因为I/O请求与扇区边界未对齐、或者size太大而被分拆(split)成多个物理I/O</li><li>Merge: 可能会因为与其它I/O请求的物理位置相邻而合并(merge)成一个I/O</li><li>被IO Scheduler依照调度策略发送给driver</li><li>被driver提交给硬件，经过HBA、电缆（光纤、网线等）、交换机（SAN或网络）、最后到达存储设备，设备完成IO请求之后再把结果发回。</li></ul><p>blktrace能记录I/O所经历的各个步骤，来看一下它记录的数据，包含9个字段，下图标示了其中8个字段的含义，大致的意思是“哪个进程在访问哪个硬盘的哪个扇区，进行什么操作，进行到哪个步骤，时间戳是多少”：<br><img src="https://i.imgur.com/NZL9qcm.png" alt=""></p><p>-第一个字段：8,0 这个字段是设备号 major device ID和minor device ID。</p><p>-第二个字段：3 表示CPU</p><p>-第三个字段：11 序列号</p><p>-第四个字段：0.009507758 Time Stamp是时间偏移</p><p>-第五个字段：PID 本次IO对应的进程ID</p><p>-第六个字段：Event，这个字段非常重要，反映了IO进行到了那一步</p><p>-第七个字段：R表示 Read， W是Write，D表示block，B表示Barrier Operation</p><p>-第八个字段：223490+56，表示的是起始block number 和 number of blocks，即我们常说的Offset 和 Size</p><p>-第九个字段： 进程名</p><p>其中第六个字段非常有用：每一个字母都代表了IO请求所经历的某个阶段。</p><pre><code>A  映射值对应设备 IO was remapped to a different deviceB  IO反弹，由于32位地址长度限制，所以需要copy数据到低位内存，这会有性能损耗。IO bouncedC  IO完成 IO completionD  将IO发送给驱动 IO issued to driverF  IO请求，前合并 IO front merged with request on queueG  获取 请求 Get requestI  IO插入请求队列 IO inserted onto request queueM  IO请求，后合并 IO back merged with request on queueP  插上块设备队列（队列插入机制） Plug requestQ  io被请求队列处理代码接管。 IO handled by request queue codeS  等待发送请求。 Sleep requestT  由于超时而拔出设备队列 Unplug due to timeoutU  拔出设备队列 Unplug requestX  开始新的扇区 Split</code></pre><p>以下需要记清楚的：</p><pre><code>Q – 即将生成IO请求G – IO请求生成I – IO请求进入IO Scheduler队列D – IO请求进入driverC – IO请求执行完毕</code></pre><p>注意，整个IO路径，分成很多段，每一段开始的时候，都会有一个时间戳，根据上一段开始的时间和下一段开始的时间，就可以得到IO 路径各段花费的时间。</p><p>注意，我们心心念念的service time，也就是反应块设备处理能力的指标，就是从D到C所花费的时间，简称D2C。</p><p>而iostat输出中的await，即整个IO从生成请求到IO请求执行完毕，即从Q到C所花费的时间，我们简称Q2C。</p><p>我们知道Linux 有I/O scheduler，调度器的效率如何，I2D是重要的指标。</p><p>注意，这只是blktrace输出的一个部分，很明显，我们还能拿到offset和size，根据offset，我们能拿到某一段时间里，应用程序都访问了整个块设备的那些block，从而绘制出块设备访问轨迹图。</p><p><strong>blktrace centos7安装</strong></p><p>yum install blktrace -y </p><p>会自动生成blktrace  blkparse btt 3个工具，其中，blktrace收集数据，blkparce分析数据，btt汇总数据。</p><p>blktrace的用法</p><p>使用blktrace需要挂载debugfs：</p><p>$ mount -t debugfs debugfs /sys/kernel/debug</p><p>利用blktrace查看实时数据的方法，比如要看的硬盘是sdb：</p><p>$ blktrace -d /dev/sdb -o – | blkparse -i –</p><p>需要停止的时候，按Ctrl-C。</p><p>个人常用方法：   </p><p>blktrace -d /dev/sdc</p><p>生成数据：    应用结束后，手动终止监控，会生成cpu数量的文件</p><p>blkparse -i sdc -d sdc.blktrace.bin</p><p>分析数据：   btt</p><p>btt -i sdc.blktrace.bin -A |less</p><p>汇总后，部分截图</p><p><img src="https://i.imgur.com/uLH3VD0.png" alt=""></p><p>根据以上步骤对应的时间戳就可以计算出I/O请求在每个阶段所消耗的时间：</p><pre><code>Q2G – 生成IO请求所消耗的时间，包括remap和split的时间；G2I – IO请求进入IO Scheduler所消耗的时间，包括merge的时间；I2D – IO请求在IO Scheduler中等待的时间；D2C – IO请求在driver和硬件上所消耗的时间；Q2C – 整个IO请求所消耗的时间(Q2I + I2D + D2C = Q2C)，相当于iostat的await。</code></pre><p>如果I/O性能慢的话，以上指标有助于进一步定位缓慢发生的地方：</p><pre><code>D2C可以作为硬件性能的指标；I2D可以作为IO Scheduler性能的指标。</code></pre><p>更多分析请参考下面链接。</p><p><a href="http://bean-li.github.io/blktrace-to-report/" target="_blank" rel="noopener">本文参考 http://bean-li.github.io/blktrace-to-report/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;利用BLKTRACE分析IO性能&lt;/p&gt;
&lt;p&gt;在Linux系统上，如果I/O发生性能问题，有没有办法进一步定位故障位置呢？iostat等最常用的工具肯定是指望不上的，blktrace在这种场合就能派上用场，因为它能记录I/O所经历的各个步骤，从中可以分析是IO Scheduler慢还是硬件响应慢。简化版io路径图：&lt;br&gt;
    
    </summary>
    
    
      <category term="blktrace btt blkparse" scheme="http://idcat.cn/tags/blktrace-btt-blkparse/"/>
    
  </entry>
  
  <entry>
    <title>cosbench例子-参考</title>
    <link href="http://idcat.cn/2018/04/15/cosbench%E4%BE%8B%E5%AD%90-%E5%8F%82%E8%80%83/"/>
    <id>http://idcat.cn/2018/04/15/cosbench例子-参考/</id>
    <published>2018-04-15T11:30:01.000Z</published>
    <updated>2018-04-15T11:34:50.074Z</updated>
    
    <content type="html"><![CDATA[<p>仅此记录，自己参考。<br><a id="more"></a></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;workload name=&quot;128k-36&quot; description=&quot;sample benchmark for s3&quot;&gt;  &lt;storage type=&quot;s3&quot; /&gt; &lt;workflow&gt;&lt;workstage name=&quot;init&quot;&gt;  &lt;storage type=&quot;s3&quot; config=&quot;accesskey=zhangadmin;secretkey=zhangadmin;endpoint=http://192.168.0.191:7480&quot; /&gt;  &lt;work type=&quot;init&quot; workers=&quot;3&quot; config=&quot;cprefix=128zhang;containers=r(1,30)&quot; /&gt;&lt;/workstage&gt;&lt;workstage name=&quot;prepare&quot;&gt;  &lt;work type=&quot;prepare&quot; workers=&quot;36&quot; config=&quot;cprefix=128zhang;containers=r(1,30);objects=r(1,1000);sizes=c(128)KB&quot; /&gt;&lt;/workstage&gt;&lt;workstage name=&quot;main&quot;&gt;  &lt;work name=&quot;write&quot; workers=&quot;36&quot; runtime=&quot;300&quot;&gt;      &lt;storage type=&quot;s3&quot; config=&quot;accesskey=zhangadmin;secretkey=zhangadmin;endpoint=http://192.168.0.191:7480&quot; /&gt;      &lt;operation type=&quot;write&quot; ratio=&quot;100&quot; config=&quot;cprefix=128zhang;containers=u(1,10);objects=u(1,1000);sizes=c(128)KB&quot; /&gt; &lt;/work&gt; &lt;work name=&quot;write&quot; workers=&quot;36&quot; runtime=&quot;300&quot;&gt;      &lt;storage type=&quot;s3&quot; config=&quot;accesskey=zhangadmin;secretkey=zhangadmin;endpoint=http://192.168.0.192:7480&quot; /&gt;      &lt;operation type=&quot;write&quot; ratio=&quot;100&quot; config=&quot;cprefix=128zhang;containers=u(11,20);objects=u(1,1000);sizes=c(128)KB&quot; /&gt; &lt;/work&gt; &lt;work name=&quot;write&quot; workers=&quot;36&quot; runtime=&quot;300&quot;&gt;      &lt;storage type=&quot;s3&quot; config=&quot;accesskey=zhangadmin;secretkey=zhangadmin;endpoint=http://192.168.0.193:7480&quot; /&gt;      &lt;operation type=&quot;write&quot; ratio=&quot;100&quot; config=&quot;cprefix=128zhang;containers=u(21,30);objects=u(1,1000);sizes=c(128)KB&quot; /&gt; &lt;/work&gt;&lt;/workstage&gt;&lt;workstage name=&quot;cleanup&quot;&gt;  &lt;work type=&quot;cleanup&quot; workers=&quot;36&quot; config=&quot;cprefix=128zhang;containers=r(1,30);objects=r(1,1000)&quot; /&gt;&lt;/workstage&gt;&lt;workstage name=&quot;dispose&quot;&gt;  &lt;work type=&quot;dispose&quot; workers=&quot;36&quot; config=&quot;cprefix=128zhang;containers=r(1,30)&quot; /&gt;&lt;/workstage&gt;    &lt;/workflow&gt;&lt;/workload&gt;</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;仅此记录，自己参考。&lt;br&gt;
    
    </summary>
    
    
      <category term="cosbench" scheme="http://idcat.cn/tags/cosbench/"/>
    
  </entry>
  
  <entry>
    <title>集群所有mon store.db丢失恢复</title>
    <link href="http://idcat.cn/2018/04/02/%E9%9B%86%E7%BE%A4%E6%89%80%E6%9C%89mon-store-db%E4%B8%A2%E5%A4%B1%E6%81%A2%E5%A4%8D/"/>
    <id>http://idcat.cn/2018/04/02/集群所有mon-store-db丢失恢复/</id>
    <published>2018-04-02T11:42:38.000Z</published>
    <updated>2018-04-02T11:50:00.576Z</updated>
    
    <content type="html"><![CDATA[<p>1、模拟测试环境</p><p>三台虚拟机，每台虚拟机一个osd，均是mon节点，mds节点</p><pre><code>node1  192.168.1.141 node2  192.168.1.142 node3  192.168.1.143 </code></pre><a id="more"></a><p>2、模拟所有节点的mon  数据丢失 </p><p>在3个集群节点上停止mon服务，拷贝数据到其他路径</p><pre><code>[root@node1 ~]# systemctl stop ceph-mon@node1[root@node1 ~]# mv /var/lib/ceph/mon/ceph-node1/store.db /tmp/[root@node2 ~]# systemctl stop ceph-mon@node2[root@node2 ~]# mv /var/lib/ceph/mon/ceph-node2/store.db/ /tmp/[root@node3 ~]# systemctl stop ceph-mon@node3[root@node3 ~]# mv  /var/lib/ceph/mon/ceph-node3/store.db/ /tmp</code></pre><p>3、从osd上获取monmap信息</p><p>先停止所有的osd服务，然后每个节点创建临时目录</p><pre><code>[root@node1 ~]# systemctl stop ceph-osd@0[root@node2 ~]# systemctl stop ceph-osd@1[root@node3 ~]# systemctl stop ceph-osd@2[root@node1 ~]# mkdir /tmp/monstore[root@node2 ~]# mkdir /tmp/monstore[root@node3 ~]# mkdir /tmp/monstore</code></pre><p>首先在node1上操作，该节点只有一个osd。从这个osd上收集mon相关的数据，存放到/tmp/monstore 目录</p><pre><code>[root@node1 ~]# ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-0/ --op update-mon-db --mon-store-path /tmp/monstore/osd.0   : 0 osdmaps trimmed, 31 osdmaps added.      256 pgs added.</code></pre><p>因为有多台osd机器多个osd的话，就得在每台服务器每个osd上分别执行上面命令。这个目录的数据一定要保持递增的。弄完一台就传递数据到下一台接着弄。</p><p>因为本例中有3个节点，每个节点只有一个osd，所以node1上只需要执行一次即可。</p><p>传递数据到node2上</p><pre><code>[root@node1 store.db]# rsync -avz /tmp/monstore/ node2:/tmp/monstore/sending incremental file list./store.db/store.db/000003.logstore.db/CURRENTstore.db/LOCKstore.db/MANIFEST-000002sent 27853 bytes  received 95 bytes  18632.00 bytes/sectotal size is 329590  speedup is 11.79</code></pre><p>在node2节点上从osd上获取mon相关数据</p><pre><code>[root@node2 ~]# ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-1/ --op update-mon-db --mon-store-path /tmp/monstore/osd.1   : 0 osdmaps trimmed, 0 osdmaps added.      96 pgs added.</code></pre><p>传递数据到node3</p><pre><code>[root@node2 ~]# rsync -avz /tmp/monstore/ node3:/tmp/monstore/sending incremental file list./store.db/store.db/000005.sststore.db/000006.logstore.db/CURRENTstore.db/LOCKstore.db/MANIFEST-000004sent 37210 bytes  received 114 bytes  74648.00 bytes/sectotal size is 132524  speedup is 3.55</code></pre><p>在node3上获取mon数据</p><pre><code>[root@node3 store.db]# ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-2/ --op update-mon-db --mon-store-path /tmp/monstore/osd.2   : 0 osdmaps trimmed, 0 osdmaps added.      82 pgs added.</code></pre><p>之后，将这个 目录的数据传递到所有需要恢复mon节点上，本例为node1 node2 node3(已有最新数据)</p><pre><code>[root@node3 store.db]# rsync -avz /tmp/monstore/ node1:/tmp/monstore/root@node1&apos;s password: sending incremental file liststore.db/store.db/000005.sststore.db/000008.sststore.db/000009.logstore.db/CURRENTstore.db/MANIFEST-000007sent 41661 bytes  received 117 bytes  16711.20 bytes/sectotal size is 134943  speedup is 3.23[root@node3 store.db]# rsync -avz /tmp/monstore/ node2:/tmp/monstore/sending incremental file liststore.db/store.db/000008.sststore.db/000009.logstore.db/CURRENTstore.db/MANIFEST-000007sent 10113 bytes  received 98 bytes  20422.00 bytes/sectotal size is 134943  speedup is 13.22</code></pre><p>4、恢复mon数据</p><p>删除之前默认路径存在的数据，或者新建目录</p><pre><code>[root@node1 mon]# mkdir /var/lib/ceph/mon/ceph-node1/[root@node1 ceph-node1]# ceph-monstore-tool /tmp/monstore/ rebuild[root@node1 ceph-node1]# cp -ra /tmp/monstore/* /var/lib/ceph/mon/ceph-node1/[root@node1 ceph-node1]# touch /var/lib/ceph/mon/ceph-node1/done[root@node1 ceph-node1]# touch /var/lib/ceph/mon/ceph-node1/systemd[root@node1 ceph-node1]# chown ceph:ceph -R /var/lib/ceph/mon/ceph-node1/</code></pre><p>在node2 node3节点上分别执行上面操作</p><p>5、重启mon</p><p>所有节点重启mon服务</p><pre><code>[root@node1 ceph-node1]# systemctl restart ceph-mon@node1Job for ceph-mon@node1.service failed because start of the service was attempted too often. See &quot;systemctl status ceph-mon@node1.service&quot; and &quot;journalctl -xe&quot; for details.To force a start use &quot;systemctl reset-failed ceph-mon@node1.service&quot; followed by &quot;systemctl start ceph-mon@node1.service&quot; again.[root@node1 ceph-node1]# systemctl reset-failed ceph-mon@node1.service[root@node1 ceph-node1]# systemctl start ceph-mon@node1.service</code></pre><p>最终查看mon均没有启动，查看日志显示如下信息：</p><pre><code>2018-04-02 17:55:29.419474 7f71222da700  1 leveldb: Compacting 5@0 + 0@1 files2018-04-02 17:55:29.419975 7f71284cc600  0 mon.node1 does not exist in monmap, will attempt to join an existing cluster2018-04-02 17:55:29.420158 7f71284cc600 -1 no public_addr or public_network specified, and mon.node1 not present in monmap or ceph.conf</code></pre><p>提示节点不在monmap表中。</p><p>参考之前写的文件 <a href="http://www.idcat.cn/2018/03/27/ceph%E9%9B%86%E7%BE%A4%E6%9B%BF%E6%8D%A2mon%E8%8A%82%E7%82%B9ip%E5%9C%B0%E5%9D%80/" title="更换mon ip地址" target="_blank" rel="noopener">http://www.idcat.cn/2018/03/27/ceph%E9%9B%86%E7%BE%A4%E6%9B%BF%E6%8D%A2mon%E8%8A%82%E7%82%B9ip%E5%9C%B0%E5%9D%80/</a></p><p>6、加入monmap</p><pre><code>[root@node1 tmp]# monmaptool --create --generate -c /etc/ceph/ceph.conf /tmp/monaaamonmaptool: monmap file /tmp/monaaamonmaptool: set fsid to 911c57dc-a930-4da8-ab0e-69f6b6586e3dmonmaptool: writing epoch 0 to /tmp/monaaa (3 monitors)[root@node1 tmp]# monmaptool --print /tmp/monaaa monmaptool: monmap file /tmp/monaaaepoch 0fsid 911c57dc-a930-4da8-ab0e-69f6b6586e3dlast_changed 2018-04-02 18:25:56.526811created 2018-04-02 18:25:56.5268110: 192.168.1.141:6789/0 mon.noname-a1: 192.168.1.142:6789/0 mon.noname-b2: 192.168.1.143:6789/0 mon.noname-c</code></pre><p>可以看到主机名显示为noname-a 等。。</p><p>删除主机节点</p><pre><code>[root@node1 tmp]# monmaptool --rm noname-a /tmp/monaaa monmaptool: monmap file /tmp/monaaamonmaptool: removing noname-amonmaptool: writing epoch 0 to /tmp/monaaa (2 monitors)[root@node1 tmp]# monmaptool --rm noname-b /tmp/monaaa monmaptool: monmap file /tmp/monaaamonmaptool: removing noname-bmonmaptool: writing epoch 0 to /tmp/monaaa (1 monitors)[root@node1 tmp]# monmaptool --rm noname-c /tmp/monaaa monmaptool: monmap file /tmp/monaaamonmaptool: removing noname-cmonmaptool: writing epoch 0 to /tmp/monaaa (0 monitors)</code></pre><p>添加新的主机节点</p><pre><code>[root@node1 tmp]# monmaptool --add node1 192.168.1.141:6789 /tmp/monaaa monmaptool: monmap file /tmp/monaaamonmaptool: writing epoch 0 to /tmp/monaaa (1 monitors)[root@node1 tmp]# monmaptool --add node2 192.168.1.142:6789 /tmp/monaaa     monmaptool: monmap file /tmp/monaaamonmaptool: writing epoch 0 to /tmp/monaaa (2 monitors)[root@node1 tmp]# monmaptool --add node3 192.168.1.143:6789 /tmp/monaaa monmaptool: monmap file /tmp/monaaamonmaptool: writing epoch 0 to /tmp/monaaa (3 monitors)</code></pre><p>将该monaaa文件传到node2 node3 需要恢复mon信息的节点上</p><pre><code>[root@node1 tmp]# scp /tmp/monaaa node2:/tmp/monaaa                                                                                     100%  481    43.3KB/s   00:00    [root@node1 tmp]# scp /tmp/monaaa node3:/tmp/monaaa                                                                                     100%  481   570.6KB/s   00:00    </code></pre><p>在各个节点上注入新的monmap表信息</p><pre><code>[root@node1 ~]# ceph-mon -i node1 --inject-monmap /tmp/monaaa [root@node2 ~]# ceph-mon -i node2 --inject-monmap /tmp/monaaa[root@node3 ~]# ceph-mon -i node3 --inject-monmap /tmp/monaaa </code></pre><p>启动mon</p><pre><code>[root@node1 ~]# systemctl start ceph-mon@node1[root@node2 ~]# systemctl start ceph-mon@node2[root@node3 ~]# systemctl start ceph-mon@node3</code></pre><p>查看集群状态</p><p>[root@node2 ~]# ceph -s</p><pre><code>cluster 911c57dc-a930-4da8-ab0e-69f6b6586e3d health HEALTH_OK monmap e1: 3 mons at {node1=192.168.1.141:6789/0,node2=192.168.1.142:6789/0,node3=192.168.1.143:6789/0}        election epoch 2, quorum 0,1,2 node1,node2,node3 osdmap e31: 3 osds: 3 up, 3 in        flags sortbitwise,require_jewel_osds  pgmap v1: 256 pgs, 3 pools, 2068 bytes data, 20 objects        0 kB used, 0 kB / 0 kB avail             225 active+clean              28 active+clean+scrubbing               3 active+clean+scrubbing+deep</code></pre><p>至此 集群恢复OK</p><p>参考<a href="http://www.zphj1987.com/2017/04/19/why-rm-object-can-get/" target="_blank" rel="noopener">http://www.zphj1987.com/2017/04/19/why-rm-object-can-get/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1、模拟测试环境&lt;/p&gt;
&lt;p&gt;三台虚拟机，每台虚拟机一个osd，均是mon节点，mds节点&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;node1  192.168.1.141 
node2  192.168.1.142 
node3  192.168.1.143 
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
    
      <category term="ceph" scheme="http://idcat.cn/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>ceph 多区域radosgw网关配置</title>
    <link href="http://idcat.cn/2018/03/28/ceph-%E5%A4%9A%E5%8C%BA%E5%9F%9Fradosgw%E7%BD%91%E5%85%B3%E9%85%8D%E7%BD%AE/"/>
    <id>http://idcat.cn/2018/03/28/ceph-多区域radosgw网关配置/</id>
    <published>2018-03-28T14:06:36.000Z</published>
    <updated>2018-03-28T14:07:43.015Z</updated>
    
    <content type="html"><![CDATA[<p>一、本文环境</p><p>2台虚拟机部署2套ceph集群。2个节点分别作为不同集群的radosgw网关。</p><p>Cluster1    节点名： ceph01   ip :  192.168.0.39</p><p>Cluster2    节点名： ceph02   ip :  192.168.1.169<br><a id="more"></a><br>二，概念：        </p><p>  zone：包含多个RGW实例的一个逻辑概念。zone不能跨集群。同一个zone的数据保存在同一组pool中。     本例中每个zone只有一个RGW实例。</p><p>  zonegroup：一个zonegroup如果包含1个或多个zone。如果一个zonegroup包含多个zone，必须指定 一个zone作为master zone，用来处理bucket和用户的创建。一个集群可以创建多个zonegroup，一个zonegroup也可以跨多个集群。     本例中只有一个zonegroup,跨2个集群，每个集群只有一个zone,分别作为master和从zone.</p><p>  realm：一个realm包含1个或多个zonegroup。如果realm包含多个zonegroup，必须指定一个zonegroup为master zonegroup， 用来处理系统操作。一个系统中可以包含多个realm，多个realm之间资源完全隔离。    本例 只有一个realm,包含一个zonegroup. </p><p>  RGW多活方式是在同一zonegroup的多个zone之间进行，即同一zonegroup中多个zone之间的数据是完全一致的，用户可以通过任意zone读写同一份数据。 但是，对元数据的操作，比如创建桶、创建用户，仍然只能在master zone进行。对数据的操作，比如创建桶中的对象，访问对象等，可以在任意zone中 处理。</p><p>三、在Cluster1集群上配置master zone</p><p>创建realm</p><pre><code>radosgw-admin realm create --rgw-realm=aaa --default[root@ceph01 ~]# radosgw-admin realm list{&quot;default_info&quot;: &quot;eff9b039-8c3c-4991-87f9-e9331b2c7824&quot;,&quot;realms&quot;: [    &quot;aaa&quot;    ]}</code></pre><p>创建master zonegroup,先删除默认的zonegroup</p><pre><code>radosgw-admin zonegroup delete --rgw-zonegroup=default</code></pre><p>创建一个为azonggroup的zonegroup</p><pre><code>radosgw-admin zonegroup create --rgw-zonegroup=azonegroup --endpoints=192.168.0.39:7480 --master --default</code></pre><p>创建master zone,先删除默认的zone</p><pre><code>adosgw-admin zone delete --rgw-zone=default</code></pre><p>创建一个为azone的zone</p><pre><code>radosgw-admin zone create --rgw-zonegroup=azonegroup --rgw-zone=azone --endpoints=192.168.0.39:7480 --default --master</code></pre><p>创建一个auser账户用于和bzone zone同步</p><pre><code>radosgw-admin user create --uid=&quot;auser&quot; --display-name=&quot;auser&quot; --system</code></pre><p>用创建auser账户产生的access 和secret更新zone配置</p><pre><code>radosgw-admin zone modify --rgw-zone=azone --access-key=BKG10IM15N8EB0I7ZE7U --secret=Cvh60vBX5ciujqRaLw3bm6wMIGmLdlJ9FB4ukOG</code></pre><p>更新period</p><pre><code>radosgw-admin period update --commit</code></pre><p>修改配置ceph.conf</p><pre><code>[client.rgw.ceph-1]host = ceph-1rgw frontends = &quot;civetweb port=7480&quot;rgw_zone=azone</code></pre><p>重启radosgw服务</p><pre><code>systemctl restart ceph-radosgw@rgw.ceph01</code></pre><p>四、在Cluster2集群上配置slave zone</p><p>从master zone拉取realm</p><pre><code>[root@ceph02 ceph]#radosgw-admin realm pull --url=192.168.0.39:7480 --access-key=BKG10IM15N8EB0I7ZE7U --secret=Cvh60vBX5ciujqRaLw3bm6wMIGmLdlJ9FB4ukOGC</code></pre><p>注意：这里的access key 和secret是master zone上auser 账户的access key和secret</p><p>拉取period</p><pre><code>[root@ceph02 ceph]# radosgw-admin period pull --url=192.168.0.39:7480 --access-key=BKG10IM15N8EB0I7ZE7U --secret=Cvh60vBX5ciujqRaLw3bm6wMIGmLdlJ9FB4ukOGC</code></pre><p>注意：这里的access key 和secret是master zone上auser 账户的access key和secret</p><p>创建slave zone，名称为bzone</p><pre><code>[root@ceph02 ceph]# radosgw-admin zone create --rgw-zonegroup=azonegroup --rgw-zone=bzone --access-key=BKG10IM15N8EB0I7ZE7U --secret=Cvh60vBX5ciujqRaLw3bm6wMIGmLdlJ9FB4ukOGC --endpoints=192.168.1.169:7480[root@ceph02 ceph]# radosgw-admin zone list{&quot;default_info&quot;: &quot;118aa9b6-458f-4b4e-9aa8-1c3577bf8dd6&quot;,&quot;zones&quot;: [    &quot;bzone&quot;,    &quot;default&quot;]}</code></pre><p> 注意：这里的access key 和secret是master zone上auser 账户的access key和secret</p><p>更新period</p><pre><code>[root@ceph02 ceph]#radosgw-admin period update --commit</code></pre><p>有如下警告，只需要更新libcurl既可</p><pre><code>2018-03-28 18:07:46.941333 7f27d9f499c0  0 WARNING: detected a version of libcurl which contains a bug in curl_multi_wait(). enabling a workaround that may degrade performance slightly.[root@ceph02 ceph]# yum update libcurl -y</code></pre><p>修改配置ceph.conf</p><pre><code>[client.rgw.ceph-2] host = ceph-2 rgw frontends = &quot;civetweb port=7480&quot; rgw_zone=bzone</code></pre><p>重启radosgw服务</p><pre><code>systemctl restart ceph-radosgw@rgw.ceph02</code></pre><p>五、验证zone之间数据同步</p><p>在ceph02  从节点执行</p><pre><code>[root@ceph02 ~]# radosgw-admin sync status      realm eff9b039-8c3c-4991-87f9-e9331b2c7824 (aaa)  zonegroup 7e6ff2ed-f8aa-44fc-b2f3-5ca81464dd9b (azonegroup)       zone 118aa9b6-458f-4b4e-9aa8-1c3577bf8dd6 (bzone)metadata sync syncing            full sync: 0/64 shards            incremental sync: 64/64 shards            metadata is caught up with master  data sync source: 08fcacca-5e53-4499-9413-5212d2477576 (azone)                    syncing                    full sync: 0/128 shards                    incremental sync: 128/128 shards                    data is caught up with source</code></pre><p>在master zone  节点ceph01上创建用户</p><pre><code>[root@ceph01 ~]# radosgw-admin user create --uid=&quot;zhang&quot; --display-name=&quot;zhang&quot;</code></pre><p>安装s3客户端 创建桶tong1，并put 对象</p><pre><code>[root@ceph01 ~]# s3cmd mb s3://tong1[root@ceph01 ~]# s3cmd put release.asc s3://tong1upload: &apos;release.asc&apos; -&gt; &apos;s3://tong1/release.asc&apos;  [1 of 1]1645 of 1645   100% in    0s    33.23 kB/s  done[root@ceph01 ~]# s3cmd ls s3://tong12018-03-28 12:26      1082   s3://tong1/anaconda-ks.cfg2018-03-28 13:05      1645   s3://tong1/release.asc</code></pre><p>在slave zone 节点ceph02 查看（将ceph01 /root/.s3cfg 拷贝到ceph02节点同样的目录）</p><pre><code>[root@ceph02 ~]# s3cmd  ls s3://tong12018-03-28 12:26      1082   s3://tong1/anaconda-ks.cfg2018-03-28 13:05      1645   s3://tong1/release.asc</code></pre><p>至此，2个zone之间可做到高可用，保证一定的数据安全。详细测试以后进行。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一、本文环境&lt;/p&gt;
&lt;p&gt;2台虚拟机部署2套ceph集群。2个节点分别作为不同集群的radosgw网关。&lt;/p&gt;
&lt;p&gt;Cluster1    节点名： ceph01   ip :  192.168.0.39&lt;/p&gt;
&lt;p&gt;Cluster2    节点名： ceph02   ip :  192.168.1.169&lt;br&gt;
    
    </summary>
    
    
      <category term="ceph" scheme="http://idcat.cn/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>ceph-dencoder基本使用</title>
    <link href="http://idcat.cn/2018/03/27/ceph-dencoder%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"/>
    <id>http://idcat.cn/2018/03/27/ceph-dencoder基本使用/</id>
    <published>2018-03-27T14:06:41.000Z</published>
    <updated>2018-03-27T14:07:21.265Z</updated>
    
    <content type="html"><![CDATA[<p>本文摘自<a href="https://blog.csdn.net/scaleqiao/article/details/51987426" target="_blank" rel="noopener">https://blog.csdn.net/scaleqiao/article/details/51987426</a></p><p>0 简介</p><p>贯穿Ceph OSD端数据处理的一个核心结构就是ObjectStore::Transaction，OSD处理的所有操作以及其关联的数据都会封装进入Transaction中的bufferlist结构里，这里的封装也就是序列化(encode)，它将各种数据结构无论简单或者复杂都作为字节流，存入bufferlist中。最终Transaction会由具体的ObjectStore后端实现来处理，当然，处理时会对bufferlist中的数据进行反序列化（decode）。而本文介绍的ceph-dencoder工具就是Ceph提供的可以进行encode、decode以及dump ceph相关数据结构的工具，同时它也可以用来调试以及测试Ceph不同版本之间的兼容性。今天这里主要介绍它的decode功能，其他功能大家可以自行研究。<br><a id="more"></a><br><strong>1 安装</strong></p><p>ceph-dencoder工具是默认安装的。</p><p><strong>2 使用</strong></p><p>可以通过它的manpage或者help文档来了解它的使用</p><pre><code>[root@ceph03 ~]# man ceph-dencoder  [root@ceph03 ~]# ceph-dencoder -h  usage: ceph-dencoder [commands ...]    version            print version string (to stdout)    import &lt;encfile&gt;   read encoded data from encfile    export &lt;outfile&gt;    write encoded data to outfile    set_features &lt;num&gt;  set feature bits used for encoding    get_features        print feature bits (int) to stdout    list_types          list supported types    type &lt;classname&gt;    select in-memory type    skip &lt;num&gt;          skip &lt;num&gt; leading bytes before decoding    decode              decode into in-memory object    encode              encode in-memory object    dump_json           dump in-memory object as json (to stdout)    copy                copy object (via operator=)    copy_ctor           copy object (via copy ctor)    count_tests         print number of generated test objects (to stdout)    select_test &lt;n&gt;     select generated test object as in-memory object    is_deterministic    exit w/ success if type encodes deterministically  </code></pre><p>它的用法比较简单，即ceph-dencoder跟上相应的子命令即可。<br>在具体使用时，可以通过以下命令查看ceph-dencoder当前支持哪些结构：</p><pre><code>ceph-dencoder list_types  </code></pre><p>要确定需要通过哪种结构来进行数据解析，只能通过阅读源码，找到encode数据时，数据对应的数据结构。</p><p><strong>3 使用事例</strong></p><p>下面以查看object的object_info信息为例，介绍一下这个工具的使用。</p><p>在使用XFS作为后端存储时，一个object就对应一个文件，obejct的object_info信息通常是作为文件的扩展属性存在的。</p><p>首先先找到一个object对应的文件，并查看object文件的扩展属性，这里会用到XFS一个工具attr，主要用于操作XFS文件的扩展属性的：</p><pre><code>[root@ceph02 0.10_head]# rados ls -p rbdrbd_header.d3652ae8944arbd_directoryrbd_id.test</code></pre><p>查看ceph osd map rbd rbd_header.d3652ae8944a  找到对应的osd</p><pre><code>[root@ceph02 0.10_head]# [root@ceph02 0.10_head]# attr -l rbd\\uheader.d3652ae8944a__head_9EB01B90__0 Attribute &quot;cephos.spill_out&quot; has a 2 byte value for rbd\uheader.d3652ae8944a__head_9EB01B90__0Attribute &quot;ceph._lock.rbd_lock&quot; has a 23 byte value for rbd\uheader.d3652ae8944a__head_9EB01B90__0Attribute &quot;ceph._&quot; has a 250 byte value for rbd\uheader.d3652ae8944a__head_9EB01B90__0Attribute &quot;ceph._@1&quot; has a 250 byte value for rbd\uheader.d3652ae8944a__head_9EB01B90__0Attribute &quot;ceph._@2&quot; has a 92 byte value for rbd\uheader.d3652ae8944a__head_9EB01B90__0Attribute &quot;ceph.snapset&quot; has a 31 byte value for rbd\uheader.d3652ae8944a__head_9EB01B90__0</code></pre><p>从上面的输出可以看到扩展属性中总共有3部分，其中”ceph.<em>“和”ceph.</em>@1”，”ceph.<em>@2”应该算是一部分，因为ceph.</em>超出了XFS扩展属性的长度限制，所以拆成了3个。而它里面就存放了我们要找的object_info_t的数据。”cephos.spill_out”是一个两个字节的字符数组，用于记录文件的扩展属性是否溢出到了Omap里，一般它的值是“0”或者“1”，“0”就是源码里的宏定义XATTR_NO_SPILL_OUT，表示没有溢出，而“1”是XATTR_SPILL_OUT，表示有溢出。“ceph.snapset”用来记录和Object相关的Snapshot信息，它的数据以SnapSet这个结构的形式存在。</p><p>另外还有一个通用的命令getfattr也可以干类似的事情。</p><pre><code>[root@ceph02 0.10_head]# getfattr -d rbd\\uheader.d3652ae8944a__head_9EB01B90__0 -m &apos;user\.ceph\._&apos;# file: rbd\134uheader.d3652ae8944a__head_9EB01B90__0user.ceph._=0sDwhKAgAABAM4AAAAAAAAABcAAAByYmRfaGVhZGVyLmQzNjUyYWU4OTQ0Yf7/////////kBuwngAAAAAAAAAAAAAAAAAGAxwAAAAAAAAAAAAAAP////8AAAAAAAAAAP//////////AAAAAAoAAAAAAAAAOAAAAAkAAAAAAAAAOAAAAAICFQAAAAhr0wAAAAAAAAIAAAAAAAAAAQAAAAAAAAAAAAAAIki6WizsNiECAhUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEAAAAIa9MAAAAAAAAEA5QAAAABAAAAAAAAAB4AAAAAAA==user.ceph._@1=0sAADTQ2beAAIAAMCoAakAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAAAAAAAAAAAAAAAAQAAAAEAAAAAAAAACGvTAAAAAAAABAOUAAAAAQAAAAAAAAAeAAAAAAAAANNDZt4AAgAAwKgBqQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==user.ceph._@2=0sAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHAAAACJIulr/91Ah//////////8=user.ceph._lock.rbd_lock=0sAQERAAAAAAAAAAEIAAAAaW50ZXJuYWw=</code></pre><p>接着，我们将扩展属性中的数据dump到一个文件当中，注意应该将”ceph.<em>“和”ceph.</em>@1”，”ceph._@1”的数据拼起来。</p><pre><code>[root@ceph02 0.10_head]# attr -q -g &quot;ceph._&quot; rbd\\uheader.d3652ae8944a__head_9EB01B90__0  &gt; 1.txt[root@ceph02 0.10_head]# attr -q -g &quot;ceph._@1&quot; rbd\\uheader.d3652ae8944a__head_9EB01B90__0  &gt;&gt; 1.txt[root@ceph02 0.10_head]# attr -q -g &quot;ceph._@2&quot; rbd\\uheader.d3652ae8944a__head_9EB01B90__0  &gt;&gt; 1.txt </code></pre><p>最后使用ceph-dencoder工具将其内容解析出来。</p><pre><code>[root@ceph02 0.10_head]# ceph-dencoder import 1.txt type object_info_t decode dump_json{&quot;oid&quot;: {    &quot;oid&quot;: &quot;rbd_header.d3652ae8944a&quot;,    &quot;key&quot;: &quot;&quot;,    &quot;snapid&quot;: -2,    &quot;hash&quot;: 2662341520,    &quot;max&quot;: 0,    &quot;pool&quot;: 0,    &quot;namespace&quot;: &quot;&quot;},&quot;version&quot;: &quot;56&apos;10&quot;,&quot;prior_version&quot;: &quot;56&apos;9&quot;,&quot;last_reqid&quot;: &quot;client.54123.1:2&quot;,&quot;user_version&quot;: 8,&quot;size&quot;: 0,&quot;mtime&quot;: &quot;2018-03-27 21:33:22.557247&quot;,&quot;local_mtime&quot;: &quot;2018-03-27 21:33:22.558954&quot;,&quot;lost&quot;: 0,&quot;flags&quot;: 28,&quot;snaps&quot;: [],&quot;truncate_seq&quot;: 0,&quot;truncate_size&quot;: 0,&quot;data_digest&quot;: 4294967295,&quot;omap_digest&quot;: 4294967295,&quot;watchers&quot;: {    &quot;client.54123&quot;: {        &quot;cookie&quot;: 1,        &quot;timeout_seconds&quot;: 30,        &quot;addr&quot;: {            &quot;nonce&quot;: 3731243987,            &quot;addr&quot;: &quot;192.168.1.169:0&quot;        }    }}}</code></pre><p>后期遇到详细学习。仅做记录。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文摘自&lt;a href=&quot;https://blog.csdn.net/scaleqiao/article/details/51987426&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/scaleqiao/article/details/51987426&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;0 简介&lt;/p&gt;
&lt;p&gt;贯穿Ceph OSD端数据处理的一个核心结构就是ObjectStore::Transaction，OSD处理的所有操作以及其关联的数据都会封装进入Transaction中的bufferlist结构里，这里的封装也就是序列化(encode)，它将各种数据结构无论简单或者复杂都作为字节流，存入bufferlist中。最终Transaction会由具体的ObjectStore后端实现来处理，当然，处理时会对bufferlist中的数据进行反序列化（decode）。而本文介绍的ceph-dencoder工具就是Ceph提供的可以进行encode、decode以及dump ceph相关数据结构的工具，同时它也可以用来调试以及测试Ceph不同版本之间的兼容性。今天这里主要介绍它的decode功能，其他功能大家可以自行研究。&lt;br&gt;
    
    </summary>
    
    
      <category term="ceph" scheme="http://idcat.cn/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>ceph-kvstore-tool工具简单介绍</title>
    <link href="http://idcat.cn/2018/03/27/ceph-kvstore-tool%E5%B7%A5%E5%85%B7%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/"/>
    <id>http://idcat.cn/2018/03/27/ceph-kvstore-tool工具简单介绍/</id>
    <published>2018-03-27T12:07:00.000Z</published>
    <updated>2018-03-27T12:09:04.629Z</updated>
    
    <content type="html"><![CDATA[<p>参考<a href="https://blog.csdn.net/scaleqiao/article/details/51946042" target="_blank" rel="noopener">https://blog.csdn.net/scaleqiao/article/details/51946042</a></p><p>大家都知道Ceph的很多数据比如PG log、Monitor的数据都存在kvstore里（leveldb或者RocksDB中），Ceph也提供了查看kvstore里数据的工具，它就是ceph-kvstore-tool。<br><a id="more"></a><br>1 安装ceph-kvstore-tool工具</p><p>如果你是从官网释放的rpm包安装的Ceph，那么ceph-kvstore-tool默认是没有安装的，它包含在ceph-test这个rpm中，你可以通过以下方法安装。</p><pre><code>yum install ceph-test  </code></pre><p>2 ceph-kvstore-tool命令使用介绍</p><p>以下介绍基于Ceph 10.2.3版本，</p><pre><code>[root@ceph02 ~]# ceph --versionceph version 10.2.3 (ecc23778eb545d8dd55e2e4735b53cc93f92e65b) </code></pre><p>查看帮助</p><pre><code>[root@ceph02 ~]# ceph-kvstore-tool -hUsage: ceph-kvstore-tool &lt;leveldb|rocksdb|...&gt; &lt;store path&gt; command [args...]Commands:  list [prefix]  list-crc [prefix]  exists &lt;prefix&gt; [key]  get &lt;prefix&gt; &lt;key&gt; [out &lt;file&gt;]  crc &lt;prefix&gt; &lt;key&gt;  get-size [&lt;prefix&gt; &lt;key&gt;]  set &lt;prefix&gt; &lt;key&gt; [ver &lt;N&gt;|in &lt;file&gt;]  store-copy &lt;path&gt; [num-keys-per-tx]  store-crc &lt;path&gt;</code></pre><p>描述的比较简单，但是基本上告诉你了这个命令的用法。当前使用的是leveldb数据库，store path指定的是leveldb数据库路径，例如mon 目录的store.db， osd目录下的current/omap 目录。</p><p>我们知道leveldb是一个kvstore也就是kv的数据库，prefix就是数据库的表名。key就是表里面的key值，而value值这里通过get 后指定输出到out file中。<br>如下所示，list输出格式为 表名：key</p><p>查看mon leveldb数据则停止mon服务，osd也是如此</p><pre><code>[root@ceph02 ~]# systemctl stop ceph-mon@ceph02[root@ceph02 ~]# ceph-kvstore-tool leveldb /var/lib/ceph/mon/ceph-ceph02/store.db/  list |head -n 52018-03-27 17:18:52.735099 7fede398b040  1 leveldb: Recovering log #2652018-03-27 17:18:52.735886 7fede398b040  1 leveldb: Level-0 table #267: started2018-03-27 17:18:52.737658 7fede398b040  1 leveldb: Level-0 table #267: 54804 bytes OKauth:1auth:10auth:100auth:101auth:102 </code></pre><p>查看所有表名</p><pre><code>[root@ceph02 ~]# ceph-kvstore-tool leveldb /var/lib/ceph/mon/ceph-ceph02/store.db/ list |awk -F &apos;:&apos; &apos;{print $1}&apos;|uniq2018-03-27 17:24:59.608065 7f8580087040  1 leveldb: Recovering log #2682018-03-27 17:24:59.656403 7f8580087040  1 leveldb: Delete type=3 #2662018-03-27 17:24:59.656449 7f8580087040  1 leveldb: Delete type=0 #268authlogmmds_healthmds_metadatamdsmapmonitormonitor_storemonmaposd_metadataosdmappaxospgmappgmap_metapgmap_osdpgmap_pg </code></pre><p>从中我们发现了比较熟悉的各种map，mdsmap、monmap、osdmap、pgmap、auth等。而如上所讲，其中每一张表都有很多表项组成，接下来我们使用ceph-kvstore-tool来查看一下monmap这个表中某个表项中的数据。</p><pre><code>[root@ceph02 ~]# ceph-kvstore-tool leveldb /var/lib/ceph/mon/ceph-ceph02/store.db/ list| grep monmap2018-03-27 17:26:52.435840 7f6601faf040  1 leveldb: Recovering log #2702018-03-27 17:26:52.440832 7f6601faf040  1 leveldb: Delete type=3 #2692018-03-27 17:26:52.440874 7f6601faf040  1 leveldb: Delete type=0 #270monmap:1monmap:2monmap:first_committedmonmap:last_committedmonmap:latest</code></pre><p>从中，可以看出monmap目前有三个表项，看看monmap:1这项包含什么数据。其中指定leveldb数据库，get获取表monmap，中key为1 的值，将值out指定到文件monmap.1.txt中</p><pre><code>[root@ceph02 ~]# ceph-kvstore-tool leveldb /var/lib/ceph/mon/ceph-ceph02/store.db/ get monmap 1 out monmap.1.txt2018-03-27 17:30:25.096235 7f8181d4b040  1 leveldb: Recovering log #272(monmap, 1)2018-03-27 17:30:25.120278 7f8181d4b040  1 leveldb: Delete type=3 #2712018-03-27 17:30:25.120318 7f8181d4b040  1 leveldb: Delete type=0 #272[root@ceph02 ~]# file monmap.1.txt monmap.1.txt: DBase 3 data file (301334528 records)</code></pre><p>导出来的这个文件是一个DBase文件，是编译过的，需要使用工具来解析它。我们知道Ceph中很多数据都是经过序列化（encode）之后持久化的（这里有篇文章有相关的介绍<a href="https://www.ustack.com/blog/cephxuliehua/），所以要解析这些数据需要将它们反序列化（decode），Ceph提供了一个反序列化的工具ceph-dencode，关于ceph-dencoder的使用会在以后介绍。" target="_blank" rel="noopener">https://www.ustack.com/blog/cephxuliehua/），所以要解析这些数据需要将它们反序列化（decode），Ceph提供了一个反序列化的工具ceph-dencode，关于ceph-dencoder的使用会在以后介绍。</a></p><pre><code>[root@ceph02 ~]# ceph-dencoder import monmap.1.txt type MonMap decode dump_json{   &quot;epoch&quot;: 1,&quot;fsid&quot;: &quot;f6110559-f0f1-4e14-9213-f29471329ee9&quot;,&quot;modified&quot;: &quot;2017-02-09 15:34:21.701901&quot;,&quot;created&quot;: &quot;2017-02-09 15:34:21.701901&quot;,&quot;mons&quot;: [    {        &quot;rank&quot;: 0,        &quot;name&quot;: &quot;ceph02&quot;,        &quot;addr&quot;: &quot;192.168.0.40:6789\/0&quot;    }]</code></pre><p>}<br>可以看到内容。如下查看monmap表中key为1 的size</p><pre><code>[root@ceph02 ~]# ceph-kvstore-tool leveldb /var/lib/ceph/mon/ceph-ceph02/store.db/ get-size monmap 12018-03-27 17:39:57.819864 7f2800190040  1 leveldb: Recovering log #276log - 0misc - 65908sst - 12156146total - 12222054total: 12222054estimated store size: 12222054(monmap,1) size 1922018-03-27 17:39:57.821447 7f2800190040  1 leveldb: Delete type=0 #2762018-03-27 17:39:57.821473 7f2800190040  1 leveldb: Delete type=3 #275</code></pre><p>如下查看osdmap信息</p><pre><code>[root@ceph02 ~]# ceph-kvstore-tool leveldb /var/lib/ceph/mon/ceph-ceph02/store.db/ list osdmap2018-03-27 17:43:46.711599 7fcf02a6a040  1 leveldb: Recovering log #280osdmap:1osdmap:10osdmap:11。。。osdmap:19。。。。osdmap:full_45osdmap:full_46osdmap:full_47osdmap:full_48</code></pre><p>查看full_48内容</p><pre><code>[root@ceph02 ~]# ceph-kvstore-tool leveldb /var/lib/ceph/mon/ceph-ceph02/store.db/ get osdmap full_48 out osdmap-full48.txt2018-03-27 17:44:34.623250 7f8bd795f040  1 leveldb: Recovering log #282(osdmap, full_48)2018-03-27 17:44:34.624751 7f8bd795f040  1 leveldb: Delete type=3 #2812018-03-27 17:44:34.624785 7f8bd795f040  1 leveldb: Delete type=0 #282 [root@ceph02 ~]# ceph-dencoder import osdmap-full48.txt type OSDMap decode dump_json{&quot;epoch&quot;: 48,&quot;fsid&quot;: &quot;f6110559-f0f1-4e14-9213-f29471329ee9&quot;,&quot;created&quot;: &quot;2017-02-09 15:34:22.284668&quot;,&quot;modified&quot;: &quot;2018-03-27 16:55:39.569689&quot;,&quot;flags&quot;: &quot;sortbitwise&quot;,&quot;cluster_snapshot&quot;: &quot;&quot;,&quot;pool_max&quot;: 2,&quot;max_osd&quot;: 5,&quot;pools&quot;: [    {        &quot;pool&quot;: 0,        &quot;pool_name&quot;: &quot;rbd&quot;,        &quot;flags&quot;: 1,        &quot;flags_names&quot;: &quot;hashpspool&quot;,        &quot;type&quot;: 1,        &quot;size&quot;: 1,        &quot;min_size&quot;: 1,        &quot;crush_ruleset&quot;: 0,        &quot;object_hash&quot;: 2,        &quot;pg_num&quot;: 64,        &quot;pg_placement_num&quot;: 64,        &quot;crash_replay_interval&quot;: 0,        &quot;last_change&quot;: &quot;22&quot;,        &quot;last_force_op_resend&quot;: &quot;0&quot;,        &quot;auid&quot;: 0,        &quot;snap_mode&quot;: &quot;selfmanaged&quot;,        &quot;snap_seq&quot;: 0,        &quot;snap_epoch&quot;: 0,        &quot;pool_snaps&quot;: [],        &quot;removed_snaps&quot;: &quot;[]&quot;,        &quot;quota_max_bytes&quot;: 0,        &quot;quota_max_objects&quot;: 0,        &quot;tiers&quot;: [],        &quot;tier_of&quot;: -1,        &quot;read_tier&quot;: -1,        &quot;write_tier&quot;: -1,        &quot;cache_mode&quot;: &quot;none&quot;,        &quot;target_max_bytes&quot;: 0,        &quot;target_max_objects&quot;: 0,        &quot;cache_target_dirty_ratio_micro&quot;: 0,        &quot;cache_target_dirty_high_ratio_micro&quot;: 0,        &quot;cache_target_full_ratio_micro&quot;: 0,        &quot;cache_min_flush_age&quot;: 0,        &quot;cache_min_evict_age&quot;: 0,        &quot;erasure_code_profile&quot;: &quot;&quot;,        &quot;hit_set_params&quot;: {            &quot;type&quot;: &quot;none&quot;        },        &quot;hit_set_period&quot;: 0,        &quot;hit_set_count&quot;: 0,        &quot;use_gmt_hitset&quot;: true,        &quot;min_read_recency_for_promote&quot;: 0,        &quot;min_write_recency_for_promote&quot;: 0,        &quot;hit_set_grade_decay_rate&quot;: 0,        &quot;hit_set_search_last_n&quot;: 0,        &quot;grade_table&quot;: [],        &quot;stripe_width&quot;: 0,        &quot;expected_num_objects&quot;: 0,        &quot;fast_read&quot;: false,        &quot;options&quot;: {}    },    {        &quot;pool&quot;: 1,        &quot;pool_name&quot;: &quot;data&quot;,        &quot;flags&quot;: 1,        &quot;flags_names&quot;: &quot;hashpspool&quot;,        &quot;type&quot;: 1,        &quot;size&quot;: 1,        &quot;min_size&quot;: 1,        &quot;crush_ruleset&quot;: 0,        &quot;object_hash&quot;: 2,        &quot;pg_num&quot;: 64,        &quot;pg_placement_num&quot;: 64,        &quot;crash_replay_interval&quot;: 45,        &quot;last_change&quot;: &quot;29&quot;,        &quot;last_force_op_resend&quot;: &quot;0&quot;,        &quot;auid&quot;: 0,        &quot;snap_mode&quot;: &quot;selfmanaged&quot;,        &quot;snap_seq&quot;: 0,        &quot;snap_epoch&quot;: 0,        &quot;pool_snaps&quot;: [],        &quot;removed_snaps&quot;: &quot;[]&quot;,        &quot;quota_max_bytes&quot;: 0,        &quot;quota_max_objects&quot;: 0,        &quot;tiers&quot;: [],        &quot;tier_of&quot;: -1,        &quot;read_tier&quot;: -1,        &quot;write_tier&quot;: -1,        &quot;cache_mode&quot;: &quot;none&quot;,        &quot;target_max_bytes&quot;: 0,        &quot;target_max_objects&quot;: 0,        &quot;cache_target_dirty_ratio_micro&quot;: 400000,        &quot;cache_target_dirty_high_ratio_micro&quot;: 600000,        &quot;cache_target_full_ratio_micro&quot;: 800000,        &quot;cache_min_flush_age&quot;: 0,        &quot;cache_min_evict_age&quot;: 0,        &quot;erasure_code_profile&quot;: &quot;&quot;,        &quot;hit_set_params&quot;: {            &quot;type&quot;: &quot;none&quot;        },        &quot;hit_set_period&quot;: 0,        &quot;hit_set_count&quot;: 0,        &quot;use_gmt_hitset&quot;: true,        &quot;min_read_recency_for_promote&quot;: 0,        &quot;min_write_recency_for_promote&quot;: 0,        &quot;hit_set_grade_decay_rate&quot;: 0,        &quot;hit_set_search_last_n&quot;: 0,        &quot;grade_table&quot;: [],        &quot;stripe_width&quot;: 0,        &quot;expected_num_objects&quot;: 0,        &quot;fast_read&quot;: false,        &quot;options&quot;: {}    },    {        &quot;pool&quot;: 2,        &quot;pool_name&quot;: &quot;metadata&quot;,        &quot;flags&quot;: 1,        &quot;flags_names&quot;: &quot;hashpspool&quot;,        &quot;type&quot;: 1,        &quot;size&quot;: 1,        &quot;min_size&quot;: 1,        &quot;crush_ruleset&quot;: 0,        &quot;object_hash&quot;: 2,        &quot;pg_num&quot;: 64,        &quot;pg_placement_num&quot;: 64,        &quot;crash_replay_interval&quot;: 0,        &quot;last_change&quot;: &quot;32&quot;,        &quot;last_force_op_resend&quot;: &quot;0&quot;,        &quot;auid&quot;: 0,        &quot;snap_mode&quot;: &quot;selfmanaged&quot;,        &quot;snap_seq&quot;: 0,        &quot;snap_epoch&quot;: 0,        &quot;pool_snaps&quot;: [],        &quot;removed_snaps&quot;: &quot;[]&quot;,        &quot;quota_max_bytes&quot;: 0,        &quot;quota_max_objects&quot;: 0,        &quot;tiers&quot;: [],        &quot;tier_of&quot;: -1,        &quot;read_tier&quot;: -1,        &quot;write_tier&quot;: -1,        &quot;cache_mode&quot;: &quot;none&quot;,        &quot;target_max_bytes&quot;: 0,        &quot;target_max_objects&quot;: 0,        &quot;cache_target_dirty_ratio_micro&quot;: 400000,        &quot;cache_target_dirty_high_ratio_micro&quot;: 600000,        &quot;cache_target_full_ratio_micro&quot;: 800000,        &quot;cache_min_flush_age&quot;: 0,        &quot;cache_min_evict_age&quot;: 0,        &quot;erasure_code_profile&quot;: &quot;&quot;,        &quot;hit_set_params&quot;: {            &quot;type&quot;: &quot;none&quot;        },        &quot;hit_set_period&quot;: 0,        &quot;hit_set_count&quot;: 0,        &quot;use_gmt_hitset&quot;: true,        &quot;min_read_recency_for_promote&quot;: 0,        &quot;min_write_recency_for_promote&quot;: 0,        &quot;hit_set_grade_decay_rate&quot;: 0,        &quot;hit_set_search_last_n&quot;: 0,        &quot;grade_table&quot;: [],        &quot;stripe_width&quot;: 0,        &quot;expected_num_objects&quot;: 0,        &quot;fast_read&quot;: false,        &quot;options&quot;: {}    }],&quot;osds&quot;: [    {        &quot;osd&quot;: 3,        &quot;uuid&quot;: &quot;74267031-f0a0-4bbc-9952-11726bdda80b&quot;,        &quot;up&quot;: 0,        &quot;in&quot;: 0,        &quot;weight&quot;: 0.000000,        &quot;primary_affinity&quot;: 1.000000,        &quot;last_clean_begin&quot;: 38,        &quot;last_clean_end&quot;: 40,        &quot;up_from&quot;: 43,        &quot;up_thru&quot;: 43,        &quot;down_at&quot;: 46,        &quot;lost_at&quot;: 0,        &quot;public_addr&quot;: &quot;192.168.1.169:6805\/17244&quot;,        &quot;cluster_addr&quot;: &quot;192.168.1.169:6806\/17244&quot;,        &quot;heartbeat_back_addr&quot;: &quot;192.168.1.169:6807\/17244&quot;,        &quot;heartbeat_front_addr&quot;: &quot;192.168.1.169:6808\/17244&quot;,        &quot;state&quot;: [            &quot;autoout&quot;,            &quot;exists&quot;        ]    },    {        &quot;osd&quot;: 4,        &quot;uuid&quot;: &quot;9ca8254e-3036-49b7-a340-66831590b37b&quot;,        &quot;up&quot;: 1,        &quot;in&quot;: 1,        &quot;weight&quot;: 1.000000,        &quot;primary_affinity&quot;: 1.000000,        &quot;last_clean_begin&quot;: 36,        &quot;last_clean_end&quot;: 40,        &quot;up_from&quot;: 43,        &quot;up_thru&quot;: 43,        &quot;down_at&quot;: 42,        &quot;lost_at&quot;: 0,        &quot;public_addr&quot;: &quot;192.168.1.169:6801\/17240&quot;,        &quot;cluster_addr&quot;: &quot;192.168.1.169:6802\/17240&quot;,        &quot;heartbeat_back_addr&quot;: &quot;192.168.1.169:6803\/17240&quot;,        &quot;heartbeat_front_addr&quot;: &quot;192.168.1.169:6804\/17240&quot;,        &quot;state&quot;: [            &quot;exists&quot;,            &quot;up&quot;        ]    }],&quot;osd_xinfo&quot;: [    {        &quot;osd&quot;: 3,        &quot;down_stamp&quot;: &quot;2018-03-27 16:26:00.221545&quot;,        &quot;laggy_probability&quot;: 0.000000,        &quot;laggy_interval&quot;: 0,        &quot;features&quot;: 576460752032874495,        &quot;old_weight&quot;: 65536    },    {        &quot;osd&quot;: 4,        &quot;down_stamp&quot;: &quot;2018-03-27 15:46:44.704581&quot;,        &quot;laggy_probability&quot;: 0.000000,        &quot;laggy_interval&quot;: 0,        &quot;features&quot;: 576460752032874495,        &quot;old_weight&quot;: 0    }],&quot;pg_temp&quot;: [],&quot;primary_temp&quot;: [],&quot;blacklist&quot;: {},&quot;erasure_code_profiles&quot;: {    &quot;default&quot;: {        &quot;k&quot;: &quot;2&quot;,        &quot;m&quot;: &quot;1&quot;,        &quot;plugin&quot;: &quot;jerasure&quot;,        &quot;technique&quot;: &quot;reed_sol_van&quot;    }}} </code></pre><p>后期具体使用过程结合实际情况进行。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考&lt;a href=&quot;https://blog.csdn.net/scaleqiao/article/details/51946042&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/scaleqiao/article/details/51946042&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;大家都知道Ceph的很多数据比如PG log、Monitor的数据都存在kvstore里（leveldb或者RocksDB中），Ceph也提供了查看kvstore里数据的工具，它就是ceph-kvstore-tool。&lt;br&gt;
    
    </summary>
    
    
      <category term="ceph" scheme="http://idcat.cn/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>ceph集群替换mon节点ip地址</title>
    <link href="http://idcat.cn/2018/03/27/ceph%E9%9B%86%E7%BE%A4%E6%9B%BF%E6%8D%A2mon%E8%8A%82%E7%82%B9ip%E5%9C%B0%E5%9D%80/"/>
    <id>http://idcat.cn/2018/03/27/ceph集群替换mon节点ip地址/</id>
    <published>2018-03-27T12:06:32.000Z</published>
    <updated>2018-03-27T12:11:28.596Z</updated>
    
    <content type="html"><![CDATA[<p>是由：原来单节点虚拟机集群，搬迁服务器，从A服务器复制虚拟机到B服务器上。然后启动后发现改节点IP地址已经变换，且无法恢复到原有IP地址，则必须使用新的ip地址作为mon 地址。<br><a id="more"></a><br>mon节点日志</p><pre><code>2018-03-27 14:37:13.691047 7f6dee6354c0  0 starting mon.ceph02 rank 0 at 192.168.0.40:6789/0 mon_data /var/lib/ceph/mon/ceph-ceph02 fsid f6110559-f0f1-4e14-9213-f29471329ee92018-03-27 14:37:13.691080 7f6dee6354c0 -1 accepter.accepter.bind unable to bind to 192.168.0.40:6789: (99) Cannot assign requested address2018-03-27 14:37:13.691088 7f6dee6354c0 -1 accepter.accepter.bind was unable to bind. Trying again in 5 seconds 2018-03-27 14:37:18.691584 7f6dee6354c0 -1 accepter.accepter.bind unable to bind to 192.168.0.40:6789: (99) Cannot assign requested address2018-03-27 14:37:18.691618 7f6dee6354c0 -1 accepter.accepter.bind was unable to bind. Trying again in 5 seconds 2018-03-27 14:37:23.692664 7f6dee6354c0 -1 accepter.accepter.bind unable to bind to 192.168.0.40:6789: (99) Cannot assign requested address2018-03-27 14:37:23.692699 7f6dee6354c0 -1 accepter.accepter.bind was unable to bind after 3 attempts: (99) Cannot assign requested address2018-03-27 14:37:23.692712 7f6dee6354c0 -1 unable to bind monitor to 192.168.0.40:6789/0</code></pre><p>1、获取当前集群monmap</p><pre><code>[root@ceph02 ~]# ceph mon getmap -o /tmp/momnmap2018-03-27 14:54:38.217464 7fa5f46ae700  0 -- :/2440828314 &gt;&gt; 192.168.0.40:6789/0 pipe(0x7fa5e8000c80 sd=3 :0 s=1 pgs=0 cs=0 l=1 c=0x7fa5e8001f90).fault2018-03-27 14:54:41.236171 7fa5f47af700  0 -- :/2440828314 &gt;&gt; 192.168.0.40:6789/0 pipe(0x7fa5e80052b0 sd=3 :0 s=1 pgs=0 cs=0 l=1 c=0x7fa5e8006570).fault</code></pre><p>单节点IP已经更改，也无法从其他mon节点获取monmap信息。下面换到从ceph.conf配置文件中获取monmap信息</p><pre><code>[root@ceph02 tmp]#  monmaptool --create --generate -c /etc/ceph/ceph.conf /tmp/monmapmonmaptool: monmap file /tmp/monmapmonmaptool: set fsid to f6110559-f0f1-4e14-9213-f29471329ee9monmaptool: writing epoch 0 to /tmp/monmap (1 monitors)</code></pre><p>2、查看信息</p><pre><code>[root@ceph02 tmp]# monmaptool --print monmap monmaptool: monmap file monmapepoch 0fsid f6110559-f0f1-4e14-9213-f29471329ee9last_changed 2018-03-27 15:09:11.680108created 2018-03-27 15:09:11.6801080: 192.168.0.40:6789/0 mon.noname-a</code></pre><p>可以看到原有ip。</p><p>3、删除原有ip，若有多个mon ip 则执行多次需要替换的mon 节点</p><pre><code>[root@ceph02 tmp]# monmaptool --rm noname-a /tmp/monmap monmaptool: monmap file /tmp/monmapmonmaptool: removing noname-amonmaptool: writing epoch 0 to /tmp/monmap (0 monitors)</code></pre><p>4、原有的monitor信息删除后，添加新的monitor节点，多个节点则执行多次，如下：</p><pre><code>[root@ceph02 tmp]# monmaptool --add ceph02 192.168.1.169:6789 /tmp/monmap monmaptool: monmap file /tmp/monmapmonmaptool: writing epoch 0 to /tmp/monmap (1 monitors)[root@ceph02 tmp]# monmaptool --print monmap monmaptool: monmap file monmapepoch 0fsid f6110559-f0f1-4e14-9213-f29471329ee9last_changed 2018-03-27 15:09:11.680108created 2018-03-27 15:09:11.6801080: 192.168.1.169:6789/0 mon.ceph02</code></pre><p>将新的manmap文件拷贝到所有运行ceph-mon服务的机器上（若多个mon 节点）</p><p>5、停止mon服务以及修改ceph.conf文件，并同步到所有节点</p><pre><code>[root@ceph02 tmp]# systemctl stop ceph-mon@ceph02[global]fsid = f6110559-f0f1-4e14-9213-f29471329ee9mon_initial_members = ceph02mon_host = 192.168.1.169auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephx[mon.ceph02]host = ceph02mon addr = 192.168.1.169</code></pre><p>6、注入新的monmap 多个节点则在多个节点执行 -i 后面输入实际节点主机名</p><pre><code>[root@ceph02 tmp]# ceph-mon -i ceph02 --inject-monmap /tmp/monmap </code></pre><p>7、启动mon。</p><pre><code>[root@ceph02 tmp]# systemctl start ceph-mon@ceph02[root@ceph02 tmp]# systemctl status ceph-mon@ceph02● ceph-mon@ceph02.service - Ceph cluster monitor daemon       Loaded: loaded (/usr/lib/systemd/system/ceph-mon@.service; enabled; vendor preset: disabled)       Active: active (running) since Tue 2018-03-27 15:36:30 CST; 4s ago     Main PID: 16664 (ceph-mon)       CGroup: /system.slice/system-ceph\x2dmon.slice/ceph-mon@ceph02.service       └─16664 /usr/bin/ceph-mon -f --cluster ceph --id ceph02 --setuser ceph --setgroup cephMar 27 15:36:30 ceph02 systemd[1]: Started Ceph cluster monitor daemon.Mar 27 15:36:30 ceph02 systemd[1]: Starting Ceph cluster monitor daemon...Mar 27 15:36:30 ceph02 ceph-mon[16664]: starting mon.ceph02 rank 0 at 192.168.1.169:6789/0 mon_data /var/lib/ceph/m...329ee9Mar 27 15:36:30 ceph02 ceph-mon[16664]: 2018-03-27 15:36:30.386583 7f71023634c0 -1 WARNING: &apos;mon addr&apos; config optio...p fileMar 27 15:36:30 ceph02 ceph-mon[16664]: continuing with monmap configurationHint: Some lines were ellipsized, use -l to show in full.[root@ceph02 tmp]# ceph -scluster f6110559-f0f1-4e14-9213-f29471329ee9 health HEALTH_OK monmap e2: 1 mons at {ceph02=192.168.1.169:6789/0}        election epoch 6, quorum 0 ceph02  fsmap e11: 1/1/1 up {0=ceph02=up:active} osdmap e40: 2 osds: 2 up, 2 in        flags sortbitwise  pgmap v1334: 192 pgs, 3 pools, 2068 bytes data, 20 objects        70932 kB used, 10148 MB / 10217 MB avail             192 active+clean</code></pre><p>最好重启下所有osd，否则可能会遇到mds degraded </p><p>mds日志</p><pre><code>2018-03-27 15:43:29.540226 7f7c96308180  0 set uid:gid to 167:167 (ceph:ceph)2018-03-27 15:43:29.540240 7f7c96308180  0 ceph version 10.2.3 (ecc23778eb545d8dd55e2e4735b53cc93f92e65b), process ceph-mds, pid 169102018-03-27 15:43:29.540395 7f7c96308180  0 pidfile_write: ignore empty --pid-file2018-03-27 15:43:29.719622 7f7c902e3700  1 mds.ceph02 handle_mds_map standby2018-03-27 15:43:29.720884 7f7c902e3700  1 mds.0.14 handle_mds_map i am now mds.0.142018-03-27 15:43:29.720888 7f7c902e3700  1 mds.0.14 handle_mds_map state change up:boot --&gt; up:replay2018-03-27 15:43:29.720895 7f7c902e3700  1 mds.0.14 replay_start2018-03-27 15:43:29.720899 7f7c902e3700  1 mds.0.14  recovery set is2018-03-27 15:43:29.720902 7f7c902e3700  1 mds.0.14  waiting for osdmap 41 (which blacklists prior instance)2018-03-27 15:43:32.771498 7f7c8b1d8700  0 -- 192.168.1.169:6800/16910 &gt;&gt; 192.168.0.40:6805/2661 pipe(0x7f7ca2014800 sd=17 :0 s=1 pgs=0 cs=0 l=1 c=0x7f7ca1fd6a80).fault2018-03-27 15:43:32.771585 7f7c8a0d5700  0 -- 192.168.1.169:6800/16910 &gt;&gt; 192.168.0.40:6801/2448 pipe(0x7f7ca20a4000 sd=18 :0 s=1 pgs=0 cs=0 l=1 c=0x7f7ca1fd6c00).fault</code></pre><p>以上，自己做个记录。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;是由：原来单节点虚拟机集群，搬迁服务器，从A服务器复制虚拟机到B服务器上。然后启动后发现改节点IP地址已经变换，且无法恢复到原有IP地址，则必须使用新的ip地址作为mon 地址。&lt;br&gt;
    
    </summary>
    
    
      <category term="ceph" scheme="http://idcat.cn/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>leveldb python基础操作方法</title>
    <link href="http://idcat.cn/2018/03/27/leveldb-python%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C%E6%96%B9%E6%B3%95/"/>
    <id>http://idcat.cn/2018/03/27/leveldb-python基础操作方法/</id>
    <published>2018-03-27T12:06:04.000Z</published>
    <updated>2018-03-27T12:10:47.129Z</updated>
    
    <content type="html"><![CDATA[<p>leveldb是google实现的一种非常高效的key-value数据库。以下内容仅做记录用。</p><p>1、源码安装</p><pre><code>下载源码 git clone https://code.google.com/p/leveldb/编译源码 cd leveldb &amp;&amp; make all</code></pre><p>2、python版本的leveldb安装很简单，pip install leveldb<br><a id="more"></a><br>实例脚本</p><pre><code>#!/usr/bin/env python#-*-coding: utf-8-*-import leveldbimport os, sys#初始化一个数据库studentsdef initialize():       db = leveldb.LevelDB(&quot;students&quot;);    return db;#插入def insert(db, sid, name):    db.Put(str(sid), name);#删除def delete(db, sid):    db.Delete(str(sid));#更新def update(db, sid, name):    db.Put(str(sid), name);#搜索def search(db, sid):    name = db.Get(str(sid));    return name;#遍历def display(db):    for key, value in db.RangeIter():        print (key, value);db = initialize();print &quot;Insert 3 records.&quot;insert(db, 1, &quot;Alice&quot;);insert(db, 2, &quot;Bob&quot;);insert(db, 3, &quot;Peter&quot;);display(db);print &quot;Delete the record where sid = 1.&quot;delete(db, 1);display(db);print &quot;Update the record where sid = 3.&quot;update(db, 3, &quot;Mark&quot;);display(db);print &quot;Get the name of student whose sid = 3.&quot;name = search(db, 3);print name;</code></pre><p>新建数据库很方便，如果这个目录已经存在就会直接打开，没有的话就会新建。示例中给出了添加，删除，和获取的方法，修改，更新，遍历等基本操作。</p><p><strong>以下摘自网络</strong></p><p>二 、 遍历</p><p>如何遍历数据呢，也非常方便，你可以指定开始的key和结束的key，也可以指定顺序，是否带value</p><pre><code>def test_iter():    db = leveldb.LevelDB(&apos;./data&apos;)    for i in xrange(10):        db.Put(str(i), &apos;string_%s&apos; % i)    print list(db.RangeIter(key_from = &apos;2&apos;, key_to = &apos;5&apos;))    print list(db.RangeIter(key_from = &apos;2&apos;, key_to = &apos;5&apos;,reverse=True))def iter_key_values():    db = leveldb.LevelDB(&apos;./data&apos;)    for i in xrange(10):        db.Put(str(i), &apos;string_%s&apos; % i)    keys = list(db.RangeIter(include_value = False))    print keys    keys_values = list(db.RangeIter())    print keys_values</code></pre><p>三、 批量操作<br>如果我对数据库有一大批操作，每一次都和数据库进行交互，其实挺浪费性能的，因此像mongodb，redis都提供了批量操作的方法，leveldb也是如此。下面是一个清空数据库的例子</p><pre><code>def clear_db():    db = leveldb.LevelDB(&apos;./data&apos;)    b = leveldb.WriteBatch()    for k in db.RangeIter(include_value = False, reverse = True):        b.Delete(k)    db.Write(b)</code></pre><p>b.Delete(k)并没有真正的删除数据，而是在db.Write(b)时执行所有的操作</p><p>四、 快照</p><p>创建快照非常简单，美中不足的是，再次加载数据库以后，没有方法找到之前创建的快照，难道已关闭这些快照就都不见了，这样的快照还有什么意思呢，也许只有python版本的快照是这样的吧</p><pre><code>def test_snapshot():    db = leveldb.LevelDB(&apos;./data&apos;)    db.Put(&apos;foo&apos;,&apos;s1&apos;)    s1 = db.CreateSnapshot()    db.Put(&apos;foo&apos;,&apos;s2&apos;)    s2 = db.CreateSnapshot()    print db.Get(&apos;foo&apos;)    print s1.Get(&apos;foo&apos;)    print s2.Get(&apos;foo&apos;)</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;leveldb是google实现的一种非常高效的key-value数据库。以下内容仅做记录用。&lt;/p&gt;
&lt;p&gt;1、源码安装&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;下载源码 git clone https://code.google.com/p/leveldb/

编译源码 cd leveldb &amp;amp;&amp;amp; make all
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2、python版本的leveldb安装很简单，pip install leveldb&lt;br&gt;
    
    </summary>
    
    
      <category term="leveldb ceph" scheme="http://idcat.cn/tags/leveldb-ceph/"/>
    
  </entry>
  
  <entry>
    <title>rbd对象组成以及介绍</title>
    <link href="http://idcat.cn/2018/03/26/rbd%E5%AF%B9%E8%B1%A1%E7%BB%84%E6%88%90%E4%BB%A5%E5%8F%8A%E4%BB%8B%E7%BB%8D/"/>
    <id>http://idcat.cn/2018/03/26/rbd对象组成以及介绍/</id>
    <published>2018-03-26T12:39:50.000Z</published>
    <updated>2018-03-26T12:50:34.560Z</updated>
    
    <content type="html"><![CDATA[<p>创建format 2格式的image 加上–image-format=2参数，下面直接看下rbd format2下有哪些对象</p><pre><code>[root@node1 ~]# rbd create image --size 100M --image-format=2[root@node1 ~]# rados ls -p rbd rbd_directoryrbd_id.imagerbd_header.3289416b8b4567</code></pre><a id="more"></a><p><strong>rbd_id.{image 名字}</strong></p><p>rbd_id对象的格式为：rbd\uid.{image name}head_hashpoolid</p><pre><code>[root@node1 ~]# ceph osd map rbd rbd_id.imageosdmap e188 pool &apos;rbd&apos; (0) object &apos;rbd_id.image&apos; -&gt; pg 0.1e6f8db0 (0.30) -&gt; up ([2,0], p2) acting ([2,0], p2)</code></pre><p>到osd2上 current下查看对象</p><pre><code>[root@node2 ~]# cd /var/lib/ceph/osd/ceph-2/current/0.30_head/[root@node2 0.30_head]# lltotal 4 -rw-r--r-- 1 ceph ceph  0 Jan 31 15:32 __head_00000030__0 -rw-r--r-- 1 ceph ceph 18 Mar 26 14:58 rbd\uid.image__head_1E6F8DB0__0[root@node2 0.30_head]# cat rbd\\uid.image__head_1E6F8DB0__0 3289416b8b4567</code></pre><p>cat查看到该对象存储的值为3289416b8b4567 ，也就是image这个块的id，与下面命令查看一致。其中文件最后_0表示的是pool id号  rbd池编号。</p><pre><code>[root@node2 0.30_head]# rbd info imagerbd image &apos;image&apos;:size 102400 kB in 25 objectsorder 22 (4096 kB objects)block_name_prefix: rbd_data.3289416b8b4567format: 2features: layeringflags: </code></pre><p><strong>rbd_directory</strong></p><p>rbd_directory对象的命名格式为：rbd\udirectoryhead_hashpoolid</p><pre><code>[root@node1 0.46_head]# ceph osd map rbd rbd_directoryosdmap e188 pool &apos;rbd&apos; (0) object &apos;rbd_directory&apos; -&gt; pg 0.30a98c1c (0.1c) -&gt; up ([1,2], p1) acting ([1,2], p1)[root@node1 0.46_head]# pwd/var/lib/ceph/osd/ceph-1/current/0.46_head[root@node1 0.46_head]# cd ../0.1c_head/[root@node1 0.1c_head]# lltotal 0 -rw-r--r-- 1 ceph ceph 0 Jan 31 15:32 __head_0000001C__0 -rw-r--r-- 1 ceph ceph 0 Mar 26 14:58 rbd\udirectory__head_30A98C1C__0</code></pre><p>这个对象里面包含对应存储池里所有的image的name和id的双向映射</p><pre><code>[root@node1 0.1c_head]# rados -p rbd  listomapvals  rbd_directoryid_3289416b8b4567value (9 bytes) :00000000  05 00 00 00 69 6d 61 67  65                       |....image|00000009name_imagevalue (18 bytes) :00000000  0e 00 00 00 33 32 38 39  34 31 36 62 38 62 34 35  |....3289416b8b45|36 37                                                       |67|00000012</code></pre><p>可以看到id对应的块的名字image，以及name_image（块名）对应的id号。</p><p><strong>rbd_header.{image id}</strong></p><p>rbd_header对象的格式为：rbd\uheader.{image id}head_hashpoolid</p><pre><code>[root@node1 ~]# ceph osd map rbd rbd_header.3289416b8b4567osdmap e188 pool &apos;rbd&apos; (0) object &apos;rbd_header.3289416b8b4567&apos; -&gt; pg 0.d49b9246 (0.46) -&gt; up ([0,1], p0) acting ([0,1], p0)[root@node1 ~]# cd /var/lib/ceph/osd/ceph-1/current/0.46_head/[root@node1 0.46_head]# lltotal 0 -rw-r--r-- 1 ceph ceph 0 Jan 31 15:33 __head_00000046__0 -rw-r--r-- 1 ceph ceph 0 Mar 26 14:58 rbd\uheader.3289416b8b4567__head_D49B9246__0</code></pre><p>记录rbd image的元数据，其内容包括size，order，object_prefix, snapseq, parent（克隆的image才有）, snapshot{snap id}（各个快照的信息）。</p><p>通过listomapvals查看对象的各属性值（k/v）</p><pre><code>[root@node1 0.46_head]# rados -p rbd listomapvals rbd_header.3289416b8b4567featuresvalue (8 bytes) :00000000  01 00 00 00 00 00 00 00                           |........|00000008object_prefixvalue (27 bytes) :00000000  17 00 00 00 72 62 64 5f  64 61 74 61 2e 33 32 38  |....rbd_data.328|00000010  39 34 31 36 62 38 62 34  35 36 37                 |9416b8b4567|0000001bordervalue (1 bytes) :00000000  16                                                |.|00000001sizevalue (8 bytes) :00000000  00 00 40 06 00 00 00 00                           |..@.....|00000008snap_seqvalue (8 bytes) :00000000  00 00 00 00 00 00 00 00                           |........|00000008</code></pre><p>通过listomapkeys查看对象的key值</p><pre><code>[root@node1 0.b_head]# rados -p rbd listomapkeys rbd_header.3289416b8b4567featuresobject_prefixordersizesnap_seq</code></pre><p><strong>没有快照的image</strong></p><ul><li>object_prefix：对象的名字前缀</li></ul><ul><li>order：用来计算block size的，比如22，那么块大小就是1&lt;&lt;22=4MB</li></ul><ul><li>size：对象大小</li></ul><ul><li>snap_seq：快照编号，没有快照的时候是0</li></ul><p><strong>做过快照的image</strong></p><p>新增如下属性值</p><ul><li>snapshot_id：记录对应快照的信息</li></ul><p>给image做快照</p><pre><code>[root@node1 0.46_head]# rbd snap create rbd/image@snap_image [root@node1 0.46_head]# rados -p rbd listomapvals rbd_header.3289416b8b4567。。。snap_seqvalue (8 bytes) :00000000  04 00 00 00 00 00 00 00                           |........|00000008snapshot_0000000000000004value (87 bytes) :00000000  04 01 51 00 00 00 04 00  00 00 00 00 00 00 0a 00  |..Q.............|00000010  00 00 73 6e 61 70 5f 69  6d 61 67 65 00 00 40 06  |..snap_image..@.|00000020  00 00 00 00 01 00 00 00  00 00 00 00 01 01 1c 00  |................|00000030  00 00 ff ff ff ff ff ff  ff ff 00 00 00 00 fe ff  |................|00000040  ff ff ff ff ff ff 00 00  00 00 00 00 00 00 00 00  |................|00000050  00 00 00 00 00 00 00                              |.......|  00000057</code></pre><p><strong>rbd_data.{image id}.{offset}</strong></p><p>rbd_data的对象命名格式为：rbd\udata.{image id}.fragementhead(snap)_hashpoolid</p><p>rbd image的数据对象，存放具体的数据内容。</p><p>列出对象</p><pre><code>[root@node1 0.b_head]# rados ls -p rbd | grep 3289rbd_data.3289416b8b4567.0000000000000006rbd_data.3289416b8b4567.0000000000000012rbd_data.3289416b8b4567.0000000000000017rbd_data.3289416b8b4567.0000000000000001rbd_data.3289416b8b4567.0000000000000018rbd_data.3289416b8b4567.000000000000000crbd_header.3289416b8b4567rbd_data.3289416b8b4567.0000000000000000</code></pre><p>对第一个对象查看osd存储路径</p><pre><code>[root@node1 mnt]# ceph  osd  map rbd  rbd_data.3289416b8b4567.0000000000000006osdmap e189 pool &apos;rbd&apos; (0) object &apos;rbd_data.3289416b8b4567.0000000000000006&apos; -&gt; pg 0.2b7fe90b (0.b) -&gt; up ([0,1], p0) acting ([0,1], p0)[root@node1 mnt]# cd /var/lib/ceph/osd/ceph-1/current/0.b_head/[root@node1 0.b_head]# lltotal 16-rw-r--r-- 1 ceph ceph     0 Jan 31 15:32 __head_0000000B__0-rw-r--r-- 1 ceph ceph 16384 Mar 26 18:05 rbd\udata.3289416b8b4567.0000000000000006__head_2B7FE90B__0</code></pre><p>具体对象文件格式rbd\uid.块id.对象片段号—hash_poolid</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;创建format 2格式的image 加上–image-format=2参数，下面直接看下rbd format2下有哪些对象&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 ~]# rbd create image --size 100M --image-format=2
[root@node1 ~]# rados ls -p rbd 
rbd_directory
rbd_id.image
rbd_header.3289416b8b4567
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
    
      <category term="ceph" scheme="http://idcat.cn/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>利用火焰图分析ceph-mds进程的cpu占用-简单介绍</title>
    <link href="http://idcat.cn/2018/03/14/%E5%88%A9%E7%94%A8%E7%81%AB%E7%84%B0%E5%9B%BE%E5%88%86%E6%9E%90ceph-mds%E8%BF%9B%E7%A8%8B%E7%9A%84cpu%E5%8D%A0%E7%94%A8-%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/"/>
    <id>http://idcat.cn/2018/03/14/利用火焰图分析ceph-mds进程的cpu占用-简单介绍/</id>
    <published>2018-03-14T12:53:48.000Z</published>
    <updated>2018-03-14T12:59:08.579Z</updated>
    
    <content type="html"><![CDATA[<p>利用火焰图分析ceph-mds进程的cpu占用-简单介绍</p><p><strong>Perf工具简介</strong></p><p>它是 Linux 系统原生提供的性能分析工具，用来进行软件性能分析的工具。是linux中用于分析性能问题的一大利器，可以针对一个运行着的程序，对系统进行采样，会返回 CPU 正在执行的函数名以及调用栈（stack）。<br>通常，它的执行频率是 99Hz（每秒99次），如果99次都返回同一个函数名，那就说明 CPU 这一秒钟都在执行同一个函数，可能存在性能问题。<br><a id="more"></a><br>安装</p><pre><code>yum install perf</code></pre><p><strong>火焰图（Flame Graphs）简介：</strong></p><p><a href="http://www.brendangregg.com/flamegraphs.html" target="_blank" rel="noopener">http://www.brendangregg.com/flamegraphs.html</a></p><p>火焰图是基于 perf 结果产生的 svg图片，用来展示 CPU 的调用栈。<br><img src="https://i.imgur.com/rniB5kO.jpg" alt=""></p><p>y 轴表示调用栈，每一层都是一个函数。调用栈越深，火焰就越高，顶部就是正在执行的函数，下方都是它的父函数。<br>x 轴表示抽样数，如果一个函数在 x 轴占据的宽度越宽，就表示它被抽到的次数多，即执行的时间长。注意，x 轴不代表时间，而是所有的调用栈合并后，按字母顺序排列的。<br>火焰图就是看顶层的哪个函数占据的宽度最大。只要有”平顶”（plateaus），就表示该函数可能存在性能问题</p><p><strong>下载</strong></p><p>Git  brendangregg大神的火焰图生成程序</p><pre><code>git clone https://github.com/brendangress/FlameGraph</code></pre><p><strong>使用</strong></p><p>进入到下载目录，另外窗口客户端对集群进行写操作，期间通过perf对mds进程进行采样收集。</p><p>客户端写入：</p><pre><code>time  dd if=/dev/zero of=a bs=1M count=1024 oflag=direct</code></pre><p>对mds进程进行perf测试并采样30s</p><pre><code>[root@ceph01 FlameGraph]# perf record -e cpu-clock --call-graph dwarf  -p 17484 -- sleep 30 [ perf record: Woken up 1 times to write data ][ perf record: Captured and wrote 0.327 MB perf.data (1 samples) ][root@ceph01 FlameGraph]#</code></pre><p><strong>分析</strong></p><p>首先对之前的采样结果进行整理</p><pre><code>[root@ceph01 FlameGraph]# perf script | ./stackcollapse-perf.pl &gt; ceph-mds.out</code></pre><p>生成火焰图</p><pre><code>[root@ceph01 FlameGraph]# ./flamegraph.pl ceph-mds.out &gt; ceph-mds.svg</code></pre><p>下载该svg文件并通过chrome浏览器打开<br><img src="https://i.imgur.com/ZcFZ2OM.jpg" alt=""><br>总结</p><p>火焰图的功能不止这一点，当然perf作为linux下的性能测试的一大神器，其功能也不仅仅这一点。该PPT的内容仅作为抛砖引玉之用。</p><p>参考网站 <a href="http://www.brendangregg.com/flamegraphs.html" target="_blank" rel="noopener">http://www.brendangregg.com/flamegraphs.html</a></p><p>后期关于一些测试方面的经验再做分享。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;利用火焰图分析ceph-mds进程的cpu占用-简单介绍&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Perf工具简介&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;它是 Linux 系统原生提供的性能分析工具，用来进行软件性能分析的工具。是linux中用于分析性能问题的一大利器，可以针对一个运行着的程序，对系统进行采样，会返回 CPU 正在执行的函数名以及调用栈（stack）。&lt;br&gt;通常，它的执行频率是 99Hz（每秒99次），如果99次都返回同一个函数名，那就说明 CPU 这一秒钟都在执行同一个函数，可能存在性能问题。&lt;br&gt;
    
    </summary>
    
    
      <category term="flamegraph" scheme="http://idcat.cn/tags/flamegraph/"/>
    
  </entry>
  
  <entry>
    <title>ceph获取crushmap-查看对象分布-目录绑定-查看条带</title>
    <link href="http://idcat.cn/2018/03/12/ceph%E8%8E%B7%E5%8F%96crushmap-%E6%9F%A5%E7%9C%8B%E5%AF%B9%E8%B1%A1%E5%88%86%E5%B8%83-%E7%9B%AE%E5%BD%95%E7%BB%91%E5%AE%9A-%E6%9F%A5%E7%9C%8B%E6%9D%A1%E5%B8%A6/"/>
    <id>http://idcat.cn/2018/03/12/ceph获取crushmap-查看对象分布-目录绑定-查看条带/</id>
    <published>2018-03-12T12:54:07.000Z</published>
    <updated>2018-03-12T13:17:21.346Z</updated>
    
    <content type="html"><![CDATA[<p>本文仅仅记录下常用命令，以备自己查询使用。</p><h1 id="crushmap"><a href="#crushmap" class="headerlink" title="crushmap"></a>crushmap</h1><p><strong>获取crush  map</strong></p><p>要获取集群的crush map，执行命令：</p><pre><code>ceph osd  getcrushmap -o {compiled-crushmap-filename}</code></pre><p>ceph将crush输出（-o）到你指定的文件，由于crush map是已编译的，所以需要反编译；</p><a id="more"></a><p><strong>反编译crush map</strong></p><p>要反编译crush map， 执行命令：</p><pre><code>crushtool -d {compiled-crushmap-filename}  -o {decompiled-crushmap-filename}</code></pre><p>ceph将反编译（-d）二进制crush图，且输出（-o）到你指定的文件；</p><p><strong>编译crush map</strong></p><p>要编译crush map，执行命令：</p><pre><code>crushtool -c  {decompiled-crushmap-filename}  -o {compiled-crushmap-filename}</code></pre><p>　　ceph将已编译的crush map保存到你指定的文件；</p><p><strong>注入crush map</strong></p><p>要把crush map应用到集群，执行命令：</p><pre><code>ceph osd  setcrushmap -i {compiled-crushmap-filename}</code></pre><p>ceph 将把你指定的已编译的crush  map输入到集群；</p><h1 id="查看对象分布"><a href="#查看对象分布" class="headerlink" title="查看对象分布"></a>查看对象分布</h1><p>查看文件系统下某个文件的对象集合</p><pre><code>[root@node192 jiushan-dir1]# cephfs ./10M show_layoutWARNING: This tool is deprecated.  Use the layout.* xattrs to query and modify layouts.layout.data_pool:     3layout.object_size:   4194304layout.stripe_unit:   4194304layout.stripe_count:  1[root@node192 jiushan-dir1]# cephfs ./10M show_location WARNING: This tool is deprecated.  Use the layout.* xattrs to query and modify layouts.location.file_offset:  0location.object_offset:0location.object_no:    0location.object_size:  4194304location.object_name:  10000001458.00000000location.block_offset: 0location.block_size:   4194304location.osd:          0[root@node192 jiushan-dir1]# cephfs ./10M mapWARNING: This tool is deprecated.  Use the layout.* xattrs to query and modify layouts.FILE OFFSET                    OBJECT        OFFSET        LENGTH  OSD          0      10000001458.00000000             0       4194304  0    4194304      10000001458.00000001             0       4194304  5    8388608      10000001458.00000002             0       4194304  2[root@node248 ~]# rados -p data ls100000003e9.00000000100000003e9.00000006100000003e9.00000002100000003e9.00000008100000003e9.00000001100000003e9.00000005100000003e9.00000004100000003e9.00000009100000003e9.00000003100000003e9.00000007[root@node248 ~]# ceph osd map data   100000003e9.00000007osdmap e39 pool &apos;data&apos; (1) object &apos;100000003e9.00000007&apos; -&gt; pg 1.e5b260d4 (1.d4) -&gt; up ([0,4], p0) acting ([0,4], p0)[root@node ~]# ceph osd treeID WEIGHT   TYPE NAME          UP/DOWN REWEIGHT PRIMARY-AFFINITY  -1 18.13797 root default                                          -2  7.25519     host node253                                    0  3.62759         osd.0           up  1.00000          1.00000  2  3.62759         osd.2           up  1.00000          1.00000  -3  7.25519     host node250                                    1  3.62759         osd.1           up  1.00000          1.00000  3  3.62759         osd.3           up  1.00000          1.00000  -4  3.62759     host node248                                    4  3.62759         osd.4           up  1.00000          1.00000 [root@node248 ~]# cd /var/lib/ceph/osd/ceph-4/current/[root@node248 current]# ls 1.d4_head/100000003e9.00000007__head_E5B260D4__1  __head_000000D4__1</code></pre><h1 id="目录绑定，查看条带"><a href="#目录绑定，查看条带" class="headerlink" title="目录绑定，查看条带"></a>目录绑定，查看条带</h1><p><strong>目录绑定到哪个存储池</strong></p><pre><code>getfattr -n ceph.dir.layout.pool /ceph</code></pre><p><strong>查看文件条带</strong></p><pre><code>getfattr -n ceph.file.layout file1 # file: file1ceph.file.layout=&quot;stripe_unit=4194304 stripe_count=1 object_size=4194304 pool=pool-rule-c2&quot;</code></pre><p>pool=存储池 </p><p><strong>查看目录所有信息</strong></p><pre><code>getfattr -n ceph.dir.layout nfs-1#file: nfs-1ceph.dir.layout=&quot;stripe_unit=4194304 stripe_count=1 object_size=4194304 pool=pool-rule-c2&quot;</code></pre><p><strong>设置 setfattr</strong></p><pre><code>[root@ceph226 ceph]# getfattr -n ceph.dir.layout nfs-2nfs-2: ceph.dir.layout: No such attribute[root@ceph226 ceph]# setfattr -n ceph.dir.layout.pool -v pool-rule-c2 nfs-2[root@ceph226 ceph]# getfattr -n ceph.dir.layout.pool  nfs-2#file: nfs-2ceph.dir.layout.pool=&quot;pool-rule-c2&quot;[root@ceph226 ceph]# getfattr -n ceph.dir.layout  nfs-2# file: nfs-2ceph.dir.layout=&quot;stripe_unit=4194304 stripe_count=1 object_size=4194304 pool=pool-rule-c2&quot;</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文仅仅记录下常用命令，以备自己查询使用。&lt;/p&gt;
&lt;h1 id=&quot;crushmap&quot;&gt;&lt;a href=&quot;#crushmap&quot; class=&quot;headerlink&quot; title=&quot;crushmap&quot;&gt;&lt;/a&gt;crushmap&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;获取crush  map&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;要获取集群的crush map，执行命令：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ceph osd  getcrushmap -o {compiled-crushmap-filename}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;ceph将crush输出（-o）到你指定的文件，由于crush map是已编译的，所以需要反编译；&lt;/p&gt;
    
    </summary>
    
    
      <category term="ceph" scheme="http://idcat.cn/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>记录客户端写入过程中进行拔盘操作导致osd日志分区故障的修复过程</title>
    <link href="http://idcat.cn/2018/03/08/%E8%AE%B0%E5%BD%95%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%86%99%E5%85%A5%E8%BF%87%E7%A8%8B%E4%B8%AD%E8%BF%9B%E8%A1%8C%E6%8B%94%E7%9B%98%E6%93%8D%E4%BD%9C%E5%AF%BC%E8%87%B4osd%E6%97%A5%E5%BF%97%E5%88%86%E5%8C%BA%E6%95%85%E9%9A%9C%E7%9A%84%E4%BF%AE%E5%A4%8D%E8%BF%87%E7%A8%8B/"/>
    <id>http://idcat.cn/2018/03/08/记录客户端写入过程中进行拔盘操作导致osd日志分区故障的修复过程/</id>
    <published>2018-03-08T13:22:25.000Z</published>
    <updated>2018-03-08T13:43:44.605Z</updated>
    
    <content type="html"><![CDATA[<p>记录客户端写入过程中进行拔盘操作导致osd日志分区故障的修复过程</p><p>在一次对ceph集群做模拟故障测试过程中，外置内核客户端挂载ceph集群，客户端开启fio进行写，dd进行读的过程中。对集群节点的某个osd磁盘进行拔盘操作。客户端读取，写入中断一段时间，之后继续恢复读写。在插入磁盘之后，发现有个其他的osd 没有up.如下图：<br><a id="more"></a></p><p><img src="https://i.imgur.com/Q8BIZIN.jpg" alt=""></p><p>通过dmesg查看到如下报错</p><pre><code>[13521.483167] sd 0:0:9:0: [sdd] tag#0 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_SENSE[13521.483175] sd 0:0:9:0: [sdd] tag#0 Sense Key : Medium Error [current] [13521.483181] sd 0:0:9:0: [sdd] tag#0 Add. Sense: Unrecovered read error[13521.483188] sd 0:0:9:0: [sdd] tag#0 CDB: Read(16) 88 00 00 00 00 00 00 dd 80 70 00 00 00 08 00 00[13521.483193] blk_update_request: critical medium error, dev sdd, sector 14516336[13521.484969] Buffer I/O error on dev sdd1, logical block 1814286, async page read[13524.093833] sd 0:0:9:0: [sdd] tag#0 CDB: Read(16) 88 00 00 00 00 00 00 dd 80 70 00 00 00 08 00 00</code></pre><p>通过查看osd日志发现如下问题</p><pre><code>-4&gt; 2018-03-08 19:53:08.955054 7feca2ec6800  2 journal read_entry 7414702080 : seq 1822676 4196207 bytes-3&gt; 2018-03-08 19:53:08.976400 7feca2ec6800  2 journal read_entry 7418904576 : seq 1822677 4196207 bytes-2&gt; 2018-03-08 19:53:08.997456 7feca2ec6800  2 journal read_entry 7423107072 : seq 1822678 4196123 bytes-1&gt; 2018-03-08 19:53:16.887077 7feca2ec6800 -1 journal FileJournal::wrap_read_bl: safe_read_exact 7427312370~4196207 returned -5 0&gt; 2018-03-08 19:53:16.890176 7feca2ec6800 -1 os/filestore/FileJournal.cc: In function &apos;void FileJournal::wrap_read_bl(off64_t, int64_t, ceph::bufferlist*, off64_t*) const&apos; thread 7feca2ec6800 time 2018-03-08 19:53:16.887097os/filestore/FileJournal.cc: 1969: FAILED assert(0)</code></pre><p>因此判断该osd日志分区磁盘有故障。</p><p><strong>修复磁盘</strong></p><p>首先解挂载该osd磁盘</p><p>[root@node193 ~]# umount /var/lib/ceph/osd/ceph-6/</p><p>通过badblocks工具对磁盘进行坏道检测以及修复</p><pre><code>[root@node193 ~]# badblocks  -s -v -c 4 -o /root/badblocks.log /dev/sdd1Checking blocks 0 to 10000383Checking for bad blocks (read-only test): done                                                 Pass completed, 4 bad blocks found. (4/0/0 errors)[root@node193 ~]# cat badblocks.log2177796217779721777982177799</code></pre><p>共发现4个bad block</p><p>修复bad block</p><pre><code>[root@node193 ~]#badblocks  -s -w  /dev/sdd1 2177799 2177796 </code></pre><p>(前面为结尾扇区，后面为起始扇区，可以设置区间范围大于该范围。例如2177800 2177700)</p><p>修复之后再次查看坏道</p><pre><code>[root@node193 ~]# badblocks  -s -v -c 4 -o /root/badblocks.log /dev/sdd1Checking blocks 0 to 10000383Checking for bad blocks (read-only test): done                                                 Pass completed, 0 bad blocks found. (0/0/0 errors)</code></pre><p>可以看到已经没有bad block了。</p><p><strong>恢复osd</strong></p><p>[root@node193 ~]#mount /dev/sdd2 /var/lib/ceph/osd/ceph-6/</p><p>[root@node193 ~]#systemctl start cephosd@6</p><p>[root@node193 ~]#systemctl reset-failed <a href="mailto:ceph-osd@6.service" target="_blank" rel="noopener">ceph-osd@6.service</a></p><p>[root@node193 ~]#systemctl start ceph-osd@6</p><p>[root@node193 ~]#systemctl status ceph-osd@6</p><p>当osd启动之后等待一会 就会看到该osd tree显示为up了，若一段时间还是为down则刷下journal脏数据，再次启动osd</p><p>[root@node193 ~]#ceph-osd -i 6 –flush-journal</p><p>[root@node193 ~]#systemctl restart ceph-osd@6</p><p>[root@node193 ~]#ceph osd tree</p><pre><code>ID WEIGHT   TYPE NAME        UP/DOWN REWEIGHT PRIMARY-AFFINITY -1 76.17947 root default                                       -2 25.39316     host node193                                   0  3.62759         osd.0         up  1.00000          1.00000 3  3.62759         osd.3         up  1.00000          1.00000 6  3.62759         osd.6         up  1.00000          1.00000 9  3.62759         osd.9         up  1.00000          1.00000 12  3.62759         osd.12        up  1.00000          1.00000 15  3.62759         osd.15        up  1.00000          1.00000 18  3.62759         osd.18        up  1.00000          1.00000 -3 25.39316     host node192                                   1  3.62759         osd.1         up  1.00000          1.00000 5  3.62759         osd.5         up  1.00000          1.00000 ....</code></pre><p><strong>转载badblocks工具介绍</strong></p><p>badblock命令用于查找磁盘中损坏的区块。 硬盘是一个损耗设备，当使用一段时间后可能会出现坏道等物理故障。电脑硬盘出现坏道后，如果不及时更换或进行技术处理，坏道就会越来越多，并会造成频繁死机和数据丢失。最好的处理方式是更换磁盘，但在临时的情况下，应及时屏蔽坏道部分的扇区，不要触动它们。badblocks就是一个很好的检查坏道位置的工具。</p><p>语法</p><pre><code>badblock(选项)(参数)选项-b&lt;区块大小&gt;：指定磁盘的区块大小，单位为字节；-o&lt;输出文件&gt;：将检查的结果写入指定的输出文件；-s：在检查时显示进度；-v：执行时显示详细的信息；-w：在检查时，执行写入测试。</code></pre><p>参数</p><pre><code>磁盘装置：指定要检查的磁盘装置；磁盘区块数：指定磁盘装置的区块总数；启始区块：指定要从哪个区块开始检查。</code></pre><p>实例</p><p>badblocks以4096的一个block，每一个block检查16次，将结果输出到“hda-badblocks-list”文件里。</p><p>badblocks -b 4096 -c 16 /dev/hda1 -o hda-badblocks-list<br>hda-badblocks-list是个文本文件，内容如下：</p><pre><code>cat hda-badblocks-list5124951250512515125351254……61245……</code></pre><p> 逻辑坏道修复方法</p><p>①、badblocks -s -w /dev/sda END START (END代表需要修复的扇区末端，START代表需要修复的扇区起始端)</p><p>②、fsck -a /dev/sda</p><p>修复后再用badblocks -s -v -o /root/bb.log /dev/sda监测看是否还有坏道存在，如果坏道还是存在的话说明坏道属于硬盘坏道。硬盘坏道要用隔离方法，首先记录监测出的硬盘坏道然后分区的时候把硬盘坏道所在的扇区分在一个分区（大小一般大于坏扇区大小），划分出的坏道分区不使用即可达到隔离的目的</p><p>0磁道坏道和硬盘坏道（准备换硬盘）</p><p>0磁道坏道的修复方法是隔离0磁道，使用fdsk划分区的时候从1磁道开始划分区。</p><p>如果是硬盘坏道的话，只能隔离不能修复</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录客户端写入过程中进行拔盘操作导致osd日志分区故障的修复过程&lt;/p&gt;
&lt;p&gt;在一次对ceph集群做模拟故障测试过程中，外置内核客户端挂载ceph集群，客户端开启fio进行写，dd进行读的过程中。对集群节点的某个osd磁盘进行拔盘操作。客户端读取，写入中断一段时间，之后继续恢复读写。在插入磁盘之后，发现有个其他的osd 没有up.如下图：&lt;br&gt;
    
    </summary>
    
    
      <category term="ceph" scheme="http://idcat.cn/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>flashcache介绍</title>
    <link href="http://idcat.cn/2018/03/08/flashcache%E4%BB%8B%E7%BB%8D/"/>
    <id>http://idcat.cn/2018/03/08/flashcache介绍/</id>
    <published>2018-03-08T13:21:33.000Z</published>
    <updated>2018-03-08T13:43:39.092Z</updated>
    
    <content type="html"><![CDATA[<p><strong>简介</strong></p><p>本文仅简单介绍flashcache的基本使用，安装，深度分析后期再补上。</p><p>Flashcache 是facebook的一个开源方案，主要被用于数据加速。基本结构为在hdd盘前面加一层缓存，既采用固态硬盘ssd。把热数据保存在ssd中，加速数据的读写，写入的过程也是先写到ssd上，然后ssd同步到后端sata磁盘上，最后的资料将保存在sata磁盘上，这样不用担心ssd损坏会造成资料丢失的问题，同时可以有大容量，高读取速度的存储。flashcache是建立在Linux devicemapper之上的，由devicemapper在SSD和backing HDD之上创建了一个逻辑的mapped device，用户使用的就是这个mapped device。flashcache把cache（SSD）按照哈希的方式进行结构化管理。</p><a id="more"></a><p><strong>安装</strong></p><pre><code>$ git clone https://github.com/facebook/flashcache.git$ cd flashcache$ make$ make install</code></pre><p>安装完成后，就可以载入flashcache模块</p><p>[root@node1 ~]#  modprobe flashcache</p><p>使用下面命令查看模块信息：</p><pre><code>[root@node1 ~]# lsmod |grep flashcacheflashcache            102400  1 dm_mod                110592  11 dm_log,dm_mirror,flashcache</code></pre><p><strong>flashcache_create相关参数说明：</strong></p><p>-p:缓存模式 </p><pre><code>writeback(数据先写到SSD，随后写到普通硬盘)，writethrough(数据同时写到SSD和普通硬盘)，writearound（数据绕过SSD，直接写到普通硬盘）</code></pre><p>三种，三种模式的所有读都会被缓存到flashcache可以通过dev.flashcache.<cachedev>.cache_all参数调整</cachedev></p><p>-s：缓存大小，可选项，如果未指定则整个SSD设备被用于缓存，默认的计数单位是扇区（sectors）,但是可以接受k/m/g单位。</p><p>-b：指定块大小，可选项，默认为4KB，必须为2的指数。默认单位为扇区。也可以用K作为单位，一般选4KB。</p><p>-f：强制创建，不进行检查</p><p>-m：设备元数据块大小，只有writeback需要存储metadata块，默认4K</p><p><strong>创建flashcache</strong></p><p>1、格式化sata磁盘</p><p>[root@node1 ~]# mkfs.xfs -f /dev/sdd</p><p>2、创建falshcache writeback模式</p><p>[root@node1 ~]# flashcache_create -p back -s 1G -b 4k cachedev /dev/sdc /dev/sdd</p><p>cachedev cachedev, ssd_devname /dev/sdc, disk_devname /dev/sdd cache mode </p><p>WRITE_BACK</p><p>block_size 8, md_block_size 8, cache_size 2097152</p><p>Flashcache metadata will use 5MB of your 1480MB main memory</p><p>3、挂载使用</p><p>[root@node1 ~]# mount /dev/mapper/cachedev /mnt/</p><p><strong>查看flashcache统计信息</strong></p><p>使用dmsetup status cachedev (缓存设备)查看缓存状态</p><p>使用dmsetup table cachedev (缓存设备)查看缓存数据</p><p>[root@node1 ~]# dmsetup status cachedev</p><pre><code>0 20971520 flashcache stats: reads(230), writes(0)read hits(116), read hit percent(50)write hits(0) write hit percent(0)dirty write hits(0) dirty write hit percent(0)replacement(0), write replacement(0)write invalidates(0), read invalidates(0)pending enqueues(0), pending inval(0)metadata dirties(0), metadata cleans(0)metadata batch(0) metadata ssd writes(0)cleanings(0) fallow cleanings(0)no room(0) front merge(0) back merge(0)force_clean_block(0)disk reads(114), disk writes(0) ssd reads(116) ssd writes(114)uncached reads(0), uncached writes(0), uncached IO requeue(0)disk read errors(0), disk write errors(0) ssd read errors(0) ssd write errors(0)uncached sequential reads(0), uncached sequential writes(0)pid_adds(0), pid_dels(0), pid_drops(0) pid_expiry(0)lru hot blocks(130304), lru warm blocks(130304)lru promotions(0), lru demotions(0)</code></pre><p>[root@node1 ~]# dmsetup table cachedev</p><pre><code>0 20971520 flashcache conf:ssd dev (/dev/sdc), disk dev (/dev/sdd) cache mode(WRITE_BACK)capacity(1018M), associativity(512), data block size(4K) metadata block size(4096b)disk assoc(0K)skip sequential thresh(0K)total blocks(260608), cached blocks(114), cache percent(0)dirty blocks(0), dirty percent(0)nr_queued(0)Size Hist: 512:747 1024:689 1536:552 2048:853 2560:549 3072:667 3584:659 4096:203067</code></pre><p>或者直接查看设备状态文件</p><p>[root@node1 ~]# cd /proc/flashcache/sda1+sdc3/</p><p>[root@node1 sdc+sdd]# ls</p><p>flashcache_errors  flashcache_iosize_hist  flashcache_pidlists  flashcache_stats       </p><p>错误统计</p><p>[root@node1 ~]# cat /proc/flashcache/sda1+sdc3/flashcache_errors </p><p>disk_read_errors=0 disk_write_errors=0 ssd_read_errors=0 ssd_write_errors=0 memory_alloc_errors=0</p><p><strong>监控flashcache信息</strong></p><p>监控文件在/root/flashcache-master/utils目录下</p><p>$ flashstat<br><img src="https://i.imgur.com/T2YWGaC.jpg" alt=""></p><p>Flashstat的用法很简单，指定监控时间间隔（-i 默认1s），监控次数（-c 默认0表示一直监控），flashcache设备（-d 默认/dev/mapper/cachedev）即可。如果需要将结果重定向到文件，则建议关闭ANSI颜色显示，使用-n或者–nocolor选项即可。</p><p>输出结果：</p><pre><code>read/s                reads per second for cachedevwrite/s               writes per second for cachedevdiskr/s               disk reads per seconddiskw/s               disk writes per secondssdr/s                ssd reads per secondssdw/s                ssd writes per seconduread/s               uncached reads per seconduwrit/s               uncached writes per secondmetaw/s               metadata ssd writes per secondclean/s               cleanings per secondrepl/s                replacement per secondwrepl/s               write replacement per secondhit%                  read hit percent(current hit%|total hit%)whit%                 write hit percent(current whit%|total whit%)dwhit%                dirty write hit percent(current dwhit%|total dwhit%)</code></pre><p><strong>卸载flashcache</strong></p><p>删除write backup设备的flashcache设备, 比较危险, 所有flashcache中的数据将被删除(未说明是否写脏数据).<br>writeback的flashcache设备不推荐这么做. 如果要删除的话, 建议使用dmsetup删除cache dev, 因为dmsetup会自动将脏数据写入磁盘.</p><p>非back模式：</p><pre><code>[root@node1 utils]#  umount /mnt[root@node1 utils]#  flashcache_destroy /dev/sdc  (ssd盘符)</code></pre><p>back模式</p><pre><code>[root@node1 utils]#  umount /mnt[root@node1 utils]#  dmsetup remove cachedev    （缓存盘名字）[root@node1 utils]#  flashcache_destroy /dev/sdc  (ssd盘符)</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;简介&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;本文仅简单介绍flashcache的基本使用，安装，深度分析后期再补上。&lt;/p&gt;
&lt;p&gt;Flashcache 是facebook的一个开源方案，主要被用于数据加速。基本结构为在hdd盘前面加一层缓存，既采用固态硬盘ssd。把热数据保存在ssd中，加速数据的读写，写入的过程也是先写到ssd上，然后ssd同步到后端sata磁盘上，最后的资料将保存在sata磁盘上，这样不用担心ssd损坏会造成资料丢失的问题，同时可以有大容量，高读取速度的存储。flashcache是建立在Linux devicemapper之上的，由devicemapper在SSD和backing HDD之上创建了一个逻辑的mapped device，用户使用的就是这个mapped device。flashcache把cache（SSD）按照哈希的方式进行结构化管理。&lt;/p&gt;
    
    </summary>
    
    
      <category term="flashcache" scheme="http://idcat.cn/tags/flashcache/"/>
    
  </entry>
  
  <entry>
    <title>ceph条带化介绍</title>
    <link href="http://idcat.cn/2018/03/04/ceph%E6%9D%A1%E5%B8%A6%E5%8C%96%E4%BB%8B%E7%BB%8D/"/>
    <id>http://idcat.cn/2018/03/04/ceph条带化介绍/</id>
    <published>2018-03-04T13:59:41.000Z</published>
    <updated>2018-03-08T13:39:40.217Z</updated>
    
    <content type="html"><![CDATA[<p>在 RADOS 层，Ceph 本身没有条带的概念，因为一个object 是作为一个文件整体性保存的。RBD 可以控制向一个 object 的写入方式，默认是将一个 object 写满再去写下一个object；还可以通过指定 stripe_unit 和 stripe_count，来将 object 分成若干个条带即 strip。一个 RDB image 会被分为多个 object 来保存，从而使得对一个 image 的多个读写可以分在多个 object 进行，从而可以防止某个 image 非常大或者非常忙时单个节点称为性能瓶颈。还可以将 object 进一步条带化为多个条带（stripe unit）。条带（stripe）是 librados 通过 ODS 写入数据的基本单位。这么做的好处是在保持对象数目的同时，进一步减少可以同步读写的粒度（从 object 粒度减少到 stripe 粒度），从而提高读写效率。<br><a id="more"></a><br>Ceph 的条带化行为（如果划分条带和如何写入条带）受三个参数控制： </p><p><strong>object-size(cephfs),order(rbd,默认为22)</strong>：默认对象大小为4MB。 </p><p><strong>stripe_unit：</strong>条带（stripe unit）的大小。每个 [stripe_unit] 的连续字节会被连续地保存到同一个对象中，client 写满 stripe unit 大小的数据后，接着去下一个 object 中写下一个 stripe unit 大小的数据。默认为 1，此时一个 stripe 就是一个 object。</p><p><strong>stripe_count：</strong>在分别写入了 [stripe_unit] 个字节到 [stripe_count] 个对象后，ceph 又重新从一个新的对象开始写下一个条带，直到该对象达到了它的最大大小。这时候，ceph 转移到下 [stripe_unit] 字节。默认为 object site。</p><p>举例：32M的rbd image，object_size=4M,stripe_unit=1M,stripe_count=4  </p><p>分析：</p><ol><li>RBD image 会被保存在总共 8 个 RADOS object （计算方式为 client data size 除以 2^[order]单位：字节）中。 </li><li>stripe_unit 为 object size 的四分之一，也就是说每个 object 包含 4 个 stripe(1M)。 </li><li>stripe_count 为 4，即每个 object set 包含四个 object。这样，client 以 4 为一个循环，向一个 object set 中的每个 object 依次写入 stripe，写到第 16 个 stripe 后，按照同样的方式写第二个 object set。</li></ol><pre><code> ---                       object set 0                          --- /---------\    /---------\         /---------\    /---------\  | obj_0 4M|    | obj_1 4M|         | obj_2 4M|   | obj_3 4M||=========|    |=========|         |=========|    |=========|| stripe  |    | stripe  |         | stripe  |    | stripe  || unit 0  |    | unit 1  |         | unit 2  |    | unit 3  |   1M             1M                   1M             1M|---------|    |---------|         |---------|    |---------|| stripe  |    | stripe  |         | stripe  |    | stripe  || unit 4  |    | unit 5  |         | unit 6  |    | unit 7  ||---------|    |---------|         |---------|    |---------||---------|    |---------|         |---------|    |---------|| stripe  |    | stripe  |         | stripe  |    | stripe  || unit 8  |    | unit 9  |         | unit 10  |   | unit 11 ||---------|    |---------|         |---------|    |---------|       |---------|    |---------|         |---------|    |---------|| stripe  |    | stripe  |         | stripe  |    | stripe  || unit 12 |   | unit 13  |         | unit14  |    | unit 15 ||---------|    |---------|         |---------|    |---------|        ---                       object set 1                         --- /---------\    /---------\         /---------\    /---------\  | obj_4 4M|    | obj_5 4M|         | obj_6 4M|    | obj_7 4M||=========|    |=========|         |=========|    |=========|| stripe  |    | stripe  |         | stripe  |    | stripe  || unit 0  |    | unit 1  |         | unit 2  |    | unit 3  |   1M             1M                   1M             1M|---------|    |---------|         |---------|    |---------|| stripe  |    | stripe  |         | stripe  |    | stripe  || unit 4  |    | unit 5  |         | unit 6  |    | unit 7  ||---------|    |---------|         |---------|    |---------||---------|    |---------|         |---------|    |---------|| stripe  |    | stripe  |         | stripe  |    | stripe  || unit 8  |    | unit 9  |         | unit 10  |   | unit 11 ||---------|    |---------|         |---------|    |---------|       |---------|    |---------|         |---------|    |---------|| stripe  |    | stripe  |         | stripe  |    | stripe  || unit 12 |   | unit 13  |         | unit14  |    | unit 15 ||---------|    |---------|         |---------|    |---------| </code></pre><p>默认的情况下，[stripe_unit] 等于 object size；stripe_count 为1。意味着 ceph client 在将第一个 object 写满后再去写下一个 object。</p><p><strong>cephfs</strong></p><p>cephfs也同样支持配置file layout，可以控制file分配到指定的ceph rados objects上，这些信息是写在file/dir的xattrs上。</p><p>文件的layout xattrs为：ceph.file.layout</p><p>目录的layout xattrs为：ceph.dir.layout</p><p>目录中的文件和子目录默认继承父目录的layout配置</p><p>支持的layout配置项有：</p><p>pool： file的数据存储在哪个RADOS pool里</p><p>namespace： file的数据存储在RADOS pool里的哪个namespace里，但现在rbd/rgw/cephfs都还不支持</p><p>stripe_unit：  条带的大小，以Bytes为单位</p><p>stripe_count：  条带的个数</p><p>比如，stripe_unit=512kb，stripe_count=2，默认object size是4MB，则file写10MB的数据分配如下：<br>（stripe_count=2表示2个对象（每个对象大小为4M）为一个对象集，接受stripe_unit单位大小的连续字节循环写入。此时写入10M文件，则除以stripe_unit等于20个条带对象，循环写入到每个对象集中的每个对象，当一个对象集里面对象都写满后，剩下的条带对象写入到第二个对象集中,然后每个对象写入到不同的osd中去）</p><pre><code> --- object set 0 ---                   --- object set 1 ---/---------\    /---------\         /---------\    /---------\  | obj_0 4M|    | obj_1 4M|         | obj_M2 4M|   | obj_3 4M||=========|    |=========|         |=========|    |=========|| stripe  |    | stripe  |         | stripe  |    | stripe  || unit 0  |    | unit 1  |         | unit 0  |    | unit 1  |   512KB          512KB              512KB          512KB|---------|    |---------|         |---------|    |---------|| stripe  |    | stripe  |         | stripe  |    | stripe  || unit 2  |    | unit 3  |         | unit 2  |    | unit 3  ||---------|    |---------|         \=========/    \=========/| stripe  |    | stripe  || unit 4  |    | unit 5  |            osd 16         osd 20|---------|    |---------|         | stripe  |    | stripe  |         | unit 6  |    | unit 7  |         |---------|    |---------|         | stripe  |    | stripe  |         | unit 8  |    | unit 9  |         |---------|    |---------|         | stripe  |    | stripe  |         | unit 10 |    | unit 11 |         |---------|    |---------|         | stripe  |    | stripe  |         | unit 12 |    | unit 13 |         |---------|    |---------|         | stripe  |    | stripe  |         | unit 14 |    | unit 15 |         \=========/    \=========/           osd 25         osd 3</code></pre><p><strong>验证file stripe</strong></p><pre><code># mount -t ceph 192.168.0.31:/ /vcfs# mkdir /vcfs/vcfs/test# setfattr -n ceph.dir.layout -v &quot;stripe_unit=524288 stripe_count=8 object_size=4194304 pool=cephfs_data2&quot; /vcfs/vcfs/test# getfattr -n ceph.dir.layout /vcfs/vcfs/testgetfattr: Removing leading &apos;/&apos; from absolute path names # file: vcfs/vcfs/testceph.dir.layout=&quot;stripe_unit=524288 stripe_count=8 object_size=4194304 pool=data&quot;# cephfs /vcfs/vcfs/test show_layoutWARNING: This tool is deprecated.  Use the layout.* xattrs to query and modify layouts.layout.data_pool:     1layout.object_size:   4194304layout.stripe_unit:   524288layout.stripe_count:  8</code></pre><p>写入文件，并查看文件的location</p><pre><code># dd if=/dev/zero of=/vcfs/vcfs/test bs=4M count=100# cephfs /vcfs/vcfs/test show_locationWARNING: This tool is deprecated.  Use the layout.* xattrs to query and modify layouts.location.file_offset:  0        // file的偏移location.object_offset:0        // object的偏移location.object_no:    0        // object的numberlocation.object_size:  4194304 // object size为4Mlocation.object_name:  10000002356.00000000 // object的namelocation.block_offset: 0        // block的偏移location.block_size:   524288    // block size为512k(这里很关键)location.osd:          0        // 存储在osd 0 上# cephfs /vcfs/vcfs/test show_location -l 524288WARNING: This tool is deprecated.  Use the layout.* xattrs to query and modify layouts.location.file_offset:  524288    // file的偏移location.object_offset:0        // object的偏移location.object_no:    1        // object的numberlocation.object_size:  4194304 // object size为4Mlocation.object_name:  10000002356.00000001 // object的namelocation.block_offset: 0        // block的偏移location.block_size:   524288    // block size为512k(这里很关键)location.osd:          2        // 存储在osd 2 上</code></pre><p>新建目录设置strip_unit为1M 即1048576字节</p><pre><code>#mkdir /vcfs/vcfs/test1    # setfattr -n ceph.dir.layout -v &quot;stripe_unit=1048576 stripe_count=8 object_size=4194304&quot; /vcfs/vcfs/test1# dd if=/dev/zero of=/vcfs/vcfs/test1/abc bs=4M count=1010+0 records in10+0 records out41943040 bytes (42 MB) copied, 0.370634 s, 113 MB/s# cephfs /vcfs/vcfs/test1/ show_layoutWARNING: This tool is deprecated.  Use the layout.* xattrs to query and modify layouts.layout.data_pool:     1layout.object_size:   4194304layout.stripe_unit:   1048576layout.stripe_count:  8#cephfs /vcfs/vcfs/test1/abc show_locationWARNING: This tool is deprecated.  Use the layout.* xattrs to query and modify layouts.location.file_offset:  0location.object_offset:0location.object_no:    0location.object_size:  4194304location.object_name:  10000000005.00000000location.block_offset: 0location.block_size:   1048576  // block size为1Mlocation.osd:          3#cephfs /vcfs/vcfs/test1/abc show_location -l 1048576WARNING: This tool is deprecated.  Use the layout.* xattrs to query and modify layouts.location.file_offset:  1048576location.object_offset:0location.object_no:    1location.object_size:  4194304location.object_name:  10000000005.00000001location.block_offset: 0location.block_size:   1048576    // block size为1Mlocation.osd:          0</code></pre><p><strong>rbd</strong></p><p>在测试rbd条带的过程中遇到如下问题，以后在解决。暂时记录，初步怀疑内核不支持。</p><pre><code>[root@radosgw1 ~]# rbd create test -p zhang --size=100M --object-size 4M --stripe-unit 1048576 --stripe-count 8 --image-format 2 --image-feature layering[root@radosgw1 ~]# rbd info zhang/testrbd image &apos;test&apos;:size 102400 kB in 28 objectsorder 22 (4096 kB objects)block_name_prefix: rbd_data.5a27626b8b4567format: 2features: layering, stripingflags: stripe unit: 1024 kBstripe count: 8[root@radosgw1 ~]# rbd map zhang/testrbd: sysfs write failedIn some cases useful info is found in syslog - try &quot;dmesg | tail&quot; or so.rbd: map failed: (22) Invalid argument[root@radosgw1 ~]# dmesg |tail -n 1[274817.782747] rbd: image test: unsupported stripe unit (got 1048576 want 4194304) [root@radosgw1 ~]# uname -r4.4.13-1.el7.elrepo.x86_64</code></pre><p>未完待续。。。。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 RADOS 层，Ceph 本身没有条带的概念，因为一个object 是作为一个文件整体性保存的。RBD 可以控制向一个 object 的写入方式，默认是将一个 object 写满再去写下一个object；还可以通过指定 stripe_unit 和 stripe_count，来将 object 分成若干个条带即 strip。一个 RDB image 会被分为多个 object 来保存，从而使得对一个 image 的多个读写可以分在多个 object 进行，从而可以防止某个 image 非常大或者非常忙时单个节点称为性能瓶颈。还可以将 object 进一步条带化为多个条带（stripe unit）。条带（stripe）是 librados 通过 ODS 写入数据的基本单位。这么做的好处是在保持对象数目的同时，进一步减少可以同步读写的粒度（从 object 粒度减少到 stripe 粒度），从而提高读写效率。&lt;br&gt;
    
    </summary>
    
    
      <category term="ceph" scheme="http://idcat.cn/tags/ceph/"/>
    
  </entry>
  
</feed>
